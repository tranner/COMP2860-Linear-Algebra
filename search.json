[
  {
    "objectID": "src/lec01.html",
    "href": "src/lec01.html",
    "title": "(Preliminary topic) Floating point number systems",
    "section": "",
    "text": "Computers store numbers with finite precision, i.e. using a finite set of bits (binary digits), typically 32 or 64 of them. You met how to store numbers as floating point numbers last year in the module COMP18XX??.\nYou will recall that many numbers cannot be stored exactly.\n\nSome numbers cannot be represented precisely using any finite set of digits:\ne.g. \\(\\sqrt{2} = 1.141 42\\ldots\\), \\(\\pi = 3.141 59\\ldots\\), etc.\nSome cannot be represented precisely in a given number base:\ne.g. \\(\\frac{1}{9} = 0.111\\ldots\\) (decimal), \\(\\frac{1}{5} = 0.0011 0011 \\ldots\\) (binary).\nOthers can be represented by a finite number of digits but only using more than are available: e.g. \\(1.526 374 856 437\\) cannot be stored exactly using 10 decimal digits.\n\nThe inaccuracies inherent in finite precision arithmetic must be modelled in order to understand:\n\nhow the numbers are represented (and the nature of associated limitations);\nthe errors in their representation;\nthe errors which occur when arithmetic operations are applied to them.\n\nThe examples shown here will be in decimal by the issues apply to any base, e.g. binary.\nThis is important when trying to solve problems with floating point numbers so that we learn how to avoid the key pitfalls.",
    "crumbs": [
      "(Preliminary topic) Floating point number systems"
    ]
  },
  {
    "objectID": "src/lec01.html#finite-precision-number-systems",
    "href": "src/lec01.html#finite-precision-number-systems",
    "title": "(Preliminary topic) Floating point number systems",
    "section": "",
    "text": "Computers store numbers with finite precision, i.e. using a finite set of bits (binary digits), typically 32 or 64 of them. You met how to store numbers as floating point numbers last year in the module COMP18XX??.\nYou will recall that many numbers cannot be stored exactly.\n\nSome numbers cannot be represented precisely using any finite set of digits:\ne.g. \\(\\sqrt{2} = 1.141 42\\ldots\\), \\(\\pi = 3.141 59\\ldots\\), etc.\nSome cannot be represented precisely in a given number base:\ne.g. \\(\\frac{1}{9} = 0.111\\ldots\\) (decimal), \\(\\frac{1}{5} = 0.0011 0011 \\ldots\\) (binary).\nOthers can be represented by a finite number of digits but only using more than are available: e.g. \\(1.526 374 856 437\\) cannot be stored exactly using 10 decimal digits.\n\nThe inaccuracies inherent in finite precision arithmetic must be modelled in order to understand:\n\nhow the numbers are represented (and the nature of associated limitations);\nthe errors in their representation;\nthe errors which occur when arithmetic operations are applied to them.\n\nThe examples shown here will be in decimal by the issues apply to any base, e.g. binary.\nThis is important when trying to solve problems with floating point numbers so that we learn how to avoid the key pitfalls.",
    "crumbs": [
      "(Preliminary topic) Floating point number systems"
    ]
  },
  {
    "objectID": "src/lec01.html#normalised-systems",
    "href": "src/lec01.html#normalised-systems",
    "title": "(Preliminary topic) Floating point number systems",
    "section": "2 Normalised systems",
    "text": "2 Normalised systems\nTo understand how this works in practice, we introduce an abstract way to think about the practicalities of floating point numbers, but our examples will have smaller numbers of digits.\nAny finite precision number can be written using the floating point representation\n\\[\nx = \\pm 0.b_1 b_2 b_3 \\ldots b_{t-1} b_t \\times \\beta^e.\n\\]\n\nThe digits \\(b_i\\) are integers satisfying \\(0 \\le b_i \\le \\beta - 1\\).\nThe mantissa, \\(b_1 b_2 b_3 \\ldots b_{t-1} b_t\\), contains \\(t\\) digits.\n\\(\\beta\\) is the base (always a positive integer).\n\\(e\\) is the integer exponent and is bounded (\\(L \\le e \\le U\\)).\n\n\\((\\beta, t, L, U)\\) fully defines a finite precision number system.\nNormalised finite precision systems will be considered here for which\n\\[\nb_1 \\neq 0 \\quad (0 &lt; b_1 \\le \\beta -1).\n\\]\nExamples:\n\nIn the case \\((\\beta, t, L, U) = (10, 4, -49, 50)\\) (base 10), \\[\n10 000 = .1000 \\times 10^5, \\quad\n22.64 = .2264 \\times 10^2, \\quad\n0.000 056 7 = .5670 \\times 10^{-4}\n\\]\nIn the case \\((\\beta, t, L, U) = (2, 6, -7, 8)\\) (binary), \\[\n1 0000 = .1000 00 \\times 2^5, \\quad\n1011.11 = .1011 11 \\times 2^4,\\] \\[\n0.0000 11 = .1100 00 \\times 2^{-4}.\n\\]\nZero is always taken to be a special case e.g., \\(0 = \\pm .00\\ldots 0 \\times \\beta^0\\).\n\nOur familiar floating point numbers can be representing using this format too:\n\nThe IEEE single precision standard is \\((\\beta, t, L, U) = (2, 23, -127, 128)\\). This is available via numpy.single.\nThe IEEE double precision standard is \\((\\beta, t, L, U) = (2, 52, -1023, 1024)\\). This is available via numpy.double.\n\n\nimport numpy as np\n\na = np.double(1.1)\nprint(type(a))\nb = np.single(1.2)\nprint(type(b))\nc = np.half(1.3)\nprint(type(c))\n\n&lt;class 'numpy.float64'&gt;\n&lt;class 'numpy.float32'&gt;\n&lt;class 'numpy.float16'&gt;\n\n\n\n\n\n\n\n\nExample 1\n\n\n\nConsider the number system given by \\((\\beta, t, L, U) = (10, 2, -1, 2)\\) which gives\n\\[\nx = \\pm .b_1 b_2 \\times 10^e \\text{ where } -1 \\le e \\le 2.\n\\]\n\n\nHow many numbers can be represented by this normalised system?\n\n\n\nthe sign can be positive or negative\n\\(b_1\\) can take on the values \\(1\\) to \\(9\\) (9 options)\n\\(b_2\\) can take on the values \\(0\\) to \\(9\\) (10 options)\n\\(e\\) can take on the values \\(-1, 0, 1, 2\\) (4 options)\n\nOverall this gives us: \\[\n2 \\times 9 \\times 10 \\times 4 \\text{ options } = 720 \\text{ options}.\n\\]\n\n\nWhat are the two largest positive numbers in this system?\n\n\nThe largest value uses \\(+\\) as a sign, \\(b_1 = 9\\), \\(b_2 = 9\\) and \\(e = 2\\) which gives \\[\n+ 0.99 \\times 10^{2} = 99.\n\\]\nThe second largest value uses \\(+\\) as a sign, \\(b_1 = 9\\), \\(b_2 = 8\\) and \\(e = 2\\) which gives \\[\n+ 0.98 \\times 10^{2} = 98.\n\\]\n\n\nWhat are the two smallest positive numbers?\n\n\nThe smallest positive number has \\(+\\) sign, \\(b_1 = 1\\), \\(b_2 =0\\) and \\(e=-1\\) which gives \\[\n+ 0.10 \\times 10^{-1} = 0.01.\n\\]\nThe second smallest positive number has \\(+\\) sign, \\(b_1 = 1\\), \\(b_2 = 1\\) and \\(e = -1\\) which gives \\[\n+ 0.11 \\times 10^{-1} = 0.011.\n\\]\n\n\nWhat is the smallest possible difference between two numbers in this system?\n\n\nThe smallest different will be between numbers of the form \\(+0.10 \\ times 10^{-1}\\) and \\(+0.11 \\times 10^{-1}\\) which gives \\[\n0.11 \\times 10^{-1} - 0.10 \\times 10^{-1} = 0.011 - 0.010 = 0.001.\n\\]\nAlternatively, we can brute force search for this:\n\n\nThe minimum difference min_diff=0.0010\nat x=+0.57 x 10^{-1} y=+0.58 x 10^{-1}.\n\n\n\n\n\n\n\n\n\n\nExample 2 (homework)\n\n\n\nConsider the number system given by \\((\\beta, t, L, U) = (10, 3, -3, 3)\\) which gives\n\\[\nx = \\pm .b_1 b_2 b_3 \\times 10^e \\text{ where } -3 \\le e \\le 3.\n\\]\n\nHow many numbers can be represented by this normalised system?\nWhat are the two largest positive numbers in this system?\nWhat are the two smallest positive numbers?\nWhat is the smallest possible difference between two numbers in this system?\nWhat is the smallest possible difference in this system, \\(x\\) and \\(y\\), for which \\(x &lt; 100 &lt; y\\)?\n\n\n\n\n\n\n\n\n\nExample 3 - what about in python\n\n\n\nWe find that even with double-precision floating point numbers, we see sum funniness when working with decimals:\n\na = np.double(0.0)\n\nfor _ in range(10):\n    a = a + np.double(0.1)\n    print(a)\n\nprint(\"Is a = 1?\", a == 1.0)\n\n0.1\n0.2\n0.30000000000000004\n0.4\n0.5\n0.6\n0.7\n0.7999999999999999\n0.8999999999999999\n0.9999999999999999\nIs a = 1? False\n\n\n\nWhy is this output not a surprise?\n\nWe also see that even adding up numbers can have different results depending on what order we add them:\n\nx = np.double(1e30)\ny = np.double(-1e30)\nz = np.double(1.0)\n\nprint(f\"{(x + y) + z=:.16f}\")\n\n(x + y) + z=1.0000000000000000\n\n\n\nprint(f\"{x + (y + z)=:.16f}\")\n\nx + (y + z)=0.0000000000000000",
    "crumbs": [
      "(Preliminary topic) Floating point number systems"
    ]
  },
  {
    "objectID": "src/lec01.html#errors-and-machine-precision",
    "href": "src/lec01.html#errors-and-machine-precision",
    "title": "(Preliminary topic) Floating point number systems",
    "section": "3 Errors and machine precision",
    "text": "3 Errors and machine precision\nFrom now on \\(fl(x)\\) will be used to represent the (approximate) stored value of \\(x\\). The error in this representation can be expressed in two ways.\n\\[\n\\begin{aligned}\n\\text{Absolute error} &= | fl(x) - x | \\\\\n\\text{Relative error} &= \\frac{| fl(x) - x |}{|x|}.\n\\end{aligned}\n\\]\nThe number \\(fl(x)\\) is said to approximate \\(x\\) to \\(t\\) significant digits (or figures) if \\(t\\) is the largest non-negative integer for which\n\\[\n\\text{Relative error} &lt; 0.5 \\times \\beta^{1-t}.\n\\]\nIt can be proved that if the relative error is equal to \\(\\beta^{-d}\\) then \\(fl(x)\\) has \\(d\\) correct significant digits.\nIn the number system given by \\((\\beta, t, L, U)\\), the nearest (larger) representable number to \\(x = 0.b_1 b_2 b_3 \\ldots b_{t-1} b_t \\times \\beta^e\\) is\n\\[\n\\tilde{x} = x + .\\underbrace{000\\ldots01}_{t \\text{ digits}} \\times \\beta^e = x + \\beta^{e-t}\n\\]\nAny number \\(y \\in (x, \\tilde{x})\\) is stored as either \\(x\\) or \\(\\tilde{x}\\) by rounding to the nearest representable number, so\n\nthe largest possible error is \\(\\frac{1}{2} \\beta^{e-t}\\),\nwhich means that \\(| y - fl(y) | \\le \\frac{1}{2} \\beta^{e-t}\\).\n\nIt follow from \\(y &gt; x \\ge .100 \\ldots 00 \\times \\beta^e = \\beta^{e-1}\\) that\n\\[\n\\frac{|y - fl(y)|}{|y|} &lt; \\frac{1}{2} \\frac{\\beta^{e-t}}{\\beta^{e-1}} = \\frac{1}{2} \\beta^{1-t},\n\\]\nand this provides a bound on the relative error: for any \\(y\\)\n\\[\n\\frac{|y - fl(y)|}{|y|} &lt; \\frac{1}{2} \\beta^{1-t}.\n\\]\nThe last term is known as machine precision or unit roundoff and is often called \\(eps\\). This is obtained in Python with\n\nnp.finfo(np.double).eps\n\nnp.float64(2.220446049250313e-16)\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\nThe number system \\((\\beta, t, L, U) = (10, 2, -1, 2)\\) gives\n\\[\neps = \\frac{1}{2} \\beta^{1-t} = \\frac{1}{2} 10^{1-2} = 0.05.\n\\]\nThe number system \\((\\beta, t, L, U) = (10, 3, -3, 3)\\) gives\n\\[\neps = \\frac{1}{2} \\beta^{1-t} = \\frac{1}{2} 10^{1-3} = 0.005.\n\\]\nThe number system \\((\\beta, t, L, U) = (10, 7, 2, 10)\\) gives\n\\[\neps = \\frac{1}{2} \\beta^{1-t} = \\frac{1}{2} 10^{1-7} = 0.000005.\n\\]\n\nFor some common types in python, we see the following values:\n\nfor dtype in [np.half, np.single, np.double]:\n    print(dtype.__name__, np.finfo(dtype).eps)\n\nfloat16 0.000977\nfloat32 1.1920929e-07\nfloat64 2.220446049250313e-16\n\n\n\n\n\n\n\n\n\n\nRemark\n\n\n\nMachine precision epsilon (\\(eps\\)) gives us an upper bound for the error in the representation of a floating point number in a particular system. We note that this is different to the smallest possible numbers that we are able to store!\n\na = np.finfo(np.double).eps\n\na = a / 10.0\na\n\nnp.float64(2.2204460492503132e-17)\n\n\n\n\nArithmetic operations are usually carried out as though infinite precision is available, after which the result is rounded to the nearest representable number.\nThis means that arithmetic cannot be completely trusted\ne.g. \\(x + y = ?\\),\nand the usual rules don’t necessarily apply\ne.g. \\(x + (y+z) = (x+y) + z\\)?\n\n\n\n\n\n\nExample 1\n\n\n\nConsider the number system \\((\\beta, t, L, U) = (10, 2, -1, 2)\\) and take\n\\[\nx = .10 \\times 10^2, \\quad\ny = .49 \\times 10^0, \\quad\nz = .51 \\times 10^0.\n\\]\n\nIn exact arithmetic \\(x + y = 10 + 0.49 = 10.49\\) and \\(x + z = 10 + 0.51 = 10.51\\).\nIn this number system rounding gives\n\\[\nfl(x+y) = .10 \\times 10^2 = x, \\qquad\nfl(x+z) = .11 \\times 10^2 \\neq x.\n\\]\n\n(Note that \\(\\frac{y}{x} &lt; eps\\) but \\(\\frac{z}{x} &gt; eps\\).)\nEvaluate the following expression in this number system.\n\\[\nx+(y+y), \\quad\n(x+y)+y, \\quad\nx+(z+z), \\quad\n(x+z) +z.\n\\]\n(Also note the benefits of adding the smallest terms first!)\n\n\n\n\n\n\n\n\nExample - computing derivatives with floating point numbers\n\n\n\nSuppose we want to compute the derivative of \\(f(x) = x^3\\) at \\(x = 1\\) using the definition of limits and floating point numbers: \\[\nf'(x) = \\lim_{\\Delta x \\to 0} \\frac{f(x + \\Delta x) - f(x)}{\\Delta x}.\n\\] We know that \\(f'(x) = 3 x^2\\) so \\(f'(1) = 3\\). We hope that using floating point numbers gives something similar:\n\ndef f(x):\n    return x**3\n\n\nx0 = np.double(1.0)\n\nprint(\"Delta_x   Approx  Abs Error\")\n\nfor j in range(20):\n    Delta_x = 10 ** (-j)\n\n    deriv_approx = (f(x0 + Delta_x) - f(x0)) / Delta_x\n    abs_error = abs(3.0 - deriv_approx)\n\n    print(f\" {Delta_x:.1e}   {deriv_approx:.4f}  {abs_error:.4e}\")\n\nDelta_x   Approx  Abs Error\n 1.0e+00   7.0000  4.0000e+00\n 1.0e-01   3.3100  3.1000e-01\n 1.0e-02   3.0301  3.0100e-02\n 1.0e-03   3.0030  3.0010e-03\n 1.0e-04   3.0003  3.0001e-04\n 1.0e-05   3.0000  3.0000e-05\n 1.0e-06   3.0000  2.9998e-06\n 1.0e-07   3.0000  3.0151e-07\n 1.0e-08   3.0000  3.9720e-09\n 1.0e-09   3.0000  2.4822e-07\n 1.0e-10   3.0000  2.4822e-07\n 1.0e-11   3.0000  2.4822e-07\n 1.0e-12   3.0003  2.6670e-04\n 1.0e-13   2.9976  2.3978e-03\n 1.0e-14   2.9976  2.3978e-03\n 1.0e-15   3.3307  3.3067e-01\n 1.0e-16   0.0000  3.0000e+00\n 1.0e-17   0.0000  3.0000e+00\n 1.0e-18   0.0000  3.0000e+00\n 1.0e-19   0.0000  3.0000e+00\n\n\nWe see that if Delta_x is not too small, we do an okay job. But if Delta_x is too small we start to have problems!\n\n\n\n\n\n\n\n\nMore examples (homework)\n\n\n\n\nVerify that a similar problem arises for the numbers\n\\[\nx = .85 \\times 10^0, \\quad\ny = .3 \\times 10^{-2}, \\quad\nz = .6 \\times 10^{-2},\n\\]\nin the system \\((\\beta, t, L, U) = (10, 2, -3, 3)\\).\nGiven the number system \\((\\beta, t, L, U) = (10, 3, -3, 3)\\) and \\(x = .100\\times 10^3\\), find nonzero numbers \\(y\\) and \\(z\\) from this system for which \\(fl(x+y) = x\\) and \\(fl(x+z) &gt; x\\).\n\n\n\nIt is sometimes helpful to think of another machine precision epsilon in other way: Machine precision epsilon is the smallest positive number \\(eps\\) such that \\(1 + eps &gt; 1\\), i.e. it is half the difference between \\(1\\) and the next largest representable number.\nExamples:\n\nFor the number system \\((\\beta, t, L, U) = (10, 2, -1, 2)\\),\n\\[\n\\begin{array}{ccl}\n    & .11 \\times 10^1 & \\qquad \\leftarrow {\\text{next number}} \\\\\n  - & .10 \\times 10^1 & \\qquad \\leftarrow 1   \\\\ \\hline\n    & .01 \\times 10^1 & \\qquad \\leftarrow 0.1\n\\end{array}\n\\]\nso \\(eps = \\frac{1}{2}(0.1) = 0.05\\).\nVerify that this approaches gives the previously calculated value for \\(eps\\) in the number system given by \\((\\beta, t, L, U) = (10, 3, -3, 3)\\).",
    "crumbs": [
      "(Preliminary topic) Floating point number systems"
    ]
  },
  {
    "objectID": "src/lec01.html#other-features-of-finite-precision",
    "href": "src/lec01.html#other-features-of-finite-precision",
    "title": "(Preliminary topic) Floating point number systems",
    "section": "4 Other “Features” of finite precision",
    "text": "4 Other “Features” of finite precision\nWhen working with floating point numbers there are other things we need to worry about too!\n\nOverflow\n\nthe number is too large to be represented, e.g. multiply the largest representable number by 10. This gives inf (infinity) with numpy.doubles and is usually “fatal”.\n\nUnderflow\n\nthe number is too small to be represented, e.g. divide the smallest representable number by 10. This gives \\(0\\) and may not be immediately obvious.\n\nDivide by zero\n\ngives a result of inf, but \\(\\frac{0}{0}\\) gives nan (not a number)\n\nDivide by inf\n\ngives \\(0.0\\) with no warning",
    "crumbs": [
      "(Preliminary topic) Floating point number systems"
    ]
  },
  {
    "objectID": "src/lec01.html#is-this-all-academic",
    "href": "src/lec01.html#is-this-all-academic",
    "title": "(Preliminary topic) Floating point number systems",
    "section": "5 Is this all academic?",
    "text": "5 Is this all academic?\nNo! There are many examples of major software errors that have occurred due to programmers not understanding the issues associated with computer arithmetic…\n\nIn February 1991, a basic rounding error within software for the US Patriot missile system caused it to fail, contributing to the loss of 28 lives.\nIn June 1996, the European Space Agency’s Ariane Rocket exploded shortly after take-off: the error was due to failing to handle overflow correctly.\nIn October 2020, a driverless car drove straight into a wall due to faulty handling of a floating point error.",
    "crumbs": [
      "(Preliminary topic) Floating point number systems"
    ]
  },
  {
    "objectID": "src/lec01.html#summary",
    "href": "src/lec01.html#summary",
    "title": "(Preliminary topic) Floating point number systems",
    "section": "6 Summary",
    "text": "6 Summary\n\nThere is inaccuracy in almost all computer arithmetic.\nCare must be taken to minimise its effects, for example:\n\nadd the smallest terms in an expression first;\navoid taking the difference of two very similar terms;\neven checking whether \\(a = b\\) is dangerous!\n\nThe usual mathematical rules no longer apply.\nThere is no point in trying to compute a solution to a problem to a greater accuracy than can be stored by the computer.\n\n\n6.1 Further reading\n\nWikipedia: Floating-point arithmetic\nDavid Goldberg, What every computer scientist should know about floating-point arithmetic, ACM Computing Surveys, Volume 23, Issue 1, March 1991.\nJohn D Cook, Floating point error is the least of my worries, online, November 2011.",
    "crumbs": [
      "(Preliminary topic) Floating point number systems"
    ]
  },
  {
    "objectID": "src/lec04.html",
    "href": "src/lec04.html",
    "title": "1 Direct solvers for systems of linear equtaions",
    "section": "",
    "text": "This is the first method we will use to solve systems of linear equations. We will see that by using the approach of Gaussian elimination (and variations of that method) we can solve any system of linear equations that have a solution.\nRecall the problem is to solve a set of \\(n\\) linear equations for \\(n\\) unknown values \\(x_j\\), for \\(j=1, 2, \\ldots, n\\).\nNotation:\n\\[\n\\begin{aligned}\n\\text{Equation } 1: && a_{11} x_1 + a_{12} x_2 + a_{13} x_3 + \\cdots + a_{1n} x_n & = b_1 \\\\\n\\text{Equation } 2: && a_{21} x_1 + a_{22} x_2 + a_{23} x_3 + \\cdots + a_{2n} x_n & = b_2 \\\\\n\\vdots \\\\\n\\text{Equation } i: && a_{i1} x_1 + a_{i2} x_2 + a_{i3} x_3 + \\cdots + a_{in} x_n & = b_i \\\\\n\\vdots \\\\\n\\text{Equation } n: && a_{n1} x_1 + a_{n2} x_2 + a_{n3} x_3 + \\cdots + a_{nn} x_n & = b_n.\n\\end{aligned}\n\\]\nWe can also write the system of linear equations in general matrix-vector form:\n\\[\n\\begin{pmatrix}\na_{11} & a_{12} & a_{13} & \\cdots & a_{1n} \\\\\na_{21} & a_{22} & a_{23} & \\cdots & a_{2n} \\\\\na_{31} & a_{32} & a_{33} & \\cdots & a_{3n} \\\\\n\\vdots & \\vdots & \\vdots & & \\vdots \\\\\na_{n1} & a_{n2} & a_{n3} & \\cdots & a_{nn}\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3 \\\\ \\vdots \\\\ x_n\n\\end{pmatrix} =\n\\begin{pmatrix}\nb_1 \\\\ b_2 \\\\ b_3 \\\\ \\vdots \\\\ b_n\n\\end{pmatrix}.\n\\]\nRecall the \\(n \\times n\\) matrix \\(A\\) represents the coefficients that multiply the unknowns in each equation (row), while the \\(n\\)-vector \\(\\vec{b}\\) represents the right-hand-side values.\n\n\nConsider equation \\(p\\) of the above system:\n\\[\na_{p1} x_1 + a_{p2} x_2 + a_{p3} x_3 + \\cdots + a_{pn} x_n = b_p,\n\\]\nand equation \\(q\\):\n\\[\na_{q1} x_1 + a_{q2} x_2 + a_{q3} x_3 + \\cdots + a_{qn} x_n = b_q.\n\\]\nNote three things…\n\nThe order in which we choose to write the \\(n\\) equations is irrelevant\nWe can multiply any equation by an arbitrary real number (\\(k \\neq 0\\) say):\n\\[\nk a_{p1} x_1 + k a_{p2} x_2 + k a_{p3} x_3 + \\cdots + k a_{pn} x_n = k b_p.\n\\]\nWe can add any two equations:\n\\[\nk a_{p1} x_1 + k a_{p2} x_2 + k a_{p3} x_3 + \\cdots + k a_{pn} x_n = k b_p\n\\]\nadded to\n\\[\na_{q1} x_1 + a_{q2} x_2 + a_{q3} x_3 + \\cdots + a_{qn} x_n = b_q\n\\]\nyields\n\\[\n(k a_{p1} + a_{q1}) x_1 + (k a_{p2} + a_{q2}) x_2 + \\cdots + (k a_{pn} + a_{qn}) x_n = k b_p + b_q.\n\\]\n\n\n\n\n\n\n\nExample 1\n\n\n\nConsider the system\n\\[\\begin{align}\n2 x_1 + 3 x_2 & = 4  \\label{eq:eg1a} \\\\\n-3 x_1 + 2 x_2 & = 7 \\label{eq:eg1b}.\n\\end{align}\\]\nThen we have:\n\\[\n\\begin{aligned}\n4 \\times \\text{\\eqref{eq:eg1a}} & \\rightarrow & 8 x_1 + 12 x_2 & = 16 \\\\\n-15. \\times \\text{\\eqref{eq:eg1b}} & \\rightarrow & 4.5 x_2 - 3 x_2 & = -10.5 \\\\\n\\text{\\eqref{eq:eg1a}} + \\text{\\eqref{eq:eg1b}} & \\rightarrow & -x_1 + 5 x_2 & = 11 \\\\\n\\text{\\eqref{eq:eg1b}} + 1.5 \\times \\text{\\eqref{eq:eg1a}} & \\rightarrow & 0 + 6.5 x_2 & = 13.\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\n\nExample 2 (homework)\n\n\n\nConsider the system\n\\[\\begin{align}\nx_1 + 2 x_2 & = 1 \\label{eq:eg2a} \\\\\n4 x_1 + x_2 & = -3 \\label{eq:eg2b}.\n\\end{align}\\]\nWork out the result of these elementary row operations:\n\\[\n\\begin{aligned}\n2 \\times \\text{\\eqref{eq:eg2a}} & \\rightarrow \\\\\n0.25 \\times \\text{\\eqref{eq:eg2b}} & \\rightarrow \\\\\n\\text{\\eqref{eq:eg2b}} + (-1) \\times \\text{\\eqref{eq:eg2a}} & \\rightarrow \\\\\n\\text{\\eqref{eq:eg2b}} + (-4) \\times \\text{\\eqref{eq:eg2a}} & \\rightarrow\n\\end{aligned}\n\\]\n\n\nFor a system written in matrix form our three observations mean the following:\n\nWe can swap any two rows of the matrix (and corresponding right-hand side entries). For example:\n\\[\n\\begin{pmatrix}\n2 & 3 \\\\ -3 & 2\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2\n\\end{pmatrix} =\n\\begin{pmatrix}\n4 \\\\ 7\n\\end{pmatrix}\n\\Rightarrow\n\\begin{pmatrix}\n-3 & 2\\\\ 2 & 3\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2\n\\end{pmatrix} =\n\\begin{pmatrix}\n7 \\\\ 4\n\\end{pmatrix}\n\\]\nWe can multiply any row of the matrix (and corresponding right-hand side entry) by a scalar. For example:\n\\[\n\\begin{pmatrix}\n2 & 3 \\\\ -3 & 2\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2\n\\end{pmatrix} =\n\\begin{pmatrix}\n4 \\\\ 7\n\\end{pmatrix}\n\\Rightarrow\n\\begin{pmatrix}\n1 & \\frac{3}{2} \\\\ -3 & 2\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2\n\\end{pmatrix} =\n\\begin{pmatrix}\n2 \\\\ 7\n\\end{pmatrix}\n\\]\nWe can replace row \\(q\\) by row \\(q + k \\times\\) row \\(p\\). For example:\n\\[\n\\begin{pmatrix}\n2 & 3 \\\\ -3 & 2\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2\n\\end{pmatrix} =\n\\begin{pmatrix}\n4 \\\\ 7\n\\end{pmatrix}\n\\Rightarrow\n\\begin{pmatrix}\n2 & 3 \\\\ 0 & 6.5\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2\n\\end{pmatrix} =\n\\begin{pmatrix}\n4 \\\\ 13\n\\end{pmatrix}\n\\]\n(here we replaced row \\(w\\) by row \\(2 + 1.5 \\times\\) row \\(1\\))\n\\[\n\\begin{pmatrix}\n1 & 2 \\\\ 4 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2\n\\end{pmatrix} =\n\\begin{pmatrix}\n1 \\\\ -3\n\\end{pmatrix}\n\\Rightarrow\n\\begin{pmatrix}\n1 & 2 \\\\ 0 & -7\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2\n\\end{pmatrix} =\n\\begin{pmatrix}\n1 \\\\ -7\n\\end{pmatrix}\n\\]\n(here we replaced row \\(2\\) by row \\(2 + (-4) \\times\\) row \\(1\\))\n\nOur strategy for solving systems of linear equations using Gaussian elimination is based on the following ideas:\n\nThree types of operation described above are called elementary row operations (ERO).\nWe will applying a sequence of ERO to reduce an arbitrary system to a triangular form, which, we will see, can be easily solved.\nThe algorithm for reducing a general matrix to upper triangular form is known as forward elimination or (more commonly) as Gaussian elimination.\n\n\n\n\nThe algorithm of Gaussian elimination is a very old method that you may have already met at school - perhaps by a different name. The details of the method may seem quite confusing at first. Really we are following the ideas of eliminating systems of simultaneous equations, but in a way a computer understands. This is an important point in this section. You will have seen different ideas of how to solve systems of simulations equations where the first step is to “look at the equations to decide the easiest first step”. When there are \\(10^9\\) equations, its not effective for a computer to try and find an easy way through the problem. It must instead of a simple set of instructions to follow: this will be our algorithm.\nThe method is so old, in fact we have evidence of Chinese mathematicians using Gaussian elimination in 179CE (From Wikipedia):\n\nThe method of Gaussian elimination appears in the Chinese mathematical text Chapter Eight: Rectangular Arrays of The Nine Chapters on the Mathematical Art. Its use is illustrated in eighteen problems, with two to five equations. The first reference to the book by this title is dated to 179 CE, but parts of it were written as early as approximately 150 BCE. It was commented on by Liu Hui in the 3rd century.\nThe method in Europe stems from the notes of Isaac Newton. In 1670, he wrote that all the algebra books known to him lacked a lesson for solving simultaneous equations, which Newton then supplied. Carl Friedrich Gauss in 1810 devised a notation for symmetric elimination that was adopted in the 19th century by professional hand computers to solve the normal equations of least-squares problems. The algorithm that is taught in high school was named for Gauss only in the 1950s as a result of confusion over the history of the subject.\n\n\n\n\nImage from Nine Chapter of the Mathematical art. By Yang Hui(1238－1298) - mybook, Public Domain, https://commons.wikimedia.org/w/index.php?curid=10317744\n\n\n\n\nThe following algorithm systematically introduces zeros into the system of equations, below the diagonal.\n\nSubtract multiples of row 1 from the rows below it to eliminate (make zero) nonzero entries in column 1.\nSubtract multiplies of the new row 2 from the rows below it to eliminate nonzero entries in column 2.\nRepeat for row \\(3, 4, \\ldots, n-1\\).\n\nAfter row \\(n-1\\) all entities below the diagonal have been eliminated, so \\(A\\) is now upper triangular and the resulting system can be solved by backward substitution.\n\n\n\n\n\n\nExample 1\n\n\n\nUse Gaussian eliminate to reduce the following system of equations to upper triangular form:\n\\[\n\\begin{pmatrix}\n2 & 1 & 4 \\\\ 1 & 2 & 2 \\\\ 2 & 4 & 6\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{pmatrix} =\n\\begin{pmatrix}\n12 \\\\ 9 \\\\ 22\n\\end{pmatrix}.\n\\]\nFirst, use the first row to eliminate the first column below the diagonal:\n\n(row 2) \\(- 0.5 \\times\\) (row 1) gives\n\\[\n\\begin{pmatrix}\n2 & 1 & 4 \\\\ \\mathbf{0} & 1.5 & 0 \\\\ 2 & 4 & 6\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{pmatrix} =\n\\begin{pmatrix}\n12 \\\\ 3 \\\\ 22\n\\end{pmatrix}\n\\]\n(row 3) \\(-\\) (row 1) then gives\n\\[\n\\begin{pmatrix}\n2 & 1 & 4 \\\\ \\mathbf{0} & 1.5 & 0 \\\\ \\mathbf{0} & 3 & 2\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{pmatrix} =\n\\begin{pmatrix}\n12 \\\\ 3 \\\\ 10\n\\end{pmatrix}\n\\]\n\nNow use the second row to eliminate the second column below the diagonal.\n\n(row 3) \\(- 2 \\times\\) (row 2) gives\n\\[\n\\begin{pmatrix}\n2 & 1 & 4 \\\\ \\mathbf{0} & 1.5 & 0 \\\\ \\mathbf{0} & \\mathbf{0} & 2\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{pmatrix} =\n\\begin{pmatrix}\n12 \\\\ 3 \\\\ 4\n\\end{pmatrix}\n\\]\n\n\n\n\n\n\n\n\n\nExample 2 (homework)\n\n\n\nUse Gaussian elimination to reduce the following system of linear equations to upper triangular form.\n\\[\n\\begin{pmatrix}\n4 & -1 & -1 \\\\ 2 & 4 & 2 \\\\ 1 & 2 & 4\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{pmatrix} =\n\\begin{pmatrix}\n9 \\\\ -6 \\\\ 3\n\\end{pmatrix}.\n\\]\n\n\n\nRemark. \n\nEach row \\(i\\) is used to eliminate the entries in column \\(i\\) below \\(a_{ii}\\), i.e. it forces \\(a_{ji} = 0\\) for \\(j &gt; i\\).\nThis is done by subtracting a multiple of row \\(i\\) from row \\(j\\):\n\\[\n(\\text{row } j) \\leftarrow (\\text{row } j) - \\frac{a_{ji}}{a_{ii}} (\\text{row } i).\n\\]\nThis guarantees that \\(a_{ji}\\) becomes zero because\n\\[\na_{ji} \\leftarrow a_{ji} - \\frac{a_{ji}}{a_{ii}} a_{ii} = a_{ji} - a_{ji} = 0.\n\\]\n\n\n\n\n\n\n\n\nExample 3 (homework)\n\n\n\nSolve the system\n\\[\n\\begin{pmatrix}\n4 & 3 & 2 & 1 \\\\ 1 & 2 & 2 & 2 \\\\\n1 & 1 & 3 & 0 \\\\ 2 & 1 & 2 & 3\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3 \\\\ x_4\n\\end{pmatrix} =\n\\begin{pmatrix}\n10 \\\\ 7 \\\\ 5 \\\\ 8\n\\end{pmatrix}.\n\\] The solution is \\(\\vec{x} = (1, 1, 1, 1)^T\\).\n\n\n\n\n\nWe start with some helper code which determines the size of the system we are working with:\n\ndef system_size(A, b):\n    \"\"\"\n    for a system of linear equations, returns the size and errors if sizes are not compatible\n    \"\"\"\n    n, m = A.shape\n    nb = b.shape[0]\n\n    assert n == m and n == nb\n    return n\n\nThen we can implement the elementary row operations\n\ndef row_swap(A, b, p, q):\n    \"\"\"\n    swap the rows p and q for the system of linear equations given by Ax = b.\n    updates entries in place\n    \"\"\"\n    n = system_size(A, b)\n    # swap rows of A\n    for j in range(n):\n        A[p, j], A[q, j] = A[q, j], A[p, j]\n    # swap rows of b\n    b[p, 0], b[q, 0] = b[q, 0], b[p, 0]\n\n\ndef row_scale(A, b, p, k):\n    \"\"\"\n    scale the entries in row p by k for the system of linear equations given by Ax = b.\n    updates entries in place\n    \"\"\"\n    n = system_size(A, b)\n\n    # scale row p of A\n    for j in range(n):\n        A[p, j] = k * A[p, j]\n    # scale row p of b\n    b[p, 0] = b[p, 0] * k\n\n\ndef row_add(A, b, p, k, q):\n    \"\"\"\n    add rows for the system of linear equations given by Ax = b\n    this operation is row p |-&gt; row p + k * row q\n    updates entries in place\n    \"\"\"\n    n = system_size(A, b)\n\n    for j in range(n):\n        A[p, j] = A[p, j] + k * A[q, j]\n    b[p, 0] = b[p, 0] + k * b[q, 0]\n\nLet’s test we are ok so far:\nTest 1: swapping rows\n\nA = np.array([[2.0, 3.0], [-3.0, 2.0]])\nb = np.array([[4.0], [7.0]])\n\nprint(\"starting arrays:\")\nprint_array(A)\nprint_array(b)\nprint()\n\nprint(\"swapping rows 0 and 1\")\nrow_swap(A, b, 0, 1)  # remember numpy arrays are indexed starting from zero!\nprint()\n\nprint(\"new arrays:\")\nprint_array(A)\nprint_array(b)\nprint()\n\nstarting arrays:\nA = [  2.0,  3.0 ]\n    [ -3.0,  2.0 ]\nb = [  4.0 ]\n    [  7.0 ]\n\nswapping rows 0 and 1\n\nnew arrays:\nA = [ -3.0,  2.0 ]\n    [  2.0,  3.0 ]\nb = [  7.0 ]\n    [  4.0 ]\n\n\n\nTest 2: scaling one row\n\nA = np.array([[2.0, 3.0], [-3.0, 2.0]])\nb = np.array([[4.0], [7.0]])\n\nprint(\"starting arrays:\")\nprint_array(A)\nprint_array(b)\nprint()\n\nprint(\"row 0 |-&gt; 0.5 * row 0\")\nrow_scale(A, b, 0, 0.5)  # remember numpy arrays are indexed started from zero!\nprint()\n\nprint(\"new arrays:\")\nprint_array(A)\nprint_array(b)\nprint()\n\nstarting arrays:\nA = [  2.0,  3.0 ]\n    [ -3.0,  2.0 ]\nb = [  4.0 ]\n    [  7.0 ]\n\nrow 0 |-&gt; 0.5 * row 0\n\nnew arrays:\nA = [  1.0,  1.5 ]\n    [ -3.0,  2.0 ]\nb = [  2.0 ]\n    [  7.0 ]\n\n\n\nTest 3: replacing a row by that adding a multiple of another row\n\nA = np.array([[2.0, 3.0], [-3.0, 2.0]])\nb = np.array([[4.0], [7.0]])\n\nprint(\"starting arrays:\")\nprint_array(A)\nprint_array(b)\nprint()\n\nprint(\"row 1 |-&gt; row 1 + 1.5 * row 0\")\nrow_add(A, b, 1, 1.5, 0)  # remember numpy arrays are indexed started from zero!\nprint()\n\nprint(\"new arrays:\")\nprint_array(A)\nprint_array(b)\nprint()\n\nstarting arrays:\nA = [  2.0,  3.0 ]\n    [ -3.0,  2.0 ]\nb = [  4.0 ]\n    [  7.0 ]\n\nrow 1 |-&gt; row 1 + 1.5 * row 0\n\nnew arrays:\nA = [  2.0,  3.0 ]\n    [  0.0,  6.5 ]\nb = [  4.0 ]\n    [ 13.0 ]\n\n\n\nNow we can define our Gaussian elimination function. We update the values in-place to avoid extra memory allocations.\n\ndef gaussian_elimination(A, b, verbose=False):\n    \"\"\"\n    perform Gaussian elimnation to reduce the system of linear equations Ax=b to upper triangular form\n    use verbose to print out intermediate representations\n    \"\"\"\n    # find shape of system\n    n = system_size(A, b)\n\n    # perform forwards elimination\n    for i in range(n - 1):\n        # eliminate column i\n        if verbose:\n            print(f\"eliminating column {i}\")\n        for j in range(i + 1, n):\n            # row j\n            factor = A[j, i] / A[i, i]\n            if verbose:\n                print(f\"  row {j} |-&gt; row {j} - {factor} * row {i}\")\n            row_add(A, b, j, -factor, i)\n\n        if verbose:\n            print()\n            print(\"new system\")\n            print_array(A)\n            print_array(b)\n            print()\n\nWe can try our code on Example 1:\n\nA = np.array([[2.0, 1.0, 4.0], [1.0, 2.0, 2.0], [2.0, 4.0, 6.0]])\nb = np.array([[12.0], [9.0], [22.0]])\n\nprint(\"starting system:\")\nprint_array(A)\nprint_array(b)\nprint()\n\nprint(\"performing Gaussian Elimination\")\ngaussian_elimination(A, b, verbose=True)\nprint()\n\nprint(\"final system:\")\nprint_array(A)\nprint_array(b)\nprint()\n\n# test that A is really upper triangular\nprint(\"Is A really upper triangular?\", np.allclose(A, np.triu(A)))\n\nstarting system:\nA = [  2.0,  1.0,  4.0 ]\n    [  1.0,  2.0,  2.0 ]\n    [  2.0,  4.0,  6.0 ]\nb = [ 12.0 ]\n    [  9.0 ]\n    [ 22.0 ]\n\nperforming Gaussian Elimination\neliminating column 0\n  row 1 |-&gt; row 1 - 0.5 * row 0\n  row 2 |-&gt; row 2 - 1.0 * row 0\n\nnew system\nA = [  2.0,  1.0,  4.0 ]\n    [  0.0,  1.5,  0.0 ]\n    [  0.0,  3.0,  2.0 ]\nb = [ 12.0 ]\n    [  3.0 ]\n    [ 10.0 ]\n\neliminating column 1\n  row 2 |-&gt; row 2 - 2.0 * row 1\n\nnew system\nA = [  2.0,  1.0,  4.0 ]\n    [  0.0,  1.5,  0.0 ]\n    [  0.0,  0.0,  2.0 ]\nb = [ 12.0 ]\n    [  3.0 ]\n    [  4.0 ]\n\n\nfinal system:\nA = [  2.0,  1.0,  4.0 ]\n    [  0.0,  1.5,  0.0 ]\n    [  0.0,  0.0,  2.0 ]\nb = [ 12.0 ]\n    [  3.0 ]\n    [  4.0 ]\n\nIs A really upper triangular? True\n\n\n\n\n\n\nA general lower triangular system of equations has \\(a_{ij} = 0\\) for \\(j &gt; i\\) and takes the form:\n\\[\n\\begin{pmatrix}\na_{11} & 0 & 0 & \\cdots & 0 \\\\\na_{21} & a_{22} & 0 & \\cdots & 0 \\\\\na_{31} & a_{32} & a_{33} & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{n1} & a_{n2} & a_{n3} & \\cdots & a_{nn}\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3 \\\\ \\vdots \\\\ x_n\n\\end{pmatrix} =\n\\begin{pmatrix}\nb_1 \\\\ b_2 \\\\ b_3 \\\\ \\vdots \\\\ b_n\n\\end{pmatrix}.\n\\]\nNote the first equation is\n\\[\na_{11} x_1 = b_1.\n\\]\nThen \\(x_i\\) can be found by calculating\n\\[\nx_i = \\frac{1}{a_{ii}} \\left(b_i - \\sum_{j=1}^{i-1} a_{ij} x_j \\right)\n\\]\nfor each row \\(i = 1, 2, \\ldots, n\\) in turn.\n\nEach calculation requires only previously computed values \\(x_j\\) (and the sum gives a loop for \\(j &lt; i\\).\nThe matrix \\(A\\) must have nonzero diagonal entries\ni.e. \\(a_{ii} \\neq 0\\) for \\(i = 1, 2, \\ldots, n\\).\nUpper triangular systems of equations can be solved in a similar manner.\n\n\n\n\n\n\n\nExample 1\n\n\n\nSolve the lower triangular system of equations given by\n\\[\n\\begin{aligned}\n2 x_1 && && &= 2 \\\\\nx_1 &+& 2 x_2 && &= 7 \\\\\n2 x_1 &+& 4 x_2 &+& 6 x_3 &= 26\n\\end{aligned}\n\\]\nor, equivalently,\n\\[\n\\begin{pmatrix}\n2 & 0 & 0 \\\\\n1 & 2 & 0 \\\\\n2 & 4 & 6\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{pmatrix} =\n\\begin{pmatrix}\n2 \\\\ 7 \\\\ 26\n\\end{pmatrix}.\n\\]\nThe solution can be calculated systematically from\n\\[\n\\begin{aligned}\nx_1 &= \\frac{b_1}{a_{11}} = \\frac{2}{2} = 1 \\\\\nx_2 &= \\frac{b_2 - a_{21} x_1}{a_{22}}\n= \\frac{7 - 1 \\times 1}{2} = \\frac{6}{2} = 3 \\\\\nx_3 &= \\frac{b_3 - a_{31} x_1 - a_{32} x_2}{a_33}\n= \\frac{26 - 2 \\times 1 - 4 \\times 3}{6}  = \\frac{12}{6}\n= 2\n\\end{aligned}\n\\]\nwhich gives the solution \\(\\vec{x} = (1, 3, 2)^T\\).\n\n\n\n\n\n\n\n\nExample 2 (homework)\n\n\n\nSolve the upper triangular linear system given by\n\\[\n\\begin{aligned}\n2 x_1 &+& x_2 &+& 4 x_3 &=& 12 \\\\\n&& 1.5 x_2 && &=& 3 \\\\\n&& && 2 x_3 &=& 4\n\\end{aligned}.\n\\]\n\n\n\nRemark. \n\nIt is simple to solve a lower (upper) triangular system of equations (provided the diagonal is nonzero).\nThis process is often referred to as forward (backward) substitution.\nA general system of equations (i.e. a full matrix \\(A\\)) can be solved rapidly once it has been reduced to upper triangular form. This is the idea of using Gaussian elimination with backward subsitution.\n\n\nWe can define functions to solve both upper or lower triangular form systems of linear equations:\n\ndef forward_substitution(A, b):\n    \"\"\"\n    solves the system of linear equationa Ax = b assuming that A is lower triangular\n    returns the solution x\n    \"\"\"\n    # get size of system\n    n = system_size(A, b)\n\n    # check is lower triangular\n    assert np.allclose(A, np.tril(A))\n\n    # create solution variable\n    x = np.empty_like(b)\n\n    # perform forwards solve\n    for i in range(n):\n        partial_sum = 0.0\n        for j in range(0, i):\n            partial_sum += A[i, j] * x[j]\n        x[i] = 1.0 / A[i, i] * (b[i] - partial_sum)\n\n    return x\n\n\ndef backward_substitution(A, b):\n    \"\"\"\n    solves the system of linear equationa Ax = b assuming that A is upper triangular\n    returns the solution x\n    \"\"\"\n    # get size of system\n    n = system_size(A, b)\n\n    # check is upper triangular\n    assert np.allclose(A, np.triu(A))\n\n    # create solution variable\n    x = np.empty_like(b)\n\n    # perform backwards solve\n    for i in range(n - 1, -1, -1):  # iterate over rows backwards\n        partial_sum = 0.0\n        for j in range(i + 1, n):\n            partial_sum += A[i, j] * x[j]\n        x[i] = 1.0 / A[i, i] * (b[i] - partial_sum)\n\n    return x\n\nAnd we can then test it out!\n\nA = np.array([[2.0, 0.0, 0.0], [1.0, 2.0, 0.0], [2.0, 4.0, 6.0]])\nb = np.array([[2.0], [7.0], [26.0]])\n\nprint(\"The system is given by:\")\nprint_array(A)\nprint_array(b)\nprint()\n\nprint(\"Solving the system using forward substitution\")\nx = forward_substitution(A, b)\nprint()\n\nprint(\"The solution using forward substitution is:\")\nprint_array(x)\nprint()\n\nprint(\"Does x really solve the system?\", np.allclose(A @ x, b))\n\nThe system is given by:\nA = [  2.0,  0.0,  0.0 ]\n    [  1.0,  2.0,  0.0 ]\n    [  2.0,  4.0,  6.0 ]\nb = [  2.0 ]\n    [  7.0 ]\n    [ 26.0 ]\n\nSolving the system using forward substitution\n\nThe solution using forward substitution is:\nx = [  1.0 ]\n    [  3.0 ]\n    [  2.0 ]\n\nDoes x really solve the system? True\n\n\nWe can also do a backward substitution test:\n\nA = np.array([[2.0, 1.0, 4.0], [0.0, 1.5, 0.0], [0.0, 0.0, 2.0]])\nb = np.array([[12.0], [3.0], [4.0]])\n\nprint(\"The system is given by:\")\nprint_array(A)\nprint_array(b)\nprint()\n\nprint(\"Solving the system using backward substitution\")\nx = backward_substitution(A, b)\nprint()\n\nprint(\"The solution using backward substitution is:\")\nprint_array(x)\nprint()\n\n\nprint(\"Does x really solve the system?\", np.allclose(A @ x, b))\n\nThe system is given by:\nA = [  2.0,  1.0,  4.0 ]\n    [  0.0,  1.5,  0.0 ]\n    [  0.0,  0.0,  2.0 ]\nb = [ 12.0 ]\n    [  3.0 ]\n    [  4.0 ]\n\nSolving the system using backward substitution\n\nThe solution using backward substitution is:\nx = [  1.0 ]\n    [  2.0 ]\n    [  2.0 ]\n\nDoes x really solve the system? True\n\n\n\n\n\nOur grant strategy can now come together so we have a method to solve systems of linear equations:\nGiven a system of linear equations \\(A\\vec{x} = \\vec{b}\\);\n\nFirst perform Gaussian elimination to give an equivalent system of equations in upper triangular form;\nThen use backward substitution to produce a solution \\(\\vec{x}\\)\n\nWe can use our code to test this:\n\nA = np.array([[2.0, 1.0, 4.0], [1.0, 2.0, 2.0], [2.0, 4.0, 6.0]])\nb = np.array([[12.0], [9.0], [22.0]])\n\nprint(\"starting system:\")\nprint_array(A)\nprint_array(b)\nprint()\n\nprint(\"performing Gaussian Elimination\")\ngaussian_elimination(A, b, verbose=True)\nprint()\n\nprint(\"upper triangular system:\")\nprint_array(A)\nprint_array(b)\nprint()\n\nprint(\"Solving the system using backward substitution\")\nx = backward_substitution(A, b)\nprint()\n\nprint(\"solution using backward substitution:\")\nprint_array(x)\nprint()\n\nA = np.array([[2.0, 1.0, 4.0], [1.0, 2.0, 2.0], [2.0, 4.0, 6.0]])\nb = np.array([[12.0], [9.0], [22.0]])\nprint(\"Does x really solve the original system?\", np.allclose(A @ x, b))\n\nstarting system:\nA = [  2.0,  1.0,  4.0 ]\n    [  1.0,  2.0,  2.0 ]\n    [  2.0,  4.0,  6.0 ]\nb = [ 12.0 ]\n    [  9.0 ]\n    [ 22.0 ]\n\nperforming Gaussian Elimination\neliminating column 0\n  row 1 |-&gt; row 1 - 0.5 * row 0\n  row 2 |-&gt; row 2 - 1.0 * row 0\n\nnew system\nA = [  2.0,  1.0,  4.0 ]\n    [  0.0,  1.5,  0.0 ]\n    [  0.0,  3.0,  2.0 ]\nb = [ 12.0 ]\n    [  3.0 ]\n    [ 10.0 ]\n\neliminating column 1\n  row 2 |-&gt; row 2 - 2.0 * row 1\n\nnew system\nA = [  2.0,  1.0,  4.0 ]\n    [  0.0,  1.5,  0.0 ]\n    [  0.0,  0.0,  2.0 ]\nb = [ 12.0 ]\n    [  3.0 ]\n    [  4.0 ]\n\n\nupper triangular system:\nA = [  2.0,  1.0,  4.0 ]\n    [  0.0,  1.5,  0.0 ]\n    [  0.0,  0.0,  2.0 ]\nb = [ 12.0 ]\n    [  3.0 ]\n    [  4.0 ]\n\nSolving the system using backward substitution\n\nsolution using backward substitution:\nx = [  1.0 ]\n    [  2.0 ]\n    [  2.0 ]\n\nDoes x really solve the original system? True\n\n\n\n\n\nGaussian elimination (GE) is unnecessarily expensive when it is applied to many systems of equations with the same matrix \\(A\\) but different right-hand sides \\(\\vec{b}\\).\n\nThe forward elimination process is the most computationally expensive part at \\(O(n^3)\\) but is exactly the same for any choice of \\(\\vec{b}\\).\nIn contrast, the solution of the resulting upper triangular system only requires \\(O(n^2)\\) operations.\n\n\n\n\n\n\n\n\n\n\nWe can use this information to improve the way in which we solve multiple systems of equations with the same matrix \\(A\\) but different right-hand sides \\(\\vec{b}\\).\n\n\n\nOur next algorithm, called LU factorisation, is a way to try to speed up Gaussian elimination by reusing information. This can be used when we solve systems of equations with the same matrix \\(A\\) but different right hand sides \\(\\vec{b}\\) - this is more common than you would think!\nRecall the elementary row operations (EROs) from above. Note that the EROs can be produced by left multiplication with a suitable matrix:\n\nRow swap:\n\\[\n\\begin{pmatrix}\n1 & 0 & 0 \\\\ 0 & 0 & 1 \\\\ 0 & 1 & 0\n\\end{pmatrix}\n\\begin{pmatrix}\na & b & c \\\\ d & e & f \\\\ g & h & i\n\\end{pmatrix}\n=\n\\begin{pmatrix}\na & b & c \\\\ g & h & i \\\\ d & e & f\n\\end{pmatrix}\n\\]\nRow swap:\n\\[\n\\begin{pmatrix}\n1 & 0 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\na & b & c & d \\\\ e & f & g & h \\\\ i & j & k & l \\\\ m & n & o & p\n\\end{pmatrix}\n=\n\\begin{pmatrix}\na & b & c & d \\\\ i & j & k & l \\\\ e & f & g & h \\\\ m & n & o & p\n\\end{pmatrix}\n\\]\nMultiply row by \\(\\alpha\\):\n\\[\n\\begin{pmatrix}\n\\alpha & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\na & b & c \\\\ d & e & f \\\\ g & h & i\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\alpha a & \\alpha b & \\alpha c \\\\ d & e & f \\\\ g & h & i\n\\end{pmatrix}\n\\]\n\\(\\alpha \\times \\text{row } p + \\text{row } q\\):\n\\[\n\\begin{pmatrix}\n1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ \\alpha & 0 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\na & b & c \\\\ d & e & f \\\\ g & h & i\n\\end{pmatrix}\n=\n\\begin{pmatrix}\na & b & c \\\\ d & e & f \\\\ \\alpha a + g & \\alpha b + h & \\alpha c + i\n\\end{pmatrix}\n\\]\n\nSince Gaussian elimination (GE) is just a sequence of EROs and each ERO just multiplication by a suitable matrix, say \\(E_k\\), forward elimination applied to the system \\(A \\vec{x} = \\vec{b}\\) can be expressed as \\[\n(E_m \\cdots E_1) A \\vec{x} = (E_m \\cdots E_1) \\vec{b},\n\\] here \\(m\\) is the number of EROs required to reduce the upper triangular form.\nLet \\(U = (E_m \\cdots E_1) A\\) and \\(L = (E_m \\cdots E_1)^{-1}\\). Now the original system \\(A \\vec{x} = \\vec{b}\\) is equivalent to\n\\[\\begin{equation}\n\\label{eq:LU}\nL U \\vec{x} = \\vec{b}\n\\end{equation}\\]\nwhere \\(U\\) is upper triangular (by construction) and \\(L\\) may be shown to be lower triangular (provided the EROs do not include any row swaps).\nOnce \\(L\\) and \\(U\\) are known it is easy to solve \\(\\eqref{eq:LU}\\)\n\nSolve \\(L \\vec{z} = \\vec{b}\\) in \\(O(n^2)\\) operations.\nSolve \\(U \\vec{x} = \\vec{z}\\) in \\(O(n^2)\\) operations.\n\n\\(L\\) and \\(U\\) may be found in \\(O(n^3)\\) operations by performing GE and saving the \\(E_i\\) matrices, however it is more convenient to find them directly (also \\(O(n^3)\\) operations).\n\n\nConsider a general \\(4 \\times 4\\) matrix \\(A\\) and its factorisation \\(LU\\):\n\\[\n\\begin{pmatrix}\na_{11} & a_{12} & a_{13} & a_{14} \\\\\na_{21} & a_{22} & a_{23} & a_{24} \\\\\na_{31} & a_{32} & a_{33} & a_{34} \\\\\na_{41} & a_{42} & a_{43} & a_{44}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n1 & 0 & 0 & 0 \\\\\nl_{21} & 1 & 0 & 0 \\\\\nl_{31} & l_{32} & 1 & 0 \\\\\nl_{41} & l_{42} & l_{43} & 1\n\\end{pmatrix}\n\\begin{pmatrix}\nu_{11} & u_{12} & u_{13} & u_{14} \\\\\n0 & u_{22} & u_{23} & u_{24} \\\\\n0 & 0 & u_{33} & u_{34} \\\\\n0 & 0 & 0 & u_{44}\n\\end{pmatrix}\n\\]\nFor the first column,\n\\[\n\\begin{aligned}\na_{11} & = (1, 0, 0, 0) (u_{11}, 0, 0, 0)^T && = u_{11}\n& \\rightarrow u_{11} & = a_{11} \\\\\na_{21} & = (l_{21}, 1, 0, 0)(u_{11}, 0, 0, 0)^T && = l_{21} u_{11}\n& \\rightarrow l_{21} & = a_{21} / u_{11} \\\\\na_{31} & = (l_{31}, l_{32}, 1, 0)(u_{11}, 0, 0, 0)^T && = l_{31} u_{11}\n& \\rightarrow l_{31} & = a_{31} / u_{11} \\\\\na_{41} & = (l_{41}, l_{42}, l_{43}, 1)(u_{11}, 0, 0, 0)^T && = l_{41} u_{11}\n& \\rightarrow l_{41} & = a_{41} / u_{11}\n\\end{aligned}\n\\]\nThe second, third and fourth columns follow in a similar manner, giving all the entries in \\(L\\) and \\(U\\).\n\nRemark. \n\n\\(L\\) is assumed to have 1’s on the diagonal, to ensure that the factorisation is unique.\nThe process involves division by the diagonal entries \\(u_{11}, u_{22}\\), etc., so they must be non-zero.\nIn general the factors \\(l_{ij}\\) and \\(u_{ij}\\) are calculated for each column \\(j\\) in turn, i.e.,\nfor j in range(n):\n  for i in range(j+1):\n      # Compute factors u_{ij}\n      ...\n  for i in range(j+1, n):\n      # Compute factors l_{ij}\n      ...\n\n\n\n\n\n\n\n\nExample 1\n\n\n\nUse \\(LU\\) factorisation to solve the linear system of equations given by\n\\[\n\\begin{pmatrix}\n2 & 1 & 4 \\\\\n1 & 2 & 2 \\\\\n2 & 4 & 6\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n12 \\\\ 9 \\\\ 22\n\\end{pmatrix}.\n\\]\nThis can be rewritten in the form \\(A = LU\\) where\n\\[\n\\begin{pmatrix}\n2 & 1 & 4 \\\\\n1 & 2 & 2 \\\\\n2 & 4 & 6\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n1 & 0 & 0 \\\\\nl_{21} & 1 & 0 \\\\\nl_{31} & l_{32} & 1\n\\end{pmatrix}\n\\begin{pmatrix}\nu_{11} & u_{12} & u_{13} \\\\\n0 & u_{22} & u_{23} \\\\\n0 & 0 & u_{33}\n\\end{pmatrix}.\n\\]\nColumn 1 of \\(A\\) gives\n\\[\n\\begin{aligned}\n2 & = u_{11} && \\rightarrow & u_{11} & = 2 \\\\\n1 & = l_{21} u_{11} && \\rightarrow & l_{21} & = 0.5 \\\\\n2 & = l_{31} u_{11} && \\rightarrow & l_{31} & = 1.\n\\end{aligned}\n\\]\nColumn 2 of \\(A\\) gives\n\\[\n\\begin{aligned}\n1 & = u_{12} && \\rightarrow & u_{12} & = 1 \\\\\n2 & = l_{21} u_{12} + u_{22} && \\rightarrow & u_{22} & = 1.5 \\\\\n4 & = l_{31} u_{12} + l_{32} u_{22} && \\rightarrow & l_{32} & = 2.\n\\end{aligned}\n\\]\nColumn 3 of \\(A\\) gives\n\\[\n\\begin{aligned}\n4 & = u_{13} && \\rightarrow & u_{13} & = 4 \\\\\n2 & = l_{21} u_{13} + u_{23} && \\rightarrow & u_{23} & = 0 \\\\\n6 & = l_{31} u_{13} + l_{32} u_{23} + u_{33} && \\rightarrow & u_{33} & = 2.\n\\end{aligned}\n\\]\nSolve the lower triangular system \\(L \\vec{z} = \\vec{b}\\):\n\\[\n\\begin{pmatrix}\n1 & 0 & 0 \\\\\n0.5 & 1 & 0 \\\\\n1 & 2 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\nz_1 \\\\ z_2 \\\\ z_3\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n12 \\\\ 9 \\\\ 22\n\\end{pmatrix}\n\\rightarrow\n\\begin{pmatrix}\nz_1 \\\\ z_2 \\\\ z_3\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n12 \\\\ 3 \\\\ 4\n\\end{pmatrix}\n\\]\nSolve the upper triangular system \\(U \\vec{x} = \\vec{z}\\):\n\\[\n\\begin{pmatrix}\n2 & 1 & 4 \\\\\n0 & 1.5 & 0 \\\\\n0 & 0 & 2\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{pmatrix}\n\\begin{pmatrix}\n12 \\\\ 3 \\\\ 4\n\\end{pmatrix}\n\\rightarrow\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n1 \\\\ 2 \\\\ 2\n\\end{pmatrix}.\n\\]\n\n\n\n\n\n\n\n\nExample 2 (homework)\n\n\n\nRewrite the matrix \\(A\\) as the product of lower and upper triangular matrices where\n\\[\nA =\n\\begin{pmatrix}\n4 & 2 & 0 \\\\\n2 & 3 & 1 \\\\\n0 & 1 & 2.5\n\\end{pmatrix}.\n\\]\n\n\n\nRemark. The first example gives\n\\[\n\\begin{pmatrix}\n2 & 1 & 4 \\\\\n1 & 2 & 2 \\\\\n2 & 4 & 6\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n1 & 0 & 0 \\\\\n0.5 & 1 & 0 \\\\\n1 & 2 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\n2 & 1 & 4 \\\\\n0 & 1.5 & 0 \\\\\n0 & 0 & 2\n\\end{pmatrix}\n\\]\nNote that\n\nthe matrix \\(U\\) is the same as the fully eliminated upper triangular form produced by Gaussian elimination;\n\\(L\\) contains the multipliers that were used at each stage to eliminate the rows.\n\n\n\n\n\nWe can implement computation of the LU factorisation:\n\ndef lu_factorisation(A):\n    \"\"\"\n    compute the LU factorisation of A\n    returns the factors L and U\n    \"\"\"\n    n, m = A.shape\n    assert n == m\n\n    # construct arrays of zeros\n    L, U = np.zeros_like(A), np.zeros_like(A)\n\n    # fill entries\n    for i in range(n):\n        L[i, i] = 1\n        # compute entries in U\n        for j in range(i, n):\n            U[i, j] = A[i, j] - sum(L[i, k] * U[k, j] for k in range(i))\n        # compute entries in L\n        for j in range(i + 1, n):\n            L[j, i] = (A[j, i] - sum(L[j, k] * U[k, i] for k in range(i))) / U[i, i]\n\n    return L, U\n\nand test our implementation:\n\nA = np.array([[2.0, 1.0, 4.0], [1.0, 2.0, 2.0], [2.0, 4.0, 6.0]])\n\nprint(\"matrix:\")\nprint_array(A)\nprint()\n\nprint(\"performing factorisation\")\nL, U = lu_factorisation(A)\nprint()\n\nprint(\"factorisation:\")\nprint_array(L)\nprint_array(U)\nprint()\n\nprint(\"Is L lower triangular?\", np.allclose(L, np.tril(L)))\nprint(\"Is U lower triangular?\", np.allclose(U, np.triu(U)))\nprint(\"Is LU a factorisation of A?\", np.allclose(L @ U, A))\n\nmatrix:\nA = [  2.0,  1.0,  4.0 ]\n    [  1.0,  2.0,  2.0 ]\n    [  2.0,  4.0,  6.0 ]\n\nperforming factorisation\n\nfactorisation:\nL = [  1.0,  0.0,  0.0 ]\n    [  0.5,  1.0,  0.0 ]\n    [  1.0,  2.0,  1.0 ]\nU = [  2.0,  1.0,  4.0 ]\n    [  0.0,  1.5,  0.0 ]\n    [  0.0,  0.0,  2.0 ]\n\nIs L lower triangular? True\nIs U lower triangular? True\nIs LU a factorisation of A? True\n\n\nand then add LU factorisation times to our plot:\n\n\n\n\n\n\n\n\n\nWe see that LU factorisation is still \\(O(n^3)\\) and that the run times are similar to Gaussian elimination. But, importantly, we can reuse this factorisation more cheaply for different right-hand sides \\(\\vec{b}\\).\n\n\n\n\n\n\n\n\n\n\nExample 1\n\n\n\nConsider the following linear system of equations\n\\[\n\\begin{pmatrix}\n0 & 2 & 1 \\\\\n2 & 1 & 0 \\\\\n1 & 2 & 0\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n7 \\\\ 4 \\\\ 5\n\\end{pmatrix}\n\\]\nProblem. We cannot eliminate the first column by the diagonal by adding multiples of row 1 to rows 2 and 3 respectively.\nSolution. Swap the order of the equations!\n\nSwap rows 1 and 2:\n\\[\n\\begin{pmatrix}\n2 & 1 & 0 \\\\\n0 & 2 & 1 \\\\\n1 & 2 & 0\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n4 \\\\ 7 \\\\ 5\n\\end{pmatrix}\n\\]\nNow apply Gaussian elimination\n\\[\n\\begin{pmatrix}\n2 & 1 & 0 \\\\\n0 & 2 & 1 \\\\\n0 & 1.5 & 0\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n4 \\\\ 7 \\\\ 3\n\\end{pmatrix}\n;\n\\begin{pmatrix}\n2 & 1 & 0 \\\\\n0 & 2 & 1 \\\\\n0 & 0 & -0.75\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n4 \\\\ 7 \\\\ -2.25\n\\end{pmatrix}.\n\\]\n\n\n\n\n\n\n\n\n\nExample 2\n\n\n\nConsider another system of equations\n\\[\n\\begin{pmatrix}\n2 & 1 & 1 \\\\\n4 & 2 & 1 \\\\\n2 & 2 & 0\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n3 \\\\ 5 \\\\ 2\n\\end{pmatrix}\n\\]\n\nApply Gaussian elimination as usual:\n\\[\n\\begin{pmatrix}\n2 & 1 & 1 \\\\\n0 & 0 & -1 \\\\\n2 & 2 & 0\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n3 \\\\ -1 \\\\ 2\n\\end{pmatrix}\n;\n\\begin{pmatrix}\n2 & 1 & 1 \\\\\n0 & 0 & -1 \\\\\n0 & 1 & -1\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n3 \\\\ -1 \\\\ -1\n\\end{pmatrix}\n\\]\nProblem. We cannot eliminate the second column below the diagonal by adding a multiple of row 2 to row 3.\nAgain this problem may be overcome simply by swapping the order of the equations - this time swapping rows 2 and 3:\n\\[\n\\begin{pmatrix}\n2 & 1 & 1 \\\\\n0 & 1 & -1 \\\\\n0 & 0 & -1\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n3 \\\\ -1 \\\\ -1\n\\end{pmatrix}\n\\]\nWe can now continue the Gaussian elimination process as usual.\n\n\n\nIn general. Gaussian elimination requires row swaps to avoid breaking down when there is a zero in the “pivot” position. This might be a familiar aspect of Gaussian elimination, but there is an additional reason to apply pivoting when working with floating point numbers:\n\n\n\n\n\n\nExample 3\n\n\n\nConsider using Gaussian elimination to solve the linear system of equations given by\n\\[\n\\begin{pmatrix}\n\\varepsilon & 1 \\\\\n1 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n2 + \\varepsilon \\\\ 3\n\\end{pmatrix}\n\\]\nwhere \\(\\varepsilon \\neq 1\\).\n\nThe true, unique solution is \\((x_1, x_2)^T = (1, 2)^T\\).\nIf \\(\\varepsilon \\neq 0\\), Gaussian elimination gives\n\\[\n\\begin{pmatrix}\n\\varepsilon & 1 \\\\\n0 & 1 - \\frac{1}{\\varepsilon}\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n2 + \\varepsilon \\\\ 3 - \\frac{2 + \\varepsilon}{\\varepsilon}\n\\end{pmatrix}\n\\]\nProblems occur not only when \\(\\varepsilon = 0\\) but also when it is very small, i.e. when \\(\\frac{1}{\\varepsilon}\\) is very large, this will introduce very significant rounding errors into the computation.\n\nUse Gaussian elimination to solve the linear system of equations given by\n\\[\n\\begin{pmatrix}\n1 & 1 \\\\\n\\varepsilon & 1\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n3 \\\\ 2 + \\varepsilon\n\\end{pmatrix}\n\\]\nwhere \\(\\varepsilon \\neq 1\\).\n\nThe true solution is still \\((x_1, x_2)^T = (1, 2)^T\\).\nGaussian elimination now gives\n\\[\n\\begin{pmatrix}\n1 & 1 \\\\\n0 & 1 - \\varepsilon\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n3 \\\\ 2 - 2\\varepsilon\n\\end{pmatrix}\n\\]\nThe problems due to small values of \\(\\varepsilon\\) have disappeared.\n\nThis is a genuine problem we see in the code versions too!\n\nprint(\"without row swapping:\")\nfor eps in [1.0e-2, 1.0e-4, 1.0e-6, 1.0e-8, 1.0e-10, 1.0e-12, 1.0e-14]:\n    A = np.array([[eps, 1.0], [1.0, 1.0]])\n    b = np.array([[2.0 + eps], [3.0]])\n\n    gaussian_elimination(A, b)\n    x = backward_substitution(A, b)\n    print(f\"{eps=:.1e}\", end=\", \")\n    print_array(x.T, \"x.T\", end=\", \")\n\n    A = np.array([[eps, 1.0], [1.0, 1.0]])\n    b = np.array([[2.0 + eps], [3.0]])\n    print(\"Solution?\", np.allclose(A @ x, b))\nprint()\n\nprint(\"with row swapping:\")\nfor eps in [1.0e-2, 1.0e-4, 1.0e-6, 1.0e-8, 1.0e-10, 1.0e-12, 1.0e-14]:\n    A = np.array([[1.0, 1.0], [eps, 1.0]])\n    b = np.array([[3.0], [2.0 + eps]])\n\n    gaussian_elimination(A, b)\n    x = backward_substitution(A, b)\n    print(f\"{eps=:.1e}\", end=\", \")\n    print_array(x.T, \"x.T\", end=\", \")\n\n    A = np.array([[1.0, 1.0], [eps, 1.0]])\n    b = np.array([[3.0], [2.0 + eps]])\n    print(\"Solution?\", np.allclose(A @ x, b))\nprint()\n\nwithout row swapping:\neps=1.0e-02, x.T = [  1.0,  2.0 ], Solution? True\neps=1.0e-04, x.T = [  1.0,  2.0 ], Solution? True\neps=1.0e-06, x.T = [  1.0,  2.0 ], Solution? True\neps=1.0e-08, x.T = [  1.0,  2.0 ], Solution? True\neps=1.0e-10, x.T = [  1.0,  2.0 ], Solution? True\neps=1.0e-12, x.T = [  1.00009,  2.00000 ], Solution? False\neps=1.0e-14, x.T = [  1.0214,  2.0000 ], Solution? False\n\nwith row swapping:\neps=1.0e-02, x.T = [  1.0,  2.0 ], Solution? True\neps=1.0e-04, x.T = [  1.0,  2.0 ], Solution? True\neps=1.0e-06, x.T = [  1.0,  2.0 ], Solution? True\neps=1.0e-08, x.T = [  1.0,  2.0 ], Solution? True\neps=1.0e-10, x.T = [  1.0,  2.0 ], Solution? True\neps=1.0e-12, x.T = [  1.0,  2.0 ], Solution? True\neps=1.0e-14, x.T = [  1.0,  2.0 ], Solution? True\n\n\n\n\n\n\nRemark. \n\nWriting the equations in a different order has removed the previous problem.\nThe diagonal entries are now always relatively larger.\nThe interchange of the order of equations is a simple example of row pivoting. This strategy avoids excessive rounding errors in the computations.\n\n\n\n\n\nBefore eliminating entries in column \\(j\\):\n\nfind the entry in column \\(j\\), below the diagonal, of maximum magnitude;\nif that entry is larger in magnitude than the diagonal entry then swap its row with row \\(j\\).\n\nThen eliminate column \\(j\\) as before.\n\nThis algorithm will always work when the matrix \\(A\\) is invertible/non-singular. Conversely, if all of the possible pivot values are zero this implies that the matrix is singular and a unique solution does not exist. At each elimination step the row multiplies used are guaranteed to be at most one in magnitude so any errors in the representation of the system cannot be amplified by the elimination process. As always, solving \\(A \\vec{x} = \\vec{b}\\) requires that the entries in \\(\\vec{b}\\) are also swapped in the appropriate way. Pivoting can be applied in an equivalent way to LU factorisation. The sequence of pivots is independent of the vector \\(\\vec{b}\\) and can be recorded and reused. The constraint imposed on the row multipliers means that for LU factorisation every entry in \\(L\\) satisfies \\(| l_{ij} | \\le 1\\).\n\n\n\n\n\n\nExample\n\n\n\nConsider the linear system of equations given by\n\\[\n\\begin{pmatrix}\n10 & -7 & 0 \\\\\n-3 & 2.1 - \\varepsilon & 6 \\\\\n5 & -1 & 5\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n7 \\\\ 9.9 + \\varepsilon \\\\ 11\n\\end{pmatrix}\n\\]\nwhere \\(0 \\le \\varepsilon \\ll 1\\), and solve it using\n\nGaussian elimination without pivoting\nGaussian elimination with pivoting.\n\nThe exact solution is \\(\\vec{x} = (0, -1, 2)^T\\) for any \\(\\varepsilon\\) in the given range.\n1. Solve the system using Gaussian elimination with no pivoting.\nEliminating the first column gives\n\\[\n\\begin{pmatrix}\n10 & -7 & 0 \\\\\n0 & -\\varepsilon & 6 \\\\\n0 & 2.5 & 5\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n7 \\\\ 12 + \\varepsilon \\\\ 7.5\n\\end{pmatrix}\n\\]\nand then the second column gives\n\\[\n\\begin{pmatrix}\n10 & -7 & 0 \\\\\n0 & -\\varepsilon & 6 \\\\\n0 & 0 & 5 + 15/\\varepsilon\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n7 \\\\ 12 + \\varepsilon \\\\ 7.5 + 2.5(12 + \\varepsilon)/\\varepsilon\n\\end{pmatrix}\n\\]\nwhich leads to\n\\[\nx_3 = \\frac{3 + \\frac{12 + \\varepsilon}{\\varepsilon}}{2 + \\frac{6}{\\varepsilon}} \\qquad\nx_2 = \\frac{(12 + \\varepsilon) - 6x_3}{-\\varepsilon} \\qquad\nx_1 = \\frac{7+ 7x_2}{10}.\n\\]\nThere are many divisions by \\(\\varepsilon\\), so we will have problems if \\(\\varepsilon\\) is (very) small.\n2. Solve the system using Gaussian elimination with pivoting.\nThe first stage is identical (because \\(a_{11} = 10\\) is largest).\n\\[\n\\begin{pmatrix}\n10 & -7 & 0 \\\\\n0 & -\\varepsilon & 6 \\\\\n0 & 2.5 & 5\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n7 \\\\ 12 + \\varepsilon \\\\ 7.5\n\\end{pmatrix}\n\\]\nbut now \\(|a_{22}| = \\varepsilon\\) and \\(|a_{32}| = 2.5\\) so we swap rows 2 and 3 to give\n\\[\n\\begin{pmatrix}\n10 & -7 & 0 \\\\\n0 & 2.5 & 5 \\\\\n0 & -\\varepsilon & 6\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n7 \\\\ 7.5 \\\\ 12 + \\varepsilon\n\\end{pmatrix}\n\\]\nNow we may eliminate column 2:\n\\[\n\\begin{pmatrix}\n10 & -7 & 0 \\\\\n0 & 2.5 & 5 \\\\\n0 & 0 & 6 + 2 \\varepsilon\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n7 \\\\ 7.5 \\\\ 12 + 4 \\varepsilon\n\\end{pmatrix}\n\\]\nwhich leads to the exact answer:\n\\[\nx_3 = \\frac{12 + 4\\varepsilon}{6 + 2 \\varepsilon} = 2 \\qquad\nx_2 = \\frac{7.5 - 5x_3}{2.5} = -1 \\qquad\nx_1 = \\frac{7 + 7 x_2}{10} = 0.\n\\]\n\n\n\n\n\n\ndef gaussian_elimination_with_pivoting(A, b, verbose=False):\n    \"\"\"\n    perform Gaussian elimnation with pivoting to reduce the system of linear equations Ax=b to upper triangular form\n    use verbose to print out intermediate representations\n    \"\"\"\n    # find shape of system\n    n = system_size(A, b)\n\n    # perform forwards elimination\n    for i in range(n - 1):\n        # eliminate column i\n        if verbose:\n            print(f\"eliminating column {i}\")\n\n        # find largest entry in column i\n        largest = abs(A[i, i])\n        j_max = i\n        for j in range(i + 1, n):\n            if abs(A[j, i]) &gt; largest:\n                largest, j_max = abs(A[j, i]), j\n\n        # swap rows j_max and i\n        row_swap(A, b, i, j_max)\n        if verbose:\n            print(f\"swapped system ({i} &lt;-&gt; {j_max})\")\n            print_array(A)\n            print_array(b)\n            print()\n\n        for j in range(i + 1, n):\n            # row j\n            factor = A[j, i] / A[i, i]\n            if verbose:\n                print(f\"row {j} |-&gt; row {j} - {factor} * row {i}\")\n            row_add(A, b, j, -factor, i)\n\n        if verbose:\n            print(\"new system\")\n            print_array(A)\n            print_array(b)\n            print()\n\nGaussian elimination without pivoting following by back subsitution:\n\neps = 1.0e-14\nA = np.array([[10.0, -7.0, 0.0], [-3.0, 2.1 - eps, 6.0], [5.0, -1.0, 5.0]])\nb = np.array([[7.0], [9.9 + eps], [11.0]])\n\nprint(\"starting system:\")\nprint_array(A)\nprint_array(b)\nprint()\n\nprint(\"performing Gaussian elimination without pivoting\")\ngaussian_elimination(A, b, verbose=True)\nprint()\n\nprint(\"upper triangular system:\")\nprint_array(A)\nprint_array(b)\nprint()\n\nprint(\"performing backward substitution\")\nx = backward_substitution(A, b)\nprint()\n\nprint(\"solution using backward substitution:\")\nprint_array(x)\nprint()\n\nA = np.array([[10.0, -7.0, 0.0], [-3.0, 2.1 - eps, 6.0], [5.0, -1.0, 5.0]])\nb = np.array([[7.0], [9.9 + eps], [11.0]])\nprint(\"Does x solve the original system?\", np.allclose(A @ x, b))\n\nstarting system:\nA = [ 10.0, -7.0,  0.0 ]\n    [ -3.0,  2.1,  6.0 ]\n    [  5.0, -1.0,  5.0 ]\nb = [  7.0 ]\n    [  9.9 ]\n    [ 11.0 ]\n\nperforming Gaussian elimination without pivoting\neliminating column 0\n  row 1 |-&gt; row 1 - -0.3 * row 0\n  row 2 |-&gt; row 2 - 0.5 * row 0\n\nnew system\nA = [ 10.0, -7.0,  0.0 ]\n    [  0.0, -0.0,  6.0 ]\n    [  0.0,  2.5,  5.0 ]\nb = [  7.0 ]\n    [ 12.0 ]\n    [  7.5 ]\n\neliminating column 1\n  row 2 |-&gt; row 2 - -244760849313613.9 * row 1\n\nnew system\nA = [ 10.0, -7.0,  0.0 ]\n    [  0.0, -0.0,  6.0 ]\n    [  0.0,  0.0, 1468565095881688.5 ]\nb = [  7.0 ]\n    [ 12.0 ]\n    [ 2937130191763377.0 ]\n\n\nupper triangular system:\nA = [ 10.0, -7.0,  0.0 ]\n    [  0.0, -0.0,  6.0 ]\n    [  0.0,  0.0, 1468565095881688.5 ]\nb = [  7.0 ]\n    [ 12.0 ]\n    [ 2937130191763377.0 ]\n\nperforming backward substitution\n\nsolution using backward substitution:\nx = [ -0.030435 ]\n    [ -1.043478 ]\n    [  2.000000 ]\n\nDoes x solve the original system? False\n\n\nGaussian elimination with pivoting following by back subsitution:\n\neps = 1.0e-14\nA = np.array([[10.0, -7.0, 0.0], [-3.0, 2.1 - eps, 6.0], [5.0, -1.0, 5.0]])\nb = np.array([[7.0], [9.9 + eps], [11.0]])\n\nprint(\"starting system:\")\nprint_array(A)\nprint_array(b)\nprint()\n\nprint(\"performing Gaussian elimination with pivoting\")\ngaussian_elimination_with_pivoting(A, b, verbose=True)\nprint()\n\nprint(\"upper triangular system:\")\nprint_array(A)\nprint_array(b)\nprint()\n\nprint(\"performing backward substitution\")\nx = backward_substitution(A, b)\nprint()\n\nprint(\"solution using backward substitution:\")\nprint_array(x)\nprint()\n\nA = np.array([[10.0, -7.0, 0.0], [-3.0, 2.1 - eps, 6.0], [5.0, -1.0, 5.0]])\nb = np.array([[7.0], [9.9 + eps], [11.0]])\nprint(\"Does x solve the original system?\", np.allclose(A @ x, b))\n\nstarting system:\nA = [ 10.0, -7.0,  0.0 ]\n    [ -3.0,  2.1,  6.0 ]\n    [  5.0, -1.0,  5.0 ]\nb = [  7.0 ]\n    [  9.9 ]\n    [ 11.0 ]\n\nperforming Gaussian elimination with pivoting\neliminating column 0\nswapped system (0 &lt;-&gt; 0)\nA = [ 10.0, -7.0,  0.0 ]\n    [ -3.0,  2.1,  6.0 ]\n    [  5.0, -1.0,  5.0 ]\nb = [  7.0 ]\n    [  9.9 ]\n    [ 11.0 ]\n\nrow 1 |-&gt; row 1 - -0.3 * row 0\nrow 2 |-&gt; row 2 - 0.5 * row 0\nnew system\nA = [ 10.0, -7.0,  0.0 ]\n    [  0.0, -0.0,  6.0 ]\n    [  0.0,  2.5,  5.0 ]\nb = [  7.0 ]\n    [ 12.0 ]\n    [  7.5 ]\n\neliminating column 1\nswapped system (1 &lt;-&gt; 2)\nA = [ 10.0, -7.0,  0.0 ]\n    [  0.0,  2.5,  5.0 ]\n    [  0.0, -0.0,  6.0 ]\nb = [  7.0 ]\n    [  7.5 ]\n    [ 12.0 ]\n\nrow 2 |-&gt; row 2 - -4.085620730620576e-15 * row 1\nnew system\nA = [ 10.0, -7.0,  0.0 ]\n    [  0.0,  2.5,  5.0 ]\n    [  0.0,  0.0,  6.0 ]\nb = [  7.0 ]\n    [  7.5 ]\n    [ 12.0 ]\n\n\nupper triangular system:\nA = [ 10.0, -7.0,  0.0 ]\n    [  0.0,  2.5,  5.0 ]\n    [  0.0,  0.0,  6.0 ]\nb = [  7.0 ]\n    [  7.5 ]\n    [ 12.0 ]\n\nperforming backward substitution\n\nsolution using backward substitution:\nx = [  0.0 ]\n    [ -1.0 ]\n    [  2.0 ]\n\nDoes x solve the original system? True\n\n\n\n\n\n\nSome basic reading:\n\nWikipedia: Gaussian elimination\nJoseph F. Grcar. How ordinary elimination became Gaussian elimination. Historia Mathematica. Volume 38, Issue 2, May 2011. (More history)\n\nSome reading on LU factorisation:\n\n\\(A = LU\\) and solving systems [pdf]\nWikipedia: LU decomposition\nWikipedia: Matrix decomposition (Other examples of decompositions).\nNick Higham: What is an LU factorization? (a very mathematical treatment with additional references)\n\nSome reading on using Gaussian elimination with pivoting:\n\nGaussian elimination with Partial Pivoting [pdf]\nGaussian elimination with partial pivoting example [pdf]\n\nA good general reference for this area:\n\nTrefethen, Lloyd N.; Bau, David (1997), Numerical linear algebra, Philadelphia: Society for Industrial and Applied Mathematics, ISBN 978-0-89871-361-9.\n\nSome implementations:\n\nNumpy numpy.linalg.solve\nScipy scipy.linalg.lu\nLAPACK Gaussian elimination (uses LU factorisation): dgesv()\nLAPACK LU Factorisation: dgetrf().",
    "crumbs": [
      "Direct solvers for systems of linear equtaions"
    ]
  },
  {
    "objectID": "src/lec04.html#elementary-row-operations",
    "href": "src/lec04.html#elementary-row-operations",
    "title": "1 Direct solvers for systems of linear equtaions",
    "section": "",
    "text": "Consider equation \\(p\\) of the above system:\n\\[\na_{p1} x_1 + a_{p2} x_2 + a_{p3} x_3 + \\cdots + a_{pn} x_n = b_p,\n\\]\nand equation \\(q\\):\n\\[\na_{q1} x_1 + a_{q2} x_2 + a_{q3} x_3 + \\cdots + a_{qn} x_n = b_q.\n\\]\nNote three things…\n\nThe order in which we choose to write the \\(n\\) equations is irrelevant\nWe can multiply any equation by an arbitrary real number (\\(k \\neq 0\\) say):\n\\[\nk a_{p1} x_1 + k a_{p2} x_2 + k a_{p3} x_3 + \\cdots + k a_{pn} x_n = k b_p.\n\\]\nWe can add any two equations:\n\\[\nk a_{p1} x_1 + k a_{p2} x_2 + k a_{p3} x_3 + \\cdots + k a_{pn} x_n = k b_p\n\\]\nadded to\n\\[\na_{q1} x_1 + a_{q2} x_2 + a_{q3} x_3 + \\cdots + a_{qn} x_n = b_q\n\\]\nyields\n\\[\n(k a_{p1} + a_{q1}) x_1 + (k a_{p2} + a_{q2}) x_2 + \\cdots + (k a_{pn} + a_{qn}) x_n = k b_p + b_q.\n\\]\n\n\n\n\n\n\n\nExample 1\n\n\n\nConsider the system\n\\[\\begin{align}\n2 x_1 + 3 x_2 & = 4  \\label{eq:eg1a} \\\\\n-3 x_1 + 2 x_2 & = 7 \\label{eq:eg1b}.\n\\end{align}\\]\nThen we have:\n\\[\n\\begin{aligned}\n4 \\times \\text{\\eqref{eq:eg1a}} & \\rightarrow & 8 x_1 + 12 x_2 & = 16 \\\\\n-15. \\times \\text{\\eqref{eq:eg1b}} & \\rightarrow & 4.5 x_2 - 3 x_2 & = -10.5 \\\\\n\\text{\\eqref{eq:eg1a}} + \\text{\\eqref{eq:eg1b}} & \\rightarrow & -x_1 + 5 x_2 & = 11 \\\\\n\\text{\\eqref{eq:eg1b}} + 1.5 \\times \\text{\\eqref{eq:eg1a}} & \\rightarrow & 0 + 6.5 x_2 & = 13.\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\n\nExample 2 (homework)\n\n\n\nConsider the system\n\\[\\begin{align}\nx_1 + 2 x_2 & = 1 \\label{eq:eg2a} \\\\\n4 x_1 + x_2 & = -3 \\label{eq:eg2b}.\n\\end{align}\\]\nWork out the result of these elementary row operations:\n\\[\n\\begin{aligned}\n2 \\times \\text{\\eqref{eq:eg2a}} & \\rightarrow \\\\\n0.25 \\times \\text{\\eqref{eq:eg2b}} & \\rightarrow \\\\\n\\text{\\eqref{eq:eg2b}} + (-1) \\times \\text{\\eqref{eq:eg2a}} & \\rightarrow \\\\\n\\text{\\eqref{eq:eg2b}} + (-4) \\times \\text{\\eqref{eq:eg2a}} & \\rightarrow\n\\end{aligned}\n\\]\n\n\nFor a system written in matrix form our three observations mean the following:\n\nWe can swap any two rows of the matrix (and corresponding right-hand side entries). For example:\n\\[\n\\begin{pmatrix}\n2 & 3 \\\\ -3 & 2\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2\n\\end{pmatrix} =\n\\begin{pmatrix}\n4 \\\\ 7\n\\end{pmatrix}\n\\Rightarrow\n\\begin{pmatrix}\n-3 & 2\\\\ 2 & 3\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2\n\\end{pmatrix} =\n\\begin{pmatrix}\n7 \\\\ 4\n\\end{pmatrix}\n\\]\nWe can multiply any row of the matrix (and corresponding right-hand side entry) by a scalar. For example:\n\\[\n\\begin{pmatrix}\n2 & 3 \\\\ -3 & 2\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2\n\\end{pmatrix} =\n\\begin{pmatrix}\n4 \\\\ 7\n\\end{pmatrix}\n\\Rightarrow\n\\begin{pmatrix}\n1 & \\frac{3}{2} \\\\ -3 & 2\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2\n\\end{pmatrix} =\n\\begin{pmatrix}\n2 \\\\ 7\n\\end{pmatrix}\n\\]\nWe can replace row \\(q\\) by row \\(q + k \\times\\) row \\(p\\). For example:\n\\[\n\\begin{pmatrix}\n2 & 3 \\\\ -3 & 2\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2\n\\end{pmatrix} =\n\\begin{pmatrix}\n4 \\\\ 7\n\\end{pmatrix}\n\\Rightarrow\n\\begin{pmatrix}\n2 & 3 \\\\ 0 & 6.5\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2\n\\end{pmatrix} =\n\\begin{pmatrix}\n4 \\\\ 13\n\\end{pmatrix}\n\\]\n(here we replaced row \\(w\\) by row \\(2 + 1.5 \\times\\) row \\(1\\))\n\\[\n\\begin{pmatrix}\n1 & 2 \\\\ 4 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2\n\\end{pmatrix} =\n\\begin{pmatrix}\n1 \\\\ -3\n\\end{pmatrix}\n\\Rightarrow\n\\begin{pmatrix}\n1 & 2 \\\\ 0 & -7\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2\n\\end{pmatrix} =\n\\begin{pmatrix}\n1 \\\\ -7\n\\end{pmatrix}\n\\]\n(here we replaced row \\(2\\) by row \\(2 + (-4) \\times\\) row \\(1\\))\n\nOur strategy for solving systems of linear equations using Gaussian elimination is based on the following ideas:\n\nThree types of operation described above are called elementary row operations (ERO).\nWe will applying a sequence of ERO to reduce an arbitrary system to a triangular form, which, we will see, can be easily solved.\nThe algorithm for reducing a general matrix to upper triangular form is known as forward elimination or (more commonly) as Gaussian elimination.",
    "crumbs": [
      "Direct solvers for systems of linear equtaions"
    ]
  },
  {
    "objectID": "src/lec04.html#gaussian-elimination",
    "href": "src/lec04.html#gaussian-elimination",
    "title": "1 Direct solvers for systems of linear equtaions",
    "section": "",
    "text": "The algorithm of Gaussian elimination is a very old method that you may have already met at school - perhaps by a different name. The details of the method may seem quite confusing at first. Really we are following the ideas of eliminating systems of simultaneous equations, but in a way a computer understands. This is an important point in this section. You will have seen different ideas of how to solve systems of simulations equations where the first step is to “look at the equations to decide the easiest first step”. When there are \\(10^9\\) equations, its not effective for a computer to try and find an easy way through the problem. It must instead of a simple set of instructions to follow: this will be our algorithm.\nThe method is so old, in fact we have evidence of Chinese mathematicians using Gaussian elimination in 179CE (From Wikipedia):\n\nThe method of Gaussian elimination appears in the Chinese mathematical text Chapter Eight: Rectangular Arrays of The Nine Chapters on the Mathematical Art. Its use is illustrated in eighteen problems, with two to five equations. The first reference to the book by this title is dated to 179 CE, but parts of it were written as early as approximately 150 BCE. It was commented on by Liu Hui in the 3rd century.\nThe method in Europe stems from the notes of Isaac Newton. In 1670, he wrote that all the algebra books known to him lacked a lesson for solving simultaneous equations, which Newton then supplied. Carl Friedrich Gauss in 1810 devised a notation for symmetric elimination that was adopted in the 19th century by professional hand computers to solve the normal equations of least-squares problems. The algorithm that is taught in high school was named for Gauss only in the 1950s as a result of confusion over the history of the subject.\n\n\n\n\nImage from Nine Chapter of the Mathematical art. By Yang Hui(1238－1298) - mybook, Public Domain, https://commons.wikimedia.org/w/index.php?curid=10317744\n\n\n\n\nThe following algorithm systematically introduces zeros into the system of equations, below the diagonal.\n\nSubtract multiples of row 1 from the rows below it to eliminate (make zero) nonzero entries in column 1.\nSubtract multiplies of the new row 2 from the rows below it to eliminate nonzero entries in column 2.\nRepeat for row \\(3, 4, \\ldots, n-1\\).\n\nAfter row \\(n-1\\) all entities below the diagonal have been eliminated, so \\(A\\) is now upper triangular and the resulting system can be solved by backward substitution.\n\n\n\n\n\n\nExample 1\n\n\n\nUse Gaussian eliminate to reduce the following system of equations to upper triangular form:\n\\[\n\\begin{pmatrix}\n2 & 1 & 4 \\\\ 1 & 2 & 2 \\\\ 2 & 4 & 6\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{pmatrix} =\n\\begin{pmatrix}\n12 \\\\ 9 \\\\ 22\n\\end{pmatrix}.\n\\]\nFirst, use the first row to eliminate the first column below the diagonal:\n\n(row 2) \\(- 0.5 \\times\\) (row 1) gives\n\\[\n\\begin{pmatrix}\n2 & 1 & 4 \\\\ \\mathbf{0} & 1.5 & 0 \\\\ 2 & 4 & 6\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{pmatrix} =\n\\begin{pmatrix}\n12 \\\\ 3 \\\\ 22\n\\end{pmatrix}\n\\]\n(row 3) \\(-\\) (row 1) then gives\n\\[\n\\begin{pmatrix}\n2 & 1 & 4 \\\\ \\mathbf{0} & 1.5 & 0 \\\\ \\mathbf{0} & 3 & 2\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{pmatrix} =\n\\begin{pmatrix}\n12 \\\\ 3 \\\\ 10\n\\end{pmatrix}\n\\]\n\nNow use the second row to eliminate the second column below the diagonal.\n\n(row 3) \\(- 2 \\times\\) (row 2) gives\n\\[\n\\begin{pmatrix}\n2 & 1 & 4 \\\\ \\mathbf{0} & 1.5 & 0 \\\\ \\mathbf{0} & \\mathbf{0} & 2\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{pmatrix} =\n\\begin{pmatrix}\n12 \\\\ 3 \\\\ 4\n\\end{pmatrix}\n\\]\n\n\n\n\n\n\n\n\n\nExample 2 (homework)\n\n\n\nUse Gaussian elimination to reduce the following system of linear equations to upper triangular form.\n\\[\n\\begin{pmatrix}\n4 & -1 & -1 \\\\ 2 & 4 & 2 \\\\ 1 & 2 & 4\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{pmatrix} =\n\\begin{pmatrix}\n9 \\\\ -6 \\\\ 3\n\\end{pmatrix}.\n\\]\n\n\n\nRemark. \n\nEach row \\(i\\) is used to eliminate the entries in column \\(i\\) below \\(a_{ii}\\), i.e. it forces \\(a_{ji} = 0\\) for \\(j &gt; i\\).\nThis is done by subtracting a multiple of row \\(i\\) from row \\(j\\):\n\\[\n(\\text{row } j) \\leftarrow (\\text{row } j) - \\frac{a_{ji}}{a_{ii}} (\\text{row } i).\n\\]\nThis guarantees that \\(a_{ji}\\) becomes zero because\n\\[\na_{ji} \\leftarrow a_{ji} - \\frac{a_{ji}}{a_{ii}} a_{ii} = a_{ji} - a_{ji} = 0.\n\\]\n\n\n\n\n\n\n\n\nExample 3 (homework)\n\n\n\nSolve the system\n\\[\n\\begin{pmatrix}\n4 & 3 & 2 & 1 \\\\ 1 & 2 & 2 & 2 \\\\\n1 & 1 & 3 & 0 \\\\ 2 & 1 & 2 & 3\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3 \\\\ x_4\n\\end{pmatrix} =\n\\begin{pmatrix}\n10 \\\\ 7 \\\\ 5 \\\\ 8\n\\end{pmatrix}.\n\\] The solution is \\(\\vec{x} = (1, 1, 1, 1)^T\\).\n\n\n\n\n\nWe start with some helper code which determines the size of the system we are working with:\n\ndef system_size(A, b):\n    \"\"\"\n    for a system of linear equations, returns the size and errors if sizes are not compatible\n    \"\"\"\n    n, m = A.shape\n    nb = b.shape[0]\n\n    assert n == m and n == nb\n    return n\n\nThen we can implement the elementary row operations\n\ndef row_swap(A, b, p, q):\n    \"\"\"\n    swap the rows p and q for the system of linear equations given by Ax = b.\n    updates entries in place\n    \"\"\"\n    n = system_size(A, b)\n    # swap rows of A\n    for j in range(n):\n        A[p, j], A[q, j] = A[q, j], A[p, j]\n    # swap rows of b\n    b[p, 0], b[q, 0] = b[q, 0], b[p, 0]\n\n\ndef row_scale(A, b, p, k):\n    \"\"\"\n    scale the entries in row p by k for the system of linear equations given by Ax = b.\n    updates entries in place\n    \"\"\"\n    n = system_size(A, b)\n\n    # scale row p of A\n    for j in range(n):\n        A[p, j] = k * A[p, j]\n    # scale row p of b\n    b[p, 0] = b[p, 0] * k\n\n\ndef row_add(A, b, p, k, q):\n    \"\"\"\n    add rows for the system of linear equations given by Ax = b\n    this operation is row p |-&gt; row p + k * row q\n    updates entries in place\n    \"\"\"\n    n = system_size(A, b)\n\n    for j in range(n):\n        A[p, j] = A[p, j] + k * A[q, j]\n    b[p, 0] = b[p, 0] + k * b[q, 0]\n\nLet’s test we are ok so far:\nTest 1: swapping rows\n\nA = np.array([[2.0, 3.0], [-3.0, 2.0]])\nb = np.array([[4.0], [7.0]])\n\nprint(\"starting arrays:\")\nprint_array(A)\nprint_array(b)\nprint()\n\nprint(\"swapping rows 0 and 1\")\nrow_swap(A, b, 0, 1)  # remember numpy arrays are indexed starting from zero!\nprint()\n\nprint(\"new arrays:\")\nprint_array(A)\nprint_array(b)\nprint()\n\nstarting arrays:\nA = [  2.0,  3.0 ]\n    [ -3.0,  2.0 ]\nb = [  4.0 ]\n    [  7.0 ]\n\nswapping rows 0 and 1\n\nnew arrays:\nA = [ -3.0,  2.0 ]\n    [  2.0,  3.0 ]\nb = [  7.0 ]\n    [  4.0 ]\n\n\n\nTest 2: scaling one row\n\nA = np.array([[2.0, 3.0], [-3.0, 2.0]])\nb = np.array([[4.0], [7.0]])\n\nprint(\"starting arrays:\")\nprint_array(A)\nprint_array(b)\nprint()\n\nprint(\"row 0 |-&gt; 0.5 * row 0\")\nrow_scale(A, b, 0, 0.5)  # remember numpy arrays are indexed started from zero!\nprint()\n\nprint(\"new arrays:\")\nprint_array(A)\nprint_array(b)\nprint()\n\nstarting arrays:\nA = [  2.0,  3.0 ]\n    [ -3.0,  2.0 ]\nb = [  4.0 ]\n    [  7.0 ]\n\nrow 0 |-&gt; 0.5 * row 0\n\nnew arrays:\nA = [  1.0,  1.5 ]\n    [ -3.0,  2.0 ]\nb = [  2.0 ]\n    [  7.0 ]\n\n\n\nTest 3: replacing a row by that adding a multiple of another row\n\nA = np.array([[2.0, 3.0], [-3.0, 2.0]])\nb = np.array([[4.0], [7.0]])\n\nprint(\"starting arrays:\")\nprint_array(A)\nprint_array(b)\nprint()\n\nprint(\"row 1 |-&gt; row 1 + 1.5 * row 0\")\nrow_add(A, b, 1, 1.5, 0)  # remember numpy arrays are indexed started from zero!\nprint()\n\nprint(\"new arrays:\")\nprint_array(A)\nprint_array(b)\nprint()\n\nstarting arrays:\nA = [  2.0,  3.0 ]\n    [ -3.0,  2.0 ]\nb = [  4.0 ]\n    [  7.0 ]\n\nrow 1 |-&gt; row 1 + 1.5 * row 0\n\nnew arrays:\nA = [  2.0,  3.0 ]\n    [  0.0,  6.5 ]\nb = [  4.0 ]\n    [ 13.0 ]\n\n\n\nNow we can define our Gaussian elimination function. We update the values in-place to avoid extra memory allocations.\n\ndef gaussian_elimination(A, b, verbose=False):\n    \"\"\"\n    perform Gaussian elimnation to reduce the system of linear equations Ax=b to upper triangular form\n    use verbose to print out intermediate representations\n    \"\"\"\n    # find shape of system\n    n = system_size(A, b)\n\n    # perform forwards elimination\n    for i in range(n - 1):\n        # eliminate column i\n        if verbose:\n            print(f\"eliminating column {i}\")\n        for j in range(i + 1, n):\n            # row j\n            factor = A[j, i] / A[i, i]\n            if verbose:\n                print(f\"  row {j} |-&gt; row {j} - {factor} * row {i}\")\n            row_add(A, b, j, -factor, i)\n\n        if verbose:\n            print()\n            print(\"new system\")\n            print_array(A)\n            print_array(b)\n            print()\n\nWe can try our code on Example 1:\n\nA = np.array([[2.0, 1.0, 4.0], [1.0, 2.0, 2.0], [2.0, 4.0, 6.0]])\nb = np.array([[12.0], [9.0], [22.0]])\n\nprint(\"starting system:\")\nprint_array(A)\nprint_array(b)\nprint()\n\nprint(\"performing Gaussian Elimination\")\ngaussian_elimination(A, b, verbose=True)\nprint()\n\nprint(\"final system:\")\nprint_array(A)\nprint_array(b)\nprint()\n\n# test that A is really upper triangular\nprint(\"Is A really upper triangular?\", np.allclose(A, np.triu(A)))\n\nstarting system:\nA = [  2.0,  1.0,  4.0 ]\n    [  1.0,  2.0,  2.0 ]\n    [  2.0,  4.0,  6.0 ]\nb = [ 12.0 ]\n    [  9.0 ]\n    [ 22.0 ]\n\nperforming Gaussian Elimination\neliminating column 0\n  row 1 |-&gt; row 1 - 0.5 * row 0\n  row 2 |-&gt; row 2 - 1.0 * row 0\n\nnew system\nA = [  2.0,  1.0,  4.0 ]\n    [  0.0,  1.5,  0.0 ]\n    [  0.0,  3.0,  2.0 ]\nb = [ 12.0 ]\n    [  3.0 ]\n    [ 10.0 ]\n\neliminating column 1\n  row 2 |-&gt; row 2 - 2.0 * row 1\n\nnew system\nA = [  2.0,  1.0,  4.0 ]\n    [  0.0,  1.5,  0.0 ]\n    [  0.0,  0.0,  2.0 ]\nb = [ 12.0 ]\n    [  3.0 ]\n    [  4.0 ]\n\n\nfinal system:\nA = [  2.0,  1.0,  4.0 ]\n    [  0.0,  1.5,  0.0 ]\n    [  0.0,  0.0,  2.0 ]\nb = [ 12.0 ]\n    [  3.0 ]\n    [  4.0 ]\n\nIs A really upper triangular? True",
    "crumbs": [
      "Direct solvers for systems of linear equtaions"
    ]
  },
  {
    "objectID": "src/lec04.html#solving-triangular-systems-of-equations",
    "href": "src/lec04.html#solving-triangular-systems-of-equations",
    "title": "1 Direct solvers for systems of linear equtaions",
    "section": "",
    "text": "A general lower triangular system of equations has \\(a_{ij} = 0\\) for \\(j &gt; i\\) and takes the form:\n\\[\n\\begin{pmatrix}\na_{11} & 0 & 0 & \\cdots & 0 \\\\\na_{21} & a_{22} & 0 & \\cdots & 0 \\\\\na_{31} & a_{32} & a_{33} & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{n1} & a_{n2} & a_{n3} & \\cdots & a_{nn}\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3 \\\\ \\vdots \\\\ x_n\n\\end{pmatrix} =\n\\begin{pmatrix}\nb_1 \\\\ b_2 \\\\ b_3 \\\\ \\vdots \\\\ b_n\n\\end{pmatrix}.\n\\]\nNote the first equation is\n\\[\na_{11} x_1 = b_1.\n\\]\nThen \\(x_i\\) can be found by calculating\n\\[\nx_i = \\frac{1}{a_{ii}} \\left(b_i - \\sum_{j=1}^{i-1} a_{ij} x_j \\right)\n\\]\nfor each row \\(i = 1, 2, \\ldots, n\\) in turn.\n\nEach calculation requires only previously computed values \\(x_j\\) (and the sum gives a loop for \\(j &lt; i\\).\nThe matrix \\(A\\) must have nonzero diagonal entries\ni.e. \\(a_{ii} \\neq 0\\) for \\(i = 1, 2, \\ldots, n\\).\nUpper triangular systems of equations can be solved in a similar manner.\n\n\n\n\n\n\n\nExample 1\n\n\n\nSolve the lower triangular system of equations given by\n\\[\n\\begin{aligned}\n2 x_1 && && &= 2 \\\\\nx_1 &+& 2 x_2 && &= 7 \\\\\n2 x_1 &+& 4 x_2 &+& 6 x_3 &= 26\n\\end{aligned}\n\\]\nor, equivalently,\n\\[\n\\begin{pmatrix}\n2 & 0 & 0 \\\\\n1 & 2 & 0 \\\\\n2 & 4 & 6\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{pmatrix} =\n\\begin{pmatrix}\n2 \\\\ 7 \\\\ 26\n\\end{pmatrix}.\n\\]\nThe solution can be calculated systematically from\n\\[\n\\begin{aligned}\nx_1 &= \\frac{b_1}{a_{11}} = \\frac{2}{2} = 1 \\\\\nx_2 &= \\frac{b_2 - a_{21} x_1}{a_{22}}\n= \\frac{7 - 1 \\times 1}{2} = \\frac{6}{2} = 3 \\\\\nx_3 &= \\frac{b_3 - a_{31} x_1 - a_{32} x_2}{a_33}\n= \\frac{26 - 2 \\times 1 - 4 \\times 3}{6}  = \\frac{12}{6}\n= 2\n\\end{aligned}\n\\]\nwhich gives the solution \\(\\vec{x} = (1, 3, 2)^T\\).\n\n\n\n\n\n\n\n\nExample 2 (homework)\n\n\n\nSolve the upper triangular linear system given by\n\\[\n\\begin{aligned}\n2 x_1 &+& x_2 &+& 4 x_3 &=& 12 \\\\\n&& 1.5 x_2 && &=& 3 \\\\\n&& && 2 x_3 &=& 4\n\\end{aligned}.\n\\]\n\n\n\nRemark. \n\nIt is simple to solve a lower (upper) triangular system of equations (provided the diagonal is nonzero).\nThis process is often referred to as forward (backward) substitution.\nA general system of equations (i.e. a full matrix \\(A\\)) can be solved rapidly once it has been reduced to upper triangular form. This is the idea of using Gaussian elimination with backward subsitution.\n\n\nWe can define functions to solve both upper or lower triangular form systems of linear equations:\n\ndef forward_substitution(A, b):\n    \"\"\"\n    solves the system of linear equationa Ax = b assuming that A is lower triangular\n    returns the solution x\n    \"\"\"\n    # get size of system\n    n = system_size(A, b)\n\n    # check is lower triangular\n    assert np.allclose(A, np.tril(A))\n\n    # create solution variable\n    x = np.empty_like(b)\n\n    # perform forwards solve\n    for i in range(n):\n        partial_sum = 0.0\n        for j in range(0, i):\n            partial_sum += A[i, j] * x[j]\n        x[i] = 1.0 / A[i, i] * (b[i] - partial_sum)\n\n    return x\n\n\ndef backward_substitution(A, b):\n    \"\"\"\n    solves the system of linear equationa Ax = b assuming that A is upper triangular\n    returns the solution x\n    \"\"\"\n    # get size of system\n    n = system_size(A, b)\n\n    # check is upper triangular\n    assert np.allclose(A, np.triu(A))\n\n    # create solution variable\n    x = np.empty_like(b)\n\n    # perform backwards solve\n    for i in range(n - 1, -1, -1):  # iterate over rows backwards\n        partial_sum = 0.0\n        for j in range(i + 1, n):\n            partial_sum += A[i, j] * x[j]\n        x[i] = 1.0 / A[i, i] * (b[i] - partial_sum)\n\n    return x\n\nAnd we can then test it out!\n\nA = np.array([[2.0, 0.0, 0.0], [1.0, 2.0, 0.0], [2.0, 4.0, 6.0]])\nb = np.array([[2.0], [7.0], [26.0]])\n\nprint(\"The system is given by:\")\nprint_array(A)\nprint_array(b)\nprint()\n\nprint(\"Solving the system using forward substitution\")\nx = forward_substitution(A, b)\nprint()\n\nprint(\"The solution using forward substitution is:\")\nprint_array(x)\nprint()\n\nprint(\"Does x really solve the system?\", np.allclose(A @ x, b))\n\nThe system is given by:\nA = [  2.0,  0.0,  0.0 ]\n    [  1.0,  2.0,  0.0 ]\n    [  2.0,  4.0,  6.0 ]\nb = [  2.0 ]\n    [  7.0 ]\n    [ 26.0 ]\n\nSolving the system using forward substitution\n\nThe solution using forward substitution is:\nx = [  1.0 ]\n    [  3.0 ]\n    [  2.0 ]\n\nDoes x really solve the system? True\n\n\nWe can also do a backward substitution test:\n\nA = np.array([[2.0, 1.0, 4.0], [0.0, 1.5, 0.0], [0.0, 0.0, 2.0]])\nb = np.array([[12.0], [3.0], [4.0]])\n\nprint(\"The system is given by:\")\nprint_array(A)\nprint_array(b)\nprint()\n\nprint(\"Solving the system using backward substitution\")\nx = backward_substitution(A, b)\nprint()\n\nprint(\"The solution using backward substitution is:\")\nprint_array(x)\nprint()\n\n\nprint(\"Does x really solve the system?\", np.allclose(A @ x, b))\n\nThe system is given by:\nA = [  2.0,  1.0,  4.0 ]\n    [  0.0,  1.5,  0.0 ]\n    [  0.0,  0.0,  2.0 ]\nb = [ 12.0 ]\n    [  3.0 ]\n    [  4.0 ]\n\nSolving the system using backward substitution\n\nThe solution using backward substitution is:\nx = [  1.0 ]\n    [  2.0 ]\n    [  2.0 ]\n\nDoes x really solve the system? True",
    "crumbs": [
      "Direct solvers for systems of linear equtaions"
    ]
  },
  {
    "objectID": "src/lec04.html#combining-gaussian-elimination-and-backward-substitution",
    "href": "src/lec04.html#combining-gaussian-elimination-and-backward-substitution",
    "title": "1 Direct solvers for systems of linear equtaions",
    "section": "",
    "text": "Our grant strategy can now come together so we have a method to solve systems of linear equations:\nGiven a system of linear equations \\(A\\vec{x} = \\vec{b}\\);\n\nFirst perform Gaussian elimination to give an equivalent system of equations in upper triangular form;\nThen use backward substitution to produce a solution \\(\\vec{x}\\)\n\nWe can use our code to test this:\n\nA = np.array([[2.0, 1.0, 4.0], [1.0, 2.0, 2.0], [2.0, 4.0, 6.0]])\nb = np.array([[12.0], [9.0], [22.0]])\n\nprint(\"starting system:\")\nprint_array(A)\nprint_array(b)\nprint()\n\nprint(\"performing Gaussian Elimination\")\ngaussian_elimination(A, b, verbose=True)\nprint()\n\nprint(\"upper triangular system:\")\nprint_array(A)\nprint_array(b)\nprint()\n\nprint(\"Solving the system using backward substitution\")\nx = backward_substitution(A, b)\nprint()\n\nprint(\"solution using backward substitution:\")\nprint_array(x)\nprint()\n\nA = np.array([[2.0, 1.0, 4.0], [1.0, 2.0, 2.0], [2.0, 4.0, 6.0]])\nb = np.array([[12.0], [9.0], [22.0]])\nprint(\"Does x really solve the original system?\", np.allclose(A @ x, b))\n\nstarting system:\nA = [  2.0,  1.0,  4.0 ]\n    [  1.0,  2.0,  2.0 ]\n    [  2.0,  4.0,  6.0 ]\nb = [ 12.0 ]\n    [  9.0 ]\n    [ 22.0 ]\n\nperforming Gaussian Elimination\neliminating column 0\n  row 1 |-&gt; row 1 - 0.5 * row 0\n  row 2 |-&gt; row 2 - 1.0 * row 0\n\nnew system\nA = [  2.0,  1.0,  4.0 ]\n    [  0.0,  1.5,  0.0 ]\n    [  0.0,  3.0,  2.0 ]\nb = [ 12.0 ]\n    [  3.0 ]\n    [ 10.0 ]\n\neliminating column 1\n  row 2 |-&gt; row 2 - 2.0 * row 1\n\nnew system\nA = [  2.0,  1.0,  4.0 ]\n    [  0.0,  1.5,  0.0 ]\n    [  0.0,  0.0,  2.0 ]\nb = [ 12.0 ]\n    [  3.0 ]\n    [  4.0 ]\n\n\nupper triangular system:\nA = [  2.0,  1.0,  4.0 ]\n    [  0.0,  1.5,  0.0 ]\n    [  0.0,  0.0,  2.0 ]\nb = [ 12.0 ]\n    [  3.0 ]\n    [  4.0 ]\n\nSolving the system using backward substitution\n\nsolution using backward substitution:\nx = [  1.0 ]\n    [  2.0 ]\n    [  2.0 ]\n\nDoes x really solve the original system? True",
    "crumbs": [
      "Direct solvers for systems of linear equtaions"
    ]
  },
  {
    "objectID": "src/lec04.html#the-cost-of-gaussian-elimination",
    "href": "src/lec04.html#the-cost-of-gaussian-elimination",
    "title": "1 Direct solvers for systems of linear equtaions",
    "section": "",
    "text": "Gaussian elimination (GE) is unnecessarily expensive when it is applied to many systems of equations with the same matrix \\(A\\) but different right-hand sides \\(\\vec{b}\\).\n\nThe forward elimination process is the most computationally expensive part at \\(O(n^3)\\) but is exactly the same for any choice of \\(\\vec{b}\\).\nIn contrast, the solution of the resulting upper triangular system only requires \\(O(n^2)\\) operations.\n\n\n\n\n\n\n\n\n\n\nWe can use this information to improve the way in which we solve multiple systems of equations with the same matrix \\(A\\) but different right-hand sides \\(\\vec{b}\\).",
    "crumbs": [
      "Direct solvers for systems of linear equtaions"
    ]
  },
  {
    "objectID": "src/lec04.html#lu-factorisation",
    "href": "src/lec04.html#lu-factorisation",
    "title": "1 Direct solvers for systems of linear equtaions",
    "section": "",
    "text": "Our next algorithm, called LU factorisation, is a way to try to speed up Gaussian elimination by reusing information. This can be used when we solve systems of equations with the same matrix \\(A\\) but different right hand sides \\(\\vec{b}\\) - this is more common than you would think!\nRecall the elementary row operations (EROs) from above. Note that the EROs can be produced by left multiplication with a suitable matrix:\n\nRow swap:\n\\[\n\\begin{pmatrix}\n1 & 0 & 0 \\\\ 0 & 0 & 1 \\\\ 0 & 1 & 0\n\\end{pmatrix}\n\\begin{pmatrix}\na & b & c \\\\ d & e & f \\\\ g & h & i\n\\end{pmatrix}\n=\n\\begin{pmatrix}\na & b & c \\\\ g & h & i \\\\ d & e & f\n\\end{pmatrix}\n\\]\nRow swap:\n\\[\n\\begin{pmatrix}\n1 & 0 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\na & b & c & d \\\\ e & f & g & h \\\\ i & j & k & l \\\\ m & n & o & p\n\\end{pmatrix}\n=\n\\begin{pmatrix}\na & b & c & d \\\\ i & j & k & l \\\\ e & f & g & h \\\\ m & n & o & p\n\\end{pmatrix}\n\\]\nMultiply row by \\(\\alpha\\):\n\\[\n\\begin{pmatrix}\n\\alpha & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\na & b & c \\\\ d & e & f \\\\ g & h & i\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\alpha a & \\alpha b & \\alpha c \\\\ d & e & f \\\\ g & h & i\n\\end{pmatrix}\n\\]\n\\(\\alpha \\times \\text{row } p + \\text{row } q\\):\n\\[\n\\begin{pmatrix}\n1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ \\alpha & 0 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\na & b & c \\\\ d & e & f \\\\ g & h & i\n\\end{pmatrix}\n=\n\\begin{pmatrix}\na & b & c \\\\ d & e & f \\\\ \\alpha a + g & \\alpha b + h & \\alpha c + i\n\\end{pmatrix}\n\\]\n\nSince Gaussian elimination (GE) is just a sequence of EROs and each ERO just multiplication by a suitable matrix, say \\(E_k\\), forward elimination applied to the system \\(A \\vec{x} = \\vec{b}\\) can be expressed as \\[\n(E_m \\cdots E_1) A \\vec{x} = (E_m \\cdots E_1) \\vec{b},\n\\] here \\(m\\) is the number of EROs required to reduce the upper triangular form.\nLet \\(U = (E_m \\cdots E_1) A\\) and \\(L = (E_m \\cdots E_1)^{-1}\\). Now the original system \\(A \\vec{x} = \\vec{b}\\) is equivalent to\n\\[\\begin{equation}\n\\label{eq:LU}\nL U \\vec{x} = \\vec{b}\n\\end{equation}\\]\nwhere \\(U\\) is upper triangular (by construction) and \\(L\\) may be shown to be lower triangular (provided the EROs do not include any row swaps).\nOnce \\(L\\) and \\(U\\) are known it is easy to solve \\(\\eqref{eq:LU}\\)\n\nSolve \\(L \\vec{z} = \\vec{b}\\) in \\(O(n^2)\\) operations.\nSolve \\(U \\vec{x} = \\vec{z}\\) in \\(O(n^2)\\) operations.\n\n\\(L\\) and \\(U\\) may be found in \\(O(n^3)\\) operations by performing GE and saving the \\(E_i\\) matrices, however it is more convenient to find them directly (also \\(O(n^3)\\) operations).\n\n\nConsider a general \\(4 \\times 4\\) matrix \\(A\\) and its factorisation \\(LU\\):\n\\[\n\\begin{pmatrix}\na_{11} & a_{12} & a_{13} & a_{14} \\\\\na_{21} & a_{22} & a_{23} & a_{24} \\\\\na_{31} & a_{32} & a_{33} & a_{34} \\\\\na_{41} & a_{42} & a_{43} & a_{44}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n1 & 0 & 0 & 0 \\\\\nl_{21} & 1 & 0 & 0 \\\\\nl_{31} & l_{32} & 1 & 0 \\\\\nl_{41} & l_{42} & l_{43} & 1\n\\end{pmatrix}\n\\begin{pmatrix}\nu_{11} & u_{12} & u_{13} & u_{14} \\\\\n0 & u_{22} & u_{23} & u_{24} \\\\\n0 & 0 & u_{33} & u_{34} \\\\\n0 & 0 & 0 & u_{44}\n\\end{pmatrix}\n\\]\nFor the first column,\n\\[\n\\begin{aligned}\na_{11} & = (1, 0, 0, 0) (u_{11}, 0, 0, 0)^T && = u_{11}\n& \\rightarrow u_{11} & = a_{11} \\\\\na_{21} & = (l_{21}, 1, 0, 0)(u_{11}, 0, 0, 0)^T && = l_{21} u_{11}\n& \\rightarrow l_{21} & = a_{21} / u_{11} \\\\\na_{31} & = (l_{31}, l_{32}, 1, 0)(u_{11}, 0, 0, 0)^T && = l_{31} u_{11}\n& \\rightarrow l_{31} & = a_{31} / u_{11} \\\\\na_{41} & = (l_{41}, l_{42}, l_{43}, 1)(u_{11}, 0, 0, 0)^T && = l_{41} u_{11}\n& \\rightarrow l_{41} & = a_{41} / u_{11}\n\\end{aligned}\n\\]\nThe second, third and fourth columns follow in a similar manner, giving all the entries in \\(L\\) and \\(U\\).\n\nRemark. \n\n\\(L\\) is assumed to have 1’s on the diagonal, to ensure that the factorisation is unique.\nThe process involves division by the diagonal entries \\(u_{11}, u_{22}\\), etc., so they must be non-zero.\nIn general the factors \\(l_{ij}\\) and \\(u_{ij}\\) are calculated for each column \\(j\\) in turn, i.e.,\nfor j in range(n):\n  for i in range(j+1):\n      # Compute factors u_{ij}\n      ...\n  for i in range(j+1, n):\n      # Compute factors l_{ij}\n      ...\n\n\n\n\n\n\n\n\nExample 1\n\n\n\nUse \\(LU\\) factorisation to solve the linear system of equations given by\n\\[\n\\begin{pmatrix}\n2 & 1 & 4 \\\\\n1 & 2 & 2 \\\\\n2 & 4 & 6\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n12 \\\\ 9 \\\\ 22\n\\end{pmatrix}.\n\\]\nThis can be rewritten in the form \\(A = LU\\) where\n\\[\n\\begin{pmatrix}\n2 & 1 & 4 \\\\\n1 & 2 & 2 \\\\\n2 & 4 & 6\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n1 & 0 & 0 \\\\\nl_{21} & 1 & 0 \\\\\nl_{31} & l_{32} & 1\n\\end{pmatrix}\n\\begin{pmatrix}\nu_{11} & u_{12} & u_{13} \\\\\n0 & u_{22} & u_{23} \\\\\n0 & 0 & u_{33}\n\\end{pmatrix}.\n\\]\nColumn 1 of \\(A\\) gives\n\\[\n\\begin{aligned}\n2 & = u_{11} && \\rightarrow & u_{11} & = 2 \\\\\n1 & = l_{21} u_{11} && \\rightarrow & l_{21} & = 0.5 \\\\\n2 & = l_{31} u_{11} && \\rightarrow & l_{31} & = 1.\n\\end{aligned}\n\\]\nColumn 2 of \\(A\\) gives\n\\[\n\\begin{aligned}\n1 & = u_{12} && \\rightarrow & u_{12} & = 1 \\\\\n2 & = l_{21} u_{12} + u_{22} && \\rightarrow & u_{22} & = 1.5 \\\\\n4 & = l_{31} u_{12} + l_{32} u_{22} && \\rightarrow & l_{32} & = 2.\n\\end{aligned}\n\\]\nColumn 3 of \\(A\\) gives\n\\[\n\\begin{aligned}\n4 & = u_{13} && \\rightarrow & u_{13} & = 4 \\\\\n2 & = l_{21} u_{13} + u_{23} && \\rightarrow & u_{23} & = 0 \\\\\n6 & = l_{31} u_{13} + l_{32} u_{23} + u_{33} && \\rightarrow & u_{33} & = 2.\n\\end{aligned}\n\\]\nSolve the lower triangular system \\(L \\vec{z} = \\vec{b}\\):\n\\[\n\\begin{pmatrix}\n1 & 0 & 0 \\\\\n0.5 & 1 & 0 \\\\\n1 & 2 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\nz_1 \\\\ z_2 \\\\ z_3\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n12 \\\\ 9 \\\\ 22\n\\end{pmatrix}\n\\rightarrow\n\\begin{pmatrix}\nz_1 \\\\ z_2 \\\\ z_3\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n12 \\\\ 3 \\\\ 4\n\\end{pmatrix}\n\\]\nSolve the upper triangular system \\(U \\vec{x} = \\vec{z}\\):\n\\[\n\\begin{pmatrix}\n2 & 1 & 4 \\\\\n0 & 1.5 & 0 \\\\\n0 & 0 & 2\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{pmatrix}\n\\begin{pmatrix}\n12 \\\\ 3 \\\\ 4\n\\end{pmatrix}\n\\rightarrow\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n1 \\\\ 2 \\\\ 2\n\\end{pmatrix}.\n\\]\n\n\n\n\n\n\n\n\nExample 2 (homework)\n\n\n\nRewrite the matrix \\(A\\) as the product of lower and upper triangular matrices where\n\\[\nA =\n\\begin{pmatrix}\n4 & 2 & 0 \\\\\n2 & 3 & 1 \\\\\n0 & 1 & 2.5\n\\end{pmatrix}.\n\\]\n\n\n\nRemark. The first example gives\n\\[\n\\begin{pmatrix}\n2 & 1 & 4 \\\\\n1 & 2 & 2 \\\\\n2 & 4 & 6\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n1 & 0 & 0 \\\\\n0.5 & 1 & 0 \\\\\n1 & 2 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\n2 & 1 & 4 \\\\\n0 & 1.5 & 0 \\\\\n0 & 0 & 2\n\\end{pmatrix}\n\\]\nNote that\n\nthe matrix \\(U\\) is the same as the fully eliminated upper triangular form produced by Gaussian elimination;\n\\(L\\) contains the multipliers that were used at each stage to eliminate the rows.\n\n\n\n\n\nWe can implement computation of the LU factorisation:\n\ndef lu_factorisation(A):\n    \"\"\"\n    compute the LU factorisation of A\n    returns the factors L and U\n    \"\"\"\n    n, m = A.shape\n    assert n == m\n\n    # construct arrays of zeros\n    L, U = np.zeros_like(A), np.zeros_like(A)\n\n    # fill entries\n    for i in range(n):\n        L[i, i] = 1\n        # compute entries in U\n        for j in range(i, n):\n            U[i, j] = A[i, j] - sum(L[i, k] * U[k, j] for k in range(i))\n        # compute entries in L\n        for j in range(i + 1, n):\n            L[j, i] = (A[j, i] - sum(L[j, k] * U[k, i] for k in range(i))) / U[i, i]\n\n    return L, U\n\nand test our implementation:\n\nA = np.array([[2.0, 1.0, 4.0], [1.0, 2.0, 2.0], [2.0, 4.0, 6.0]])\n\nprint(\"matrix:\")\nprint_array(A)\nprint()\n\nprint(\"performing factorisation\")\nL, U = lu_factorisation(A)\nprint()\n\nprint(\"factorisation:\")\nprint_array(L)\nprint_array(U)\nprint()\n\nprint(\"Is L lower triangular?\", np.allclose(L, np.tril(L)))\nprint(\"Is U lower triangular?\", np.allclose(U, np.triu(U)))\nprint(\"Is LU a factorisation of A?\", np.allclose(L @ U, A))\n\nmatrix:\nA = [  2.0,  1.0,  4.0 ]\n    [  1.0,  2.0,  2.0 ]\n    [  2.0,  4.0,  6.0 ]\n\nperforming factorisation\n\nfactorisation:\nL = [  1.0,  0.0,  0.0 ]\n    [  0.5,  1.0,  0.0 ]\n    [  1.0,  2.0,  1.0 ]\nU = [  2.0,  1.0,  4.0 ]\n    [  0.0,  1.5,  0.0 ]\n    [  0.0,  0.0,  2.0 ]\n\nIs L lower triangular? True\nIs U lower triangular? True\nIs LU a factorisation of A? True\n\n\nand then add LU factorisation times to our plot:\n\n\n\n\n\n\n\n\n\nWe see that LU factorisation is still \\(O(n^3)\\) and that the run times are similar to Gaussian elimination. But, importantly, we can reuse this factorisation more cheaply for different right-hand sides \\(\\vec{b}\\).",
    "crumbs": [
      "Direct solvers for systems of linear equtaions"
    ]
  },
  {
    "objectID": "src/lec04.html#effects-of-finite-precision-arithmetic",
    "href": "src/lec04.html#effects-of-finite-precision-arithmetic",
    "title": "1 Direct solvers for systems of linear equtaions",
    "section": "",
    "text": "Example 1\n\n\n\nConsider the following linear system of equations\n\\[\n\\begin{pmatrix}\n0 & 2 & 1 \\\\\n2 & 1 & 0 \\\\\n1 & 2 & 0\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n7 \\\\ 4 \\\\ 5\n\\end{pmatrix}\n\\]\nProblem. We cannot eliminate the first column by the diagonal by adding multiples of row 1 to rows 2 and 3 respectively.\nSolution. Swap the order of the equations!\n\nSwap rows 1 and 2:\n\\[\n\\begin{pmatrix}\n2 & 1 & 0 \\\\\n0 & 2 & 1 \\\\\n1 & 2 & 0\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n4 \\\\ 7 \\\\ 5\n\\end{pmatrix}\n\\]\nNow apply Gaussian elimination\n\\[\n\\begin{pmatrix}\n2 & 1 & 0 \\\\\n0 & 2 & 1 \\\\\n0 & 1.5 & 0\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n4 \\\\ 7 \\\\ 3\n\\end{pmatrix}\n;\n\\begin{pmatrix}\n2 & 1 & 0 \\\\\n0 & 2 & 1 \\\\\n0 & 0 & -0.75\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n4 \\\\ 7 \\\\ -2.25\n\\end{pmatrix}.\n\\]\n\n\n\n\n\n\n\n\n\nExample 2\n\n\n\nConsider another system of equations\n\\[\n\\begin{pmatrix}\n2 & 1 & 1 \\\\\n4 & 2 & 1 \\\\\n2 & 2 & 0\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n3 \\\\ 5 \\\\ 2\n\\end{pmatrix}\n\\]\n\nApply Gaussian elimination as usual:\n\\[\n\\begin{pmatrix}\n2 & 1 & 1 \\\\\n0 & 0 & -1 \\\\\n2 & 2 & 0\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n3 \\\\ -1 \\\\ 2\n\\end{pmatrix}\n;\n\\begin{pmatrix}\n2 & 1 & 1 \\\\\n0 & 0 & -1 \\\\\n0 & 1 & -1\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n3 \\\\ -1 \\\\ -1\n\\end{pmatrix}\n\\]\nProblem. We cannot eliminate the second column below the diagonal by adding a multiple of row 2 to row 3.\nAgain this problem may be overcome simply by swapping the order of the equations - this time swapping rows 2 and 3:\n\\[\n\\begin{pmatrix}\n2 & 1 & 1 \\\\\n0 & 1 & -1 \\\\\n0 & 0 & -1\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n3 \\\\ -1 \\\\ -1\n\\end{pmatrix}\n\\]\nWe can now continue the Gaussian elimination process as usual.\n\n\n\nIn general. Gaussian elimination requires row swaps to avoid breaking down when there is a zero in the “pivot” position. This might be a familiar aspect of Gaussian elimination, but there is an additional reason to apply pivoting when working with floating point numbers:\n\n\n\n\n\n\nExample 3\n\n\n\nConsider using Gaussian elimination to solve the linear system of equations given by\n\\[\n\\begin{pmatrix}\n\\varepsilon & 1 \\\\\n1 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n2 + \\varepsilon \\\\ 3\n\\end{pmatrix}\n\\]\nwhere \\(\\varepsilon \\neq 1\\).\n\nThe true, unique solution is \\((x_1, x_2)^T = (1, 2)^T\\).\nIf \\(\\varepsilon \\neq 0\\), Gaussian elimination gives\n\\[\n\\begin{pmatrix}\n\\varepsilon & 1 \\\\\n0 & 1 - \\frac{1}{\\varepsilon}\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n2 + \\varepsilon \\\\ 3 - \\frac{2 + \\varepsilon}{\\varepsilon}\n\\end{pmatrix}\n\\]\nProblems occur not only when \\(\\varepsilon = 0\\) but also when it is very small, i.e. when \\(\\frac{1}{\\varepsilon}\\) is very large, this will introduce very significant rounding errors into the computation.\n\nUse Gaussian elimination to solve the linear system of equations given by\n\\[\n\\begin{pmatrix}\n1 & 1 \\\\\n\\varepsilon & 1\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n3 \\\\ 2 + \\varepsilon\n\\end{pmatrix}\n\\]\nwhere \\(\\varepsilon \\neq 1\\).\n\nThe true solution is still \\((x_1, x_2)^T = (1, 2)^T\\).\nGaussian elimination now gives\n\\[\n\\begin{pmatrix}\n1 & 1 \\\\\n0 & 1 - \\varepsilon\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n3 \\\\ 2 - 2\\varepsilon\n\\end{pmatrix}\n\\]\nThe problems due to small values of \\(\\varepsilon\\) have disappeared.\n\nThis is a genuine problem we see in the code versions too!\n\nprint(\"without row swapping:\")\nfor eps in [1.0e-2, 1.0e-4, 1.0e-6, 1.0e-8, 1.0e-10, 1.0e-12, 1.0e-14]:\n    A = np.array([[eps, 1.0], [1.0, 1.0]])\n    b = np.array([[2.0 + eps], [3.0]])\n\n    gaussian_elimination(A, b)\n    x = backward_substitution(A, b)\n    print(f\"{eps=:.1e}\", end=\", \")\n    print_array(x.T, \"x.T\", end=\", \")\n\n    A = np.array([[eps, 1.0], [1.0, 1.0]])\n    b = np.array([[2.0 + eps], [3.0]])\n    print(\"Solution?\", np.allclose(A @ x, b))\nprint()\n\nprint(\"with row swapping:\")\nfor eps in [1.0e-2, 1.0e-4, 1.0e-6, 1.0e-8, 1.0e-10, 1.0e-12, 1.0e-14]:\n    A = np.array([[1.0, 1.0], [eps, 1.0]])\n    b = np.array([[3.0], [2.0 + eps]])\n\n    gaussian_elimination(A, b)\n    x = backward_substitution(A, b)\n    print(f\"{eps=:.1e}\", end=\", \")\n    print_array(x.T, \"x.T\", end=\", \")\n\n    A = np.array([[1.0, 1.0], [eps, 1.0]])\n    b = np.array([[3.0], [2.0 + eps]])\n    print(\"Solution?\", np.allclose(A @ x, b))\nprint()\n\nwithout row swapping:\neps=1.0e-02, x.T = [  1.0,  2.0 ], Solution? True\neps=1.0e-04, x.T = [  1.0,  2.0 ], Solution? True\neps=1.0e-06, x.T = [  1.0,  2.0 ], Solution? True\neps=1.0e-08, x.T = [  1.0,  2.0 ], Solution? True\neps=1.0e-10, x.T = [  1.0,  2.0 ], Solution? True\neps=1.0e-12, x.T = [  1.00009,  2.00000 ], Solution? False\neps=1.0e-14, x.T = [  1.0214,  2.0000 ], Solution? False\n\nwith row swapping:\neps=1.0e-02, x.T = [  1.0,  2.0 ], Solution? True\neps=1.0e-04, x.T = [  1.0,  2.0 ], Solution? True\neps=1.0e-06, x.T = [  1.0,  2.0 ], Solution? True\neps=1.0e-08, x.T = [  1.0,  2.0 ], Solution? True\neps=1.0e-10, x.T = [  1.0,  2.0 ], Solution? True\neps=1.0e-12, x.T = [  1.0,  2.0 ], Solution? True\neps=1.0e-14, x.T = [  1.0,  2.0 ], Solution? True\n\n\n\n\n\n\nRemark. \n\nWriting the equations in a different order has removed the previous problem.\nThe diagonal entries are now always relatively larger.\nThe interchange of the order of equations is a simple example of row pivoting. This strategy avoids excessive rounding errors in the computations.\n\n\n\n\n\nBefore eliminating entries in column \\(j\\):\n\nfind the entry in column \\(j\\), below the diagonal, of maximum magnitude;\nif that entry is larger in magnitude than the diagonal entry then swap its row with row \\(j\\).\n\nThen eliminate column \\(j\\) as before.\n\nThis algorithm will always work when the matrix \\(A\\) is invertible/non-singular. Conversely, if all of the possible pivot values are zero this implies that the matrix is singular and a unique solution does not exist. At each elimination step the row multiplies used are guaranteed to be at most one in magnitude so any errors in the representation of the system cannot be amplified by the elimination process. As always, solving \\(A \\vec{x} = \\vec{b}\\) requires that the entries in \\(\\vec{b}\\) are also swapped in the appropriate way. Pivoting can be applied in an equivalent way to LU factorisation. The sequence of pivots is independent of the vector \\(\\vec{b}\\) and can be recorded and reused. The constraint imposed on the row multipliers means that for LU factorisation every entry in \\(L\\) satisfies \\(| l_{ij} | \\le 1\\).\n\n\n\n\n\n\nExample\n\n\n\nConsider the linear system of equations given by\n\\[\n\\begin{pmatrix}\n10 & -7 & 0 \\\\\n-3 & 2.1 - \\varepsilon & 6 \\\\\n5 & -1 & 5\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n7 \\\\ 9.9 + \\varepsilon \\\\ 11\n\\end{pmatrix}\n\\]\nwhere \\(0 \\le \\varepsilon \\ll 1\\), and solve it using\n\nGaussian elimination without pivoting\nGaussian elimination with pivoting.\n\nThe exact solution is \\(\\vec{x} = (0, -1, 2)^T\\) for any \\(\\varepsilon\\) in the given range.\n1. Solve the system using Gaussian elimination with no pivoting.\nEliminating the first column gives\n\\[\n\\begin{pmatrix}\n10 & -7 & 0 \\\\\n0 & -\\varepsilon & 6 \\\\\n0 & 2.5 & 5\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n7 \\\\ 12 + \\varepsilon \\\\ 7.5\n\\end{pmatrix}\n\\]\nand then the second column gives\n\\[\n\\begin{pmatrix}\n10 & -7 & 0 \\\\\n0 & -\\varepsilon & 6 \\\\\n0 & 0 & 5 + 15/\\varepsilon\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n7 \\\\ 12 + \\varepsilon \\\\ 7.5 + 2.5(12 + \\varepsilon)/\\varepsilon\n\\end{pmatrix}\n\\]\nwhich leads to\n\\[\nx_3 = \\frac{3 + \\frac{12 + \\varepsilon}{\\varepsilon}}{2 + \\frac{6}{\\varepsilon}} \\qquad\nx_2 = \\frac{(12 + \\varepsilon) - 6x_3}{-\\varepsilon} \\qquad\nx_1 = \\frac{7+ 7x_2}{10}.\n\\]\nThere are many divisions by \\(\\varepsilon\\), so we will have problems if \\(\\varepsilon\\) is (very) small.\n2. Solve the system using Gaussian elimination with pivoting.\nThe first stage is identical (because \\(a_{11} = 10\\) is largest).\n\\[\n\\begin{pmatrix}\n10 & -7 & 0 \\\\\n0 & -\\varepsilon & 6 \\\\\n0 & 2.5 & 5\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n7 \\\\ 12 + \\varepsilon \\\\ 7.5\n\\end{pmatrix}\n\\]\nbut now \\(|a_{22}| = \\varepsilon\\) and \\(|a_{32}| = 2.5\\) so we swap rows 2 and 3 to give\n\\[\n\\begin{pmatrix}\n10 & -7 & 0 \\\\\n0 & 2.5 & 5 \\\\\n0 & -\\varepsilon & 6\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n7 \\\\ 7.5 \\\\ 12 + \\varepsilon\n\\end{pmatrix}\n\\]\nNow we may eliminate column 2:\n\\[\n\\begin{pmatrix}\n10 & -7 & 0 \\\\\n0 & 2.5 & 5 \\\\\n0 & 0 & 6 + 2 \\varepsilon\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n7 \\\\ 7.5 \\\\ 12 + 4 \\varepsilon\n\\end{pmatrix}\n\\]\nwhich leads to the exact answer:\n\\[\nx_3 = \\frac{12 + 4\\varepsilon}{6 + 2 \\varepsilon} = 2 \\qquad\nx_2 = \\frac{7.5 - 5x_3}{2.5} = -1 \\qquad\nx_1 = \\frac{7 + 7 x_2}{10} = 0.\n\\]\n\n\n\n\n\n\ndef gaussian_elimination_with_pivoting(A, b, verbose=False):\n    \"\"\"\n    perform Gaussian elimnation with pivoting to reduce the system of linear equations Ax=b to upper triangular form\n    use verbose to print out intermediate representations\n    \"\"\"\n    # find shape of system\n    n = system_size(A, b)\n\n    # perform forwards elimination\n    for i in range(n - 1):\n        # eliminate column i\n        if verbose:\n            print(f\"eliminating column {i}\")\n\n        # find largest entry in column i\n        largest = abs(A[i, i])\n        j_max = i\n        for j in range(i + 1, n):\n            if abs(A[j, i]) &gt; largest:\n                largest, j_max = abs(A[j, i]), j\n\n        # swap rows j_max and i\n        row_swap(A, b, i, j_max)\n        if verbose:\n            print(f\"swapped system ({i} &lt;-&gt; {j_max})\")\n            print_array(A)\n            print_array(b)\n            print()\n\n        for j in range(i + 1, n):\n            # row j\n            factor = A[j, i] / A[i, i]\n            if verbose:\n                print(f\"row {j} |-&gt; row {j} - {factor} * row {i}\")\n            row_add(A, b, j, -factor, i)\n\n        if verbose:\n            print(\"new system\")\n            print_array(A)\n            print_array(b)\n            print()\n\nGaussian elimination without pivoting following by back subsitution:\n\neps = 1.0e-14\nA = np.array([[10.0, -7.0, 0.0], [-3.0, 2.1 - eps, 6.0], [5.0, -1.0, 5.0]])\nb = np.array([[7.0], [9.9 + eps], [11.0]])\n\nprint(\"starting system:\")\nprint_array(A)\nprint_array(b)\nprint()\n\nprint(\"performing Gaussian elimination without pivoting\")\ngaussian_elimination(A, b, verbose=True)\nprint()\n\nprint(\"upper triangular system:\")\nprint_array(A)\nprint_array(b)\nprint()\n\nprint(\"performing backward substitution\")\nx = backward_substitution(A, b)\nprint()\n\nprint(\"solution using backward substitution:\")\nprint_array(x)\nprint()\n\nA = np.array([[10.0, -7.0, 0.0], [-3.0, 2.1 - eps, 6.0], [5.0, -1.0, 5.0]])\nb = np.array([[7.0], [9.9 + eps], [11.0]])\nprint(\"Does x solve the original system?\", np.allclose(A @ x, b))\n\nstarting system:\nA = [ 10.0, -7.0,  0.0 ]\n    [ -3.0,  2.1,  6.0 ]\n    [  5.0, -1.0,  5.0 ]\nb = [  7.0 ]\n    [  9.9 ]\n    [ 11.0 ]\n\nperforming Gaussian elimination without pivoting\neliminating column 0\n  row 1 |-&gt; row 1 - -0.3 * row 0\n  row 2 |-&gt; row 2 - 0.5 * row 0\n\nnew system\nA = [ 10.0, -7.0,  0.0 ]\n    [  0.0, -0.0,  6.0 ]\n    [  0.0,  2.5,  5.0 ]\nb = [  7.0 ]\n    [ 12.0 ]\n    [  7.5 ]\n\neliminating column 1\n  row 2 |-&gt; row 2 - -244760849313613.9 * row 1\n\nnew system\nA = [ 10.0, -7.0,  0.0 ]\n    [  0.0, -0.0,  6.0 ]\n    [  0.0,  0.0, 1468565095881688.5 ]\nb = [  7.0 ]\n    [ 12.0 ]\n    [ 2937130191763377.0 ]\n\n\nupper triangular system:\nA = [ 10.0, -7.0,  0.0 ]\n    [  0.0, -0.0,  6.0 ]\n    [  0.0,  0.0, 1468565095881688.5 ]\nb = [  7.0 ]\n    [ 12.0 ]\n    [ 2937130191763377.0 ]\n\nperforming backward substitution\n\nsolution using backward substitution:\nx = [ -0.030435 ]\n    [ -1.043478 ]\n    [  2.000000 ]\n\nDoes x solve the original system? False\n\n\nGaussian elimination with pivoting following by back subsitution:\n\neps = 1.0e-14\nA = np.array([[10.0, -7.0, 0.0], [-3.0, 2.1 - eps, 6.0], [5.0, -1.0, 5.0]])\nb = np.array([[7.0], [9.9 + eps], [11.0]])\n\nprint(\"starting system:\")\nprint_array(A)\nprint_array(b)\nprint()\n\nprint(\"performing Gaussian elimination with pivoting\")\ngaussian_elimination_with_pivoting(A, b, verbose=True)\nprint()\n\nprint(\"upper triangular system:\")\nprint_array(A)\nprint_array(b)\nprint()\n\nprint(\"performing backward substitution\")\nx = backward_substitution(A, b)\nprint()\n\nprint(\"solution using backward substitution:\")\nprint_array(x)\nprint()\n\nA = np.array([[10.0, -7.0, 0.0], [-3.0, 2.1 - eps, 6.0], [5.0, -1.0, 5.0]])\nb = np.array([[7.0], [9.9 + eps], [11.0]])\nprint(\"Does x solve the original system?\", np.allclose(A @ x, b))\n\nstarting system:\nA = [ 10.0, -7.0,  0.0 ]\n    [ -3.0,  2.1,  6.0 ]\n    [  5.0, -1.0,  5.0 ]\nb = [  7.0 ]\n    [  9.9 ]\n    [ 11.0 ]\n\nperforming Gaussian elimination with pivoting\neliminating column 0\nswapped system (0 &lt;-&gt; 0)\nA = [ 10.0, -7.0,  0.0 ]\n    [ -3.0,  2.1,  6.0 ]\n    [  5.0, -1.0,  5.0 ]\nb = [  7.0 ]\n    [  9.9 ]\n    [ 11.0 ]\n\nrow 1 |-&gt; row 1 - -0.3 * row 0\nrow 2 |-&gt; row 2 - 0.5 * row 0\nnew system\nA = [ 10.0, -7.0,  0.0 ]\n    [  0.0, -0.0,  6.0 ]\n    [  0.0,  2.5,  5.0 ]\nb = [  7.0 ]\n    [ 12.0 ]\n    [  7.5 ]\n\neliminating column 1\nswapped system (1 &lt;-&gt; 2)\nA = [ 10.0, -7.0,  0.0 ]\n    [  0.0,  2.5,  5.0 ]\n    [  0.0, -0.0,  6.0 ]\nb = [  7.0 ]\n    [  7.5 ]\n    [ 12.0 ]\n\nrow 2 |-&gt; row 2 - -4.085620730620576e-15 * row 1\nnew system\nA = [ 10.0, -7.0,  0.0 ]\n    [  0.0,  2.5,  5.0 ]\n    [  0.0,  0.0,  6.0 ]\nb = [  7.0 ]\n    [  7.5 ]\n    [ 12.0 ]\n\n\nupper triangular system:\nA = [ 10.0, -7.0,  0.0 ]\n    [  0.0,  2.5,  5.0 ]\n    [  0.0,  0.0,  6.0 ]\nb = [  7.0 ]\n    [  7.5 ]\n    [ 12.0 ]\n\nperforming backward substitution\n\nsolution using backward substitution:\nx = [  0.0 ]\n    [ -1.0 ]\n    [  2.0 ]\n\nDoes x solve the original system? True",
    "crumbs": [
      "Direct solvers for systems of linear equtaions"
    ]
  },
  {
    "objectID": "src/lec04.html#further-reading",
    "href": "src/lec04.html#further-reading",
    "title": "1 Direct solvers for systems of linear equtaions",
    "section": "",
    "text": "Some basic reading:\n\nWikipedia: Gaussian elimination\nJoseph F. Grcar. How ordinary elimination became Gaussian elimination. Historia Mathematica. Volume 38, Issue 2, May 2011. (More history)\n\nSome reading on LU factorisation:\n\n\\(A = LU\\) and solving systems [pdf]\nWikipedia: LU decomposition\nWikipedia: Matrix decomposition (Other examples of decompositions).\nNick Higham: What is an LU factorization? (a very mathematical treatment with additional references)\n\nSome reading on using Gaussian elimination with pivoting:\n\nGaussian elimination with Partial Pivoting [pdf]\nGaussian elimination with partial pivoting example [pdf]\n\nA good general reference for this area:\n\nTrefethen, Lloyd N.; Bau, David (1997), Numerical linear algebra, Philadelphia: Society for Industrial and Applied Mathematics, ISBN 978-0-89871-361-9.\n\nSome implementations:\n\nNumpy numpy.linalg.solve\nScipy scipy.linalg.lu\nLAPACK Gaussian elimination (uses LU factorisation): dgesv()\nLAPACK LU Factorisation: dgetrf().",
    "crumbs": [
      "Direct solvers for systems of linear equtaions"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to LA",
    "section": "",
    "text": "floating point numbers\n\nrounding errors\ntop tips for working with floating point numbers\n\nlinear systems of equations\n\nmotivation\nGaussian-elimination\npracticalities\niterative schemes\n\ntransforming matrices\n\nwhat is a basis? and what does it mean?\neigenvectors and eigenvalues\n\ndefinitions\nsmall matrix solutions\n\nlarge matrix solutions\n\n\nTODO add recap of matrices terminology - inverse, transpose, coefficients, size"
  },
  {
    "objectID": "index.html#contents-of-this-submodule",
    "href": "index.html#contents-of-this-submodule",
    "title": "Welcome to LA",
    "section": "",
    "text": "floating point numbers\n\nrounding errors\ntop tips for working with floating point numbers\n\nlinear systems of equations\n\nmotivation\nGaussian-elimination\npracticalities\niterative schemes\n\ntransforming matrices\n\nwhat is a basis? and what does it mean?\neigenvectors and eigenvalues\n\ndefinitions\nsmall matrix solutions\n\nlarge matrix solutions\n\n\nTODO add recap of matrices terminology - inverse, transpose, coefficients, size"
  },
  {
    "objectID": "index.html#motivations",
    "href": "index.html#motivations",
    "title": "Welcome to LA",
    "section": "2 Motivations",
    "text": "2 Motivations"
  },
  {
    "objectID": "index.html#learning-outcomes",
    "href": "index.html#learning-outcomes",
    "title": "Welcome to LA",
    "section": "3 Learning outcomes",
    "text": "3 Learning outcomes"
  },
  {
    "objectID": "index.html#programming",
    "href": "index.html#programming",
    "title": "Welcome to LA",
    "section": "4 Programming",
    "text": "4 Programming"
  },
  {
    "objectID": "src/lec05.html",
    "href": "src/lec05.html",
    "title": "1 Iterative solutions of linear equations",
    "section": "",
    "text": "In the previous section we looked at what are known as direct methods for solving systems of linear equations. They are guaranteed to produce a solution with a fixed amount of work (we can even prove this in exact arithmetic!), but this fixed amount of work may be very large.\nFor a general \\(n \\times n\\) system of linear equations \\(A \\vec{x} = \\vec{b}\\), the computation expense of all direct methods if \\(O(n^3)\\). The amount of storage required for these approaches is \\(O(n^2)\\) which is dominated by the cost of storing the matrix \\(A\\). As \\(n\\) becomes larger the storage and computation work required limit the practicality of direct approaches.\nAs an alternative, we will propose some iterative methods. Iterative methods produce a sequence \\((\\vec{x}^{(k)})\\) of approximations to the solution of the linear system of equations \\(A \\vec{x} = \\vec{b}\\). The iteration is defined recursively and is typically of the form: \\[\n\\vec{x}^{(k+1)} = \\vec{F}(\\vec{x}^{(k)}),\n\\] where \\(\\vec{x}^{(k)}\\) is now a vector of values and \\(\\vec{F}\\) is some vector function (which needs to be defined to define the method). We will need to choose a starting value \\(\\vec{x}^{(k)}\\) but there is often a reasonable approximation which can be used. Once all this is defined, we still need to decide when we need to stop!\n\n\n\n\n\n\nSome very bad examples\n\n\n\nExample 1\nConsider\n\\[\n\\vec{F}(\\vec{x}^{(k)}) = \\vec{x}^{(k)}.\n\\]\nEach iteration is very cheap to compute but very inaccurate - it never converges!\nExample 2\nConsider\n\\[\n\\vec{F}(\\vec{x}^{(k)}) = \\vec{x}^{(k)} + A^{-1} (\\vec{b} - A \\vec{x}^{(k)}).\n\\]\nEach iteration is very expensive to compute - you have to invert \\(A\\)! - but it converges in just one step since\n\\[\n\\begin{aligned}\nA \\vec{x}^{(k+1)} & = A \\vec{x}^{(k)} + A A^{-1} (\\vec{b} - A \\vec{x}^{(k)}) \\\\\n& = A \\vec{x}^{(k)} + \\vec{b} - A \\vec{x}^{(k)} \\\\\n& = \\vec{b}.\n\\end{aligned}\n\\]\n\n\nKey idea\nThe key point here is we want a method which is both cheap to compute but converges quickly to the solution. One way to do this is to construct iteration given by\n\\[\\begin{equation}\n\\label{eq:general-iteration}\n\\vec{F}(\\vec{x}^{(k)}) = \\vec{x}^{(k)} + P (\\vec{b} - A \\vec{x}^{(k)}).\n\\end{equation}\\]\nfor some matrix \\(P\\) such that\n\n\\(P\\) is easy to compute, or the matrix vector product \\(P \\vec{r}\\) is easy to compute,\n\\(P\\) approximates \\(A^{-1}\\) well enough that the algorithm converges in few iterations.\n\nWe call \\(\\vec{b} - A \\vec{x}^{(k)} = \\vec{r}\\) the residual. Note in the above examples we would have \\(P = O\\) (the zero matrix) or \\(P = A^{-1}\\).\n\n\nOne simple choice for \\(P\\) is given by the Jacobi method where we take \\(P = D^{-1}\\) where \\(D\\) is the diagonal of \\(A\\): \\[\nD_{ii} = A_{ii} \\quad \\text{and} \\quad D_{ij} = 0 \\text{ for } i \\neq j.\n\\]\nThe Jacobi iteration is given by\n\\[\n\\vec{x}^{(k+1)} = \\vec{x}^{(k)} + D^{-1}(\\vec{b} - A \\vec{x}^{(k)})\n\\]\n\\(D\\) is a diagonal matrix, so \\(D^{-1}\\) is trivial to form (as long as the diagonal entries are all nonzero): \\[\n(D^{-1})_{ii} = \\frac{1}{D_{ii}}\n\\quad \\text{and} \\quad\n(D^{-1})_{ij} = 0 \\text{ for } i \\neq j.\n\\]\n\nRemark. \n\nThe cost of one iteration is \\(O(n^2)\\) for a full matrix, and this is dominated by the matrix-vector product \\(A \\vec{x}^{(k)}\\).\nThis cost can be reduced to \\(O(n)\\) if the matrix \\(A\\) is sparse - this is when iterative methods are especially attractive (TODO add future ref).\nThe amount of work also depends on the number of iterations required to get a “satisfactory” solution.\n\nThe number of iterations depends on the matrix;\nFewer iterations are needed for a less accurate solution;\nA good initial estimate \\(\\vec{x}^{(0)}\\) reduces the required number of iterations.\n\nUnfortunately, the iteration might not converge!\n\n\nThe Jacobi iteration updates all elements of \\(\\vec{x}^{(k)}\\) simultaneously to get \\(\\vec{x}^{(k+1)}\\). Writing the method out component by component gives\n\\[\n\\begin{aligned}\nx_1^{(k+1)} &= x_1^{(k)} + \\frac{1}{A_{11}} \\left( b_1 - \\sum_{j=1}^n A_{1j} x_j^{(k)} \\right) \\\\\nx_2^{(k+1)} &= x_2^{(k)} + \\frac{1}{A_{22}} \\left( b_2 - \\sum_{j=1}^n A_{2j} x_j^{(k)} \\right) \\\\\n\\vdots \\quad & \\hphantom{=} \\quad \\vdots \\\\\nx_n^{(k+1)} &= x_n^{(k)} + \\frac{1}{A_{nn}} \\left( b_n - \\sum_{j=1}^n A_{nj} x_j^{(k)} \\right).\n\\end{aligned}\n\\]\nNote that once the first step has been taken, \\(x_1^{(k+1)}\\) is already known, but the Jacobi iteration does not make use of this information!\n\n\n\n\n\n\nExample 1\n\n\n\nTake two iterations of Jacobi iteration to approximate the solution of the following system using the initial guess \\(\\vec{x}^{(0)} = (1, 1)^T\\): \\[\n\\begin{pmatrix}\n2 & 1 \\\\ -1 & 4\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n3.5 \\\\ 0.5\n\\end{pmatrix}\n\\]\nStarting from \\(\\vec{x}^{(0)} = (1, 1)^T\\), the first iteration is \\[\n\\begin{aligned}\nx_1^{(1)} &= x_1^{(0)} + \\frac{1}{A_{11}} \\left( b_1 - A_{11} x_1^{(0)} - A_{12} x_2^{(0)} \\right) \\\\\n&= 1 + \\frac{1}{2} (3.5 - 2 \\times 1 - 1 \\times 1) = 1.25 \\\\\nx_2^{(1)} &= x_2^{(0)} + \\frac{1}{A_{22}} \\left( b_2 - A_{21} x_1^{(0)} - A_{22} x_2^{(0)} \\right) \\\\\n&= 1 + \\frac{1}{4} (0.5 - (-1) \\times 1 - 4 \\times 1) = 0.375. \\\\\n\\end{aligned}\n\\] So we have \\(\\vec{x}^{(1)} = (1.25, 0.375)^T\\). Then the second iteration is \\[\n\\begin{aligned}\nx_1^{(2)} &= x_1^{(1)} + \\frac{1}{A_{11}} \\left( b_1 - A_{11} x_1^{(1)} - A_{12} x_2^{(1)} \\right) \\\\\n&= 1.25 + \\frac{1}{2} (3.5 - 2 \\times 1.25 - 1 \\times 0.375) = 1.5625 \\\\\nx_2^{(2)} &= x_2^{(1)} + \\frac{1}{A_{22}} \\left( b_2 - A_{21} x_1^{(1)} - A_{22} x_2^{(1)} \\right) \\\\\n&= 0.375 + \\frac{1}{4} (0.5 - (-1) \\times 1.25 - 4 \\times 0.375) = 0.4375. \\\\\n\\end{aligned}\n\\] So we have \\(\\vec{x}^{(2)} = (1.5625, 0.4375)\\).\nNote the only difference between the formulae for Iteration 1 and 2 is the iteration number, the superscript in brackets. The exact solution is given by \\(\\vec{x} = (1.5, 0.5)^T\\).\n\n\nWe note that we can also slightly simplify the way the Jacobi iteration is written. We can expand \\(A\\) into \\(A = L + D + U\\), where \\(L\\) and \\(U\\) are the parts of the matrix from below and above the diagonal respectively: \\[\nL_{ij} = \\begin{cases}\nA_{ij} &\\quad \\text{if } i &lt; j \\\\\n0 &\\quad \\text{if } i \\ge j,\n\\end{cases}\n\\qquad\nU_{ij} = \\begin{cases}\nA_{ij} &\\quad \\text{if } i &gt; j \\\\\n0 &\\quad \\text{if } i \\le j.\n\\end{cases}\n\\] The we can calculate that: \\[\n\\begin{aligned}\n\\vec{x}^{(k+1)} & = \\vec{x}^{(k)} + D^{-1}(\\vec{b} - A \\vec{x}^{(k)}) \\\\\n& = \\vec{x}^{(k)} + D^{-1}(\\vec{b} - (L + D + U) \\vec{x}^{(k)}) \\\\\n& = \\vec{x}^{(k)} - D^{-1} D \\vec{x}^{(k)} + D^{-1}(\\vec{b} - (L + U) \\vec{x}^{(k)}) \\\\\n& = \\vec{x}^{(k)} - \\vec{x}^{(k)} + D^{-1}(\\vec{b} - (L + U) \\vec{x}^{(k)}) \\\\\n& = D^{-1}(\\vec{b} - (L + U) \\vec{x}^{(k)}).\n\\end{aligned}\n\\] In this formulation, we do not explicitly form the residual as part of the computations. In practical situations, this may be a simpler formulation we can use if we have knowledge of the coefficients of \\(A\\), but this is not always true!\n\n\n\nAs an alternative to Jacobi iteration, the iteration might use \\(x_i^{(k+1)}\\) as soon as it is calculated (rather than using the previous iteration), giving\n\\[\n\\begin{aligned}\nx_1^{(k+1)}\n& = x_1^{(k)} + \\frac{1}{A_{11}} \\left(\nb_1 - \\sum_{j=1}^n A_{1j} x_j^{(k)}\n\\right) \\\\\nx_2^{(k+1)}\n& = x_2^{(k)} + \\frac{1}{A_{22}} \\left(\nb_2 - A_{21} x_1^{(k+1)} - \\sum_{j=2}^n A_{2j} x_j^{(k)}\n\\right) \\\\\nx_3^{(k+1)}\n& = x_3^{(k)} + \\frac{1}{A_{33}} \\left(\nb_3 - \\sum_{j=1}^2 A_{3j} x_j^{(k+1)} - \\sum_{j=3}^n A_{3j} x_j^{(k)}\n\\right) \\\\\n\\vdots \\quad & \\hphantom{=} \\quad \\vdots \\\\\nx_i^{(k+1)}\n& = x_i^{(k)} + \\frac{1}{A_{ii}} \\left(\nb_i - \\sum_{j=1}^{i-1} A_{ij} x_j^{(k+1)} - \\sum_{j=i}^n A_{ij} x_j^{(k)}\n\\right) \\\\\n\\vdots \\quad & \\hphantom{=} \\quad \\vdots \\\\\nx_n^{(k+1)}\n& = x_n^{(k)} + \\frac{1}{A_{nn}} \\left(\nb_n - \\sum_{j=1}^{n-1} A_{nj} x_j^{(k+1)} - A_{nn} x_n^{(k)}\n\\right).\n\\end{aligned}\n\\]\nConsider the system \\(A \\vec{x}= b\\) with the matrix \\(A\\) split as \\(A = L + D + U\\) where \\(D\\) is the diagonal of \\(A\\), \\(L\\) contains the elements below the diagonal and \\(U\\) contains the elements above the diagonal. The componentwise iteration above can be written in matrix form as \\[\n\\begin{aligned}\n\\vec{x}^{(k+1)} & = \\vec{x}^{(k)} + D^{-1} (\\vec{b} - L \\vec{x}^{(k+1)} - (D + U) \\vec{x}^{(k)}) \\\\\n& = \\vec{x}^{(k)} - D^{-1} L \\vec{x}^{(k+1)} + D^{-1} (\\vec{b} - (D + U) \\vec{x}^{(k)}) \\\\\n& = \\vec{x}^{(k)} - D^{-1} L \\vec{x}^{(k+1)} + D^{-1} L \\vec{x}^{(k)} + D^{-1} (\\vec{b} - (L + D + U) \\vec{x}^{(k)}) \\\\\n\\vec{x}^{(k+1)} + D^{-1} L \\vec{x}^{(k+1)} & = \\vec{x}^{(k)} + D^{-1} L \\vec{x}^{(k)} + D^{-1} (\\vec{b} - (L + D + U) \\vec{x}^{(k)}) \\\\\nD^{-1} (D + L) \\vec{x}^{(k+1)}  & = D^{-1} (D + L) \\vec{x}^{(k)} + D^{-1} (\\vec{b} - A \\vec{x}^{(k)}) \\\\\n(D + L) \\vec{x}^{(k+1)} &= D D^{-1} (D + L) \\vec{x}^{(k)} + D D^{-1} (\\vec{b} - A \\vec{x}^{(k)}) \\\\\n& = (D + L) \\vec{x}^{(k)} + (\\vec{b} - A \\vec{x}^{(k)}) \\\\\n\\vec{x}^{(k+1)} &= (D + L)^{-1} (D + L) \\vec{x}^{(k)} + (D + L)^{-1} (\\vec{b} - A \\vec{x}^{(k)}) \\\\\n& = \\vec{x}^{(k)} + (D + L)^{-1} (\\vec{b} - A \\vec{x}^{(k)}).\n\\end{aligned}\n\\]\n…and hence the Gauss-Seidel iteration\n\\[\n\\vec{x}^{(k+1)} = \\vec{x}^{(k)} + (D + L)^{-1} (\\vec{b} - A \\vec{x}^{(k)}).\n\\] That is, we use \\(P = (D+L)^{-1}\\) in \\(\\eqref{eq:general-iteration}\\).\nIn general, we don’t form the inverse of \\(D + L\\) explicitly here since it is more complicated to do so than for simply computing the inverse of \\(D\\).\n\n\n\n\n\n\nExample 1\n\n\n\nTake two iterations of Gauss-Seidel iteration to approximate the solution of the following system using the initial guess \\(\\vec{x}^{(0)} = (1, 1)^T\\):\n\\[\n\\begin{pmatrix}\n2 & 1 \\\\ -1 & 4\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n3.5 \\\\ 0.5\n\\end{pmatrix}\n\\]\nStarting from \\(\\vec{x}^{(0)} = (1, 1)^T\\) we have\nIteration 1:\n\\[\n\\begin{aligned}\nx^{(1)}_1 & = x^{(0)}_1 + \\frac{1}{A_{11}} (b_1 - A_{11} x^{(0)}_1 - A_{12} x^{(0)}_2) \\\\\n          & = 2 + \\frac{1}{2} (3.5 - 1 \\times 2 - 1 \\times 1) = 2.25 \\\\\nx^{(1)}_2 & = x^{(0)}_2 + \\frac{1}{A_{22}} (b_2 - A_{21} x^{(1)}_1 - A_{22} x^{(0)}_2) \\\\\n          & = 1 + \\frac{1}{4} (0.5 - (-1) \\times 2.25 - 4 \\times 1) = 0.6875.\n\\end{aligned}\n\\]\nIteration 2:\n\\[\n\\begin{aligned}\nx^{(2)}_1 & = x^{(1)}_1 + \\frac{1}{A_{11}} (b_1 - A_{11} x^{(1)}_1 - A_{12} x^{(1)}_2) \\\\\n          & = 1.25 + \\frac{1}{2} (3.5 - 2 \\times 1.25 - 1 \\times 0.4375) = 1.53125 \\\\\nx^{(2)}_2 & = x^{(1)}_2 + \\frac{1}{A_{22}} (b_2 - A_{21} x^{(2)}_1 - A_{22} x^{(1)}_2) \\\\\n          & = 0.4375 + \\frac{1}{4} (0.5 - (-1) \\times 1.53125 - 4 \\times 0.4375) = 0.5078125.\n\\end{aligned}\n\\]\nAgain, note the changes in the iteration number on the right hand side of these equations, especially the differences against the Jacobi method.\n\nWhat happens if the initial estimate is altered to \\(\\vec{x}^{(0)} = (2, 1)^T\\) (homework).\n\n\n\n\n\n\n\n\n\nExample 2 (homework)\n\n\n\nTake one iteration of (i) Jacobi iteration; (ii) Gauss-Seidel iteration to approximate the solution of the following system using the initial guess \\(\\vec{x}^{(0)} = (1, 2, 3)^T\\):\n\\[\n\\begin{pmatrix}\n2 & 1 & 0 \\\\\n1 & 3 & 1 \\\\\n0 & 1 & 2\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n6 \\\\ 10 \\\\ 6\n\\end{pmatrix}.\n\\]\nNote that the exact solution to this system is \\(x_1 = 2, x_2 = 2, x_3 = 2\\).\n\n\n\nRemark. \n\nHere both methods converge, but fairly slowly. They might not converge at all!\nWe will discuss convergence and stopping criteria in the next lecture.\nThe Gauss-Seidel iteration generally out-performs the Jacobi iteration.\nPerformance can depend on the order in which the equations are written.\nBoth iterative algorithms can be made faster and more efficient for sparse systems of equations (far more than direct methods).\n\n\n\n\n\n\ndef jacobi_iteration(A, b, x0, max_iter, verbose=False):\n    \"\"\"\n    TODO\n    \"\"\"\n    n = system_size(A, b)\n\n    x = x0.copy()\n    xnew = np.empty_like(x)\n\n    if verbose:\n        print(\"starting value: \", end=\"\")\n        print_array(x.T, \"x.T\")\n\n    for iter in range(max_iter):\n        for i in range(n):\n            Axi = 0.0\n            for j in range(n):\n                Axi += A[i, j] * x[j]\n            xnew[i] = x[i] + 1.0 / (A[i, i]) * (b[i] - Axi)\n        x = xnew.copy()\n\n        if verbose:\n            print(f\"after {iter=}: \", end=\"\")\n            print_array(x.T, \"x.T\")\n\n    return x\n\n\ndef gauss_seidel_iteration(A, b, x0, max_iter, verbose=False):\n    \"\"\"\n    TODO\n    \"\"\"\n    n = system_size(A, b)\n\n    x = x0.copy()\n    xnew = np.empty_like(x)\n\n    if verbose:\n        print(\"starting value: \", end=\"\")\n        print_array(x.T, \"x.T\")\n\n    for iter in range(max_iter):\n        for i in range(n):\n            Axi = 0.0\n            for j in range(i):\n                Axi += A[i, j] * xnew[j]\n            for j in range(i, n):\n                Axi += A[i, j] * x[j]\n            xnew[i] = x[i] + 1.0 / (A[i, i]) * (b[i] - Axi)\n        x = xnew.copy()\n\n        if verbose:\n            print(f\"after {iter=}: \", end=\"\")\n            print_array(x.T, \"x.T\")\n\n    return x\n\n\nA = np.array([[2.0, 1.0], [-1.0, 4.0]])\nb = np.array([[3.5], [0.5]])\nx0 = np.array([[1.0], [1.0]])\n\nprint(\"jacobi iteration\")\nx = jacobi_iteration(A, b, x0, 5, verbose=True)\nprint()\n\nprint(\"gauss seidel iteration\")\nx = gauss_seidel_iteration(A, b, x0, 5, verbose=True)\nprint()\n\njacobi iteration\nstarting value: x.T = [  1.0,  1.0 ]\nafter iter=0: x.T = [  1.250,  0.375 ]\nafter iter=1: x.T = [  1.5625,  0.4375 ]\nafter iter=2: x.T = [  1.53125,  0.51562 ]\nafter iter=3: x.T = [  1.49219,  0.50781 ]\nafter iter=4: x.T = [  1.49609,  0.49805 ]\n\ngauss seidel iteration\nstarting value: x.T = [  1.0,  1.0 ]\nafter iter=0: x.T = [  1.2500,  0.4375 ]\nafter iter=1: x.T = [  1.53125,  0.50781 ]\nafter iter=2: x.T = [  1.49609,  0.49902 ]\nafter iter=3: x.T = [  1.50049,  0.50012 ]\nafter iter=4: x.T = [  1.49994,  0.49998 ]\n\n\n\n\n\n\nWe met sparse matrices as an example of a special matrix format when we first thought about systems of linear equations. Sparse matrices are very common in applications and have a structure which is very useful when used with iterative methods. There are two main ways in which sparse matrices can be exploited in order to obtain benefits within iterative methods.\n\nThe storage can be reduced from \\(O(n^2)\\).\nThe cost per iteration can be reduced from \\(O(n^2)\\).\n\nRecall that a sparse matrix is defined to be such that it has at most \\(\\alpha n\\) non-zero entries (where \\(\\alpha\\) is independent of \\(n\\)). Typically this happens when we know there are at most \\(\\alpha\\) non-zero entries in any row.\nThe simplest way in which a sparse matrix is stored is using three arrays:\n\nan array of floating point numbers (A_real say) that stores the non-zero entries;\nan array of integers (I_row say) that stores the row number of the corresponding entry in the real array;\nan array of integers (I_col say) that stores the column numbers of the corresponding entry in the real array.\n\nThis requires just \\(3 \\alpha n\\) units of storage - i.e. \\(O(n)\\).\nGiven the above storage pattern, the following algorithm will execute a sparse matrix-vector multiplication (\\(\\vec{z} = A \\vec{y}\\)) in \\(O(n)\\) operations:\nz = np.zeros((n, 1))\nfor k in range(nonzero):\n    z[I_row[k]] = z[I_row[k]] + A_real[k] * y[I_col[k]]\n\nHere nonzero is the number of non-zero entries in the matrix.\nNote that the cost of this operation is \\(O(n)\\) as required.\n\n\n\n\ndef system_size_sparse(A_real, I_row, I_col, b):\n    n = len(b)\n    nonzero = len(A_real)\n    assert nonzero == len(I_row)\n    assert nonzero == len(I_col)\n\n    return n, nonzero\n\nFirst let’s adapt our implementations to use this sparse matrix format:\n\ndef jacobi_iteration_sparse(A_real, I_row, I_col, b, x0, max_iter, verbose=False):\n    \"\"\"\n    TODO\n    \"\"\"\n    n, nonzero = system_size_sparse(A_real, I_row, I_col, b)\n\n    x = x0.copy()\n    xnew = np.empty_like(x)\n\n    if verbose:\n        print(\"starting value: \", end=\"\")\n        print_array(x.T, \"x.T\")\n\n    # determine diagonal\n    # D[i] should be A_{ii}\n    D = np.zeros_like(x)\n    for k in range(nonzero):\n        if I_row[k] == I_col[k]:\n            D[I_row[k]] = A_real[k]\n\n    for iter in range(max_iter):\n        # precompute Ax\n        Ax = np.zeros_like(x)\n        for k in range(nonzero):\n            Ax[I_row[k]] = Ax[I_row[k]] + A_real[k] * x[I_col[k]]\n\n        for i in range(n):\n            xnew[i] = x[i] + 1.0 / D[i] * (b[i] - Ax[i])\n        x = xnew.copy()\n\n        if verbose:\n            print(f\"after {iter=}: \", end=\"\")\n            print_array(x.T, \"x.T\")\n\n    return x\n\n\ndef gauss_seidel_iteration_sparse(A_real, I_row, I_col, b, x0, max_iter, verbose=False):\n    \"\"\"\n    TODO\n    \"\"\"\n    n, nonzero = system_size_sparse(A_real, I_row, I_col, b)\n\n    x = x0.copy()\n    xnew = np.empty_like(x)\n\n    if verbose:\n        print(\"starting value: \", end=\"\")\n        print_array(x.T, \"x.T\")\n\n    for iter in range(max_iter):\n        # precompute Ax using xnew if i &lt; j\n        Ax = np.zeros_like(x)\n        for k in range(nonzero):\n            if I_row[k] &lt; I_col[k]:\n                Ax[I_row[k]] = Ax[I_row[k]] + A_real[k] * xnew[I_col[k]]\n            else:\n                Ax[I_row[k]] = Ax[I_row[k]] + A_real[k] * x[I_col[k]]\n\n        for i in range(n):\n            xnew[i] = x[i] + 1.0 / (A[i, i]) * (b[i] - Ax[i])\n        x = xnew.copy()\n\n        if verbose:\n            print(f\"after {iter=}: \", end=\"\")\n            print_array(x.T, \"x.T\")\n\n    return x\n\nThen we can test the two different implementations of the methods:\n\n# random matrix\nn = 4\nnonzero = 10\nA_real, I_row, I_col, b = random_sparse_system(n, nonzero)\nprint(\"sparse matrix:\")\nprint(\"A_real =\", A_real)\nprint(\"I_row = \", I_row)\nprint(\"I_col = \", I_col)\nprint()\n\n# convert to dense for comparison\nA_dense = to_dense(A_real, I_row, I_col)\nprint(\"dense matrix:\")\nprint_array(A_dense)\nprint()\n\n# starting guess\nx0 = np.zeros((n, 1))\n\nprint(\"jacobi with sparse matrix\")\nx_sparse = jacobi_iteration_sparse(A_real, I_row, I_col, b, x0, max_iter=5, verbose=True)\nprint() \n\nprint(\"jacobi with dense matrix\")\nx_dense = jacobi_iteration(A_dense, b, x0, max_iter=5, verbose=True)\nprint()\n\nsparse matrix:\nA_real = [11.25 -3.   21.25  6.25 11.25  3.   31.5  -3.    4.    3.  ]\nI_row =  [0 0 0 1 1 2 2 2 3 3]\nI_col =  [1 2 0 1 0 3 2 0 3 2]\n\ndense matrix:\nA_dense = [ 21.25, 11.25, -3.00,  0.00 ]\n          [ 11.25,  6.25,  0.00,  0.00 ]\n          [ -3.00,  0.00, 31.50,  3.00 ]\n          [  0.00,  0.00,  3.00,  4.00 ]\n\njacobi with sparse matrix\nstarting value: x.T = [  0.0,  0.0,  0.0,  0.0 ]\nafter iter=0: x.T = [  1.38824,  2.80000,  1.00000,  1.75000 ]\nafter iter=1: x.T = [  0.047059,  0.301176,  0.965546,  1.000000 ]\nafter iter=2: x.T = [  1.36510,  2.71529,  0.90924,  1.02584 ]\nafter iter=3: x.T = [  0.07909,  0.34282,  1.03231,  1.06807 ]\nafter iter=4: x.T = [  1.35248,  2.65764,  0.90581,  0.97577 ]\n\njacobi with dense matrix\nstarting value: x.T = [  0.0,  0.0,  0.0,  0.0 ]\nafter iter=0: x.T = [  1.38824,  2.80000,  1.00000,  1.75000 ]\nafter iter=1: x.T = [  0.047059,  0.301176,  0.965546,  1.000000 ]\nafter iter=2: x.T = [  1.36510,  2.71529,  0.90924,  1.02584 ]\nafter iter=3: x.T = [  0.07909,  0.34282,  1.03231,  1.06807 ]\nafter iter=4: x.T = [  1.35248,  2.65764,  0.90581,  0.97577 ]\n\n\n\nWe see that we get the same results!\nNow let’s see how long it takes to get a solution. The following plot shows the run times of using the two different implementations of the Jacobi method. We see that, as expected, the run time of the dense formulation is \\(O(n^2)\\) and the run time of the sparse formulation is \\(O(n)\\).\n\n\n\n\n\n\n\n\n\nWe say “as expected” because we have already counted the number of operations per iteration and these implementations compute for a fixed number of iterations. In the next section, we look at alternative stopping criteria.\n\n\n\n\nWe have discussed the construction of iterations which aim to find the solution of the equations \\(A \\vec{x} = \\vec{b}\\) through a sequence of better and better approximations \\(\\vec{x}^{(k)}\\).\nIn general the iteration takes the form \\[\n\\vec{x}^{(k+1)} = \\vec{F}(\\vec{x}^{(k)})\n\\] here \\(\\vec{x}^{(k)}\\) is a vector of values and \\(\\vec{F}\\) is some vector-valued function which we have defined.\nHow can we decide if this iteration has converged? We need \\(\\vec{x} - \\vec{x}^{(k)}\\) to be small, but we don’t have access to the exact solution \\(\\vec{x}\\) so we have to do something else!\nHow do we decide that a vector/array is small? The most common measure is to use the “Euclidean norm” of an array (which you met last year!). This is defined to be the square root of the sum of squares of the entries of the array: \\[\n\\| \\vec{r} \\| = \\sqrt{ \\sum_{i=1}^n r_i^2 }.\n\\] where \\(\\vec{r}\\) is a vector with \\(n\\) entries.\n\n\n\n\n\n\nExamples\n\n\n\nConsider the following sequence \\(\\vec{x}^{(k)}\\):\n\\[\n\\begin{pmatrix}\n1 \\\\ -1\n\\end{pmatrix},\n\\begin{pmatrix}\n1.5 \\\\ 0.5\n\\end{pmatrix},\n\\begin{pmatrix}\n1.75 \\\\ 0.25\n\\end{pmatrix},\n\\begin{pmatrix}\n1.875 \\\\ 0.125\n\\end{pmatrix},\n\\begin{pmatrix}\n1.9375 \\\\ -0.0625\n\\end{pmatrix},\n\\begin{pmatrix}\n1.96875 \\\\ -0.03125\n\\end{pmatrix},\n\\ldots\n\\]\n\nWhat is \\(\\|\\vec{x}^{(1)} - \\vec{x}^{(0)}\\|\\)?\nWhat is \\(\\|\\vec{x}^{(5)} - \\vec{x}^{(4)}\\|\\)?\n\nLet \\(\\vec{x} = \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix}\\).\n\nWhat is \\(\\|\\vec{x} - \\vec{x}^{(3)}\\|\\)?\nWhat is \\(\\|\\vec{x} - \\vec{x}^{(4)}\\|\\)?\nWhat is \\(\\|\\vec{x} - \\vec{x}^{(5)}\\|\\)?\n\n\n\nRather than decide in advance how many iterations (of the Jacobi or Gauss-Seidel methods) to use stopping criteria:\n\nThis could be a maximum number of iterations.\nThis could be the change in values is small enough:\n\\[\n\\|x^{(k+1)} - \\vec{x}^{(k)}\\| &lt; tol,\n\\]\nThis could be the norm of the residual is small enough:\n\\[\n\\| \\vec{r} \\| = \\| \\vec{b} - A \\vec{x}^{(k)} \\| &lt; tol\n\\]\n\nIn both cases, we call \\(tol\\) the convergence tolerance and the choice of \\(tol\\) will control the accuracy of the solution.\n\n\n\n\n\n\nDiscussion\n\n\n\nWhat is a good convergence tolerance?\n\n\nIn general there are two possible reasons that an iteration may fail to converge.\n\nIt may diverge - this means that \\(\\|\\vec{x}^{(k)}\\| \\to \\infty\\) as \\(k\\) (the number of iterations) increases, e.g.:\n\\[\n\\begin{pmatrix}\n1 \\\\ 1\n\\end{pmatrix},\n\\begin{pmatrix}\n4 \\\\ 2\n\\end{pmatrix},\n\\begin{pmatrix}\n16 \\\\ 4\n\\end{pmatrix},\n\\begin{pmatrix}\n64 \\\\ 8\n\\end{pmatrix},\n\\begin{pmatrix}\n256 \\\\ 16\n\\end{pmatrix},\n\\begin{pmatrix}\n1024 \\\\ 32\n\\end{pmatrix},\n\\ldots\n\\]\nIt may neither converge nor diverge, e.g.:\n\\[\n\\begin{pmatrix}\n1 \\\\ 1\n\\end{pmatrix},\n\\begin{pmatrix}\n2 \\\\ 0\n\\end{pmatrix},\n\\begin{pmatrix}\n3 \\\\ 1\n\\end{pmatrix},\n\\begin{pmatrix}\n1 \\\\ 0\n\\end{pmatrix},\n\\begin{pmatrix}\n2 \\\\ 1\n\\end{pmatrix},\n\\begin{pmatrix}\n3 \\\\ 0\n\\end{pmatrix},\n\\ldots\n\\]\n\nIn addition to testing for convergence it is also necessary to include tests for failure to converge.\n\nDivergence may be detected by monitoring \\(\\|\\vec{x}^{(k)}\\|\\).\nImpose a maximum number of iterations to ensure that the loop is not repeated forever!\n\n\n\n\nMany complex computational problems simply cannot be solved with today’s computers using direct methods. Iterative methods are used instead since they can massively reduce the computational cost and storage required to get a “good enough” solution.\nThese basic iterative methods are simple to describe and program but generally slow to converge to an accurate answer - typically \\(O(n)\\) iterations are required! Their usefulness for general matrix systems is very limited therefore - but we have shown their value in the solution of sparse systems however.\nMore advanced iterative methods do exist but are beyond the scope of this module - see Final year projects, MSc projects, PhD, and beyond!\n\n\n\nMore details on these basic (and related) methods:\n\nWikipedia: Jacobi method\nWikipedia: Gauss-Seidel method\nWikipedia: Iterative methods\n\nsee also Richardson method, Damped Jacobi method, Successive over-relaxation method (SOR), Symmetric successive over-relaxation method (SSOR) and Krylov subspace methods\n\n\nMore details on sparse matrices:\n\nWikipedia Sparse matrix - including a long detailed list of software libraries support sparse matrices.\nStackoverflow: Using a sparse matrix vs numpy array\nJason Brownlee: A gentle introduction to sparse matrices for machine learning, Machine learning mastery\n\nSome related textbooks:\n\nJack Dongarra Templates for the solution of linear systems: Stopping criteria\nJack Dongarra Templates for the solution of linear systems: Stationary iterative methods\nGolub, Gene H.; Van Loan, Charles F. (1996), Matrix Computations (3rd ed.), Baltimore: Johns Hopkins, ISBN 978-0-8018-5414-9.\nSaad, Yousef (2003). Iterative Methods for Sparse Linear Systems (2nd ed.). SIAM. p. 414. ISBN 0898715342.\n\nSome software implementations:\n\nscipy.sparse custom routines specialised to sparse matrices\nSuiteSparse, a suite of sparse matrix algorithms, geared toward the direct solution of sparse linear systems\nscipy.sparse iterative solvers: Solving linear problems\nPETSc: Linear system solvers - a high performance linear algebra toolkit",
    "crumbs": [
      "Iterative solutions of linear equations"
    ]
  },
  {
    "objectID": "src/lec05.html#jacobi-iteration",
    "href": "src/lec05.html#jacobi-iteration",
    "title": "1 Iterative solutions of linear equations",
    "section": "",
    "text": "One simple choice for \\(P\\) is given by the Jacobi method where we take \\(P = D^{-1}\\) where \\(D\\) is the diagonal of \\(A\\): \\[\nD_{ii} = A_{ii} \\quad \\text{and} \\quad D_{ij} = 0 \\text{ for } i \\neq j.\n\\]\nThe Jacobi iteration is given by\n\\[\n\\vec{x}^{(k+1)} = \\vec{x}^{(k)} + D^{-1}(\\vec{b} - A \\vec{x}^{(k)})\n\\]\n\\(D\\) is a diagonal matrix, so \\(D^{-1}\\) is trivial to form (as long as the diagonal entries are all nonzero): \\[\n(D^{-1})_{ii} = \\frac{1}{D_{ii}}\n\\quad \\text{and} \\quad\n(D^{-1})_{ij} = 0 \\text{ for } i \\neq j.\n\\]\n\nRemark. \n\nThe cost of one iteration is \\(O(n^2)\\) for a full matrix, and this is dominated by the matrix-vector product \\(A \\vec{x}^{(k)}\\).\nThis cost can be reduced to \\(O(n)\\) if the matrix \\(A\\) is sparse - this is when iterative methods are especially attractive (TODO add future ref).\nThe amount of work also depends on the number of iterations required to get a “satisfactory” solution.\n\nThe number of iterations depends on the matrix;\nFewer iterations are needed for a less accurate solution;\nA good initial estimate \\(\\vec{x}^{(0)}\\) reduces the required number of iterations.\n\nUnfortunately, the iteration might not converge!\n\n\nThe Jacobi iteration updates all elements of \\(\\vec{x}^{(k)}\\) simultaneously to get \\(\\vec{x}^{(k+1)}\\). Writing the method out component by component gives\n\\[\n\\begin{aligned}\nx_1^{(k+1)} &= x_1^{(k)} + \\frac{1}{A_{11}} \\left( b_1 - \\sum_{j=1}^n A_{1j} x_j^{(k)} \\right) \\\\\nx_2^{(k+1)} &= x_2^{(k)} + \\frac{1}{A_{22}} \\left( b_2 - \\sum_{j=1}^n A_{2j} x_j^{(k)} \\right) \\\\\n\\vdots \\quad & \\hphantom{=} \\quad \\vdots \\\\\nx_n^{(k+1)} &= x_n^{(k)} + \\frac{1}{A_{nn}} \\left( b_n - \\sum_{j=1}^n A_{nj} x_j^{(k)} \\right).\n\\end{aligned}\n\\]\nNote that once the first step has been taken, \\(x_1^{(k+1)}\\) is already known, but the Jacobi iteration does not make use of this information!\n\n\n\n\n\n\nExample 1\n\n\n\nTake two iterations of Jacobi iteration to approximate the solution of the following system using the initial guess \\(\\vec{x}^{(0)} = (1, 1)^T\\): \\[\n\\begin{pmatrix}\n2 & 1 \\\\ -1 & 4\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n3.5 \\\\ 0.5\n\\end{pmatrix}\n\\]\nStarting from \\(\\vec{x}^{(0)} = (1, 1)^T\\), the first iteration is \\[\n\\begin{aligned}\nx_1^{(1)} &= x_1^{(0)} + \\frac{1}{A_{11}} \\left( b_1 - A_{11} x_1^{(0)} - A_{12} x_2^{(0)} \\right) \\\\\n&= 1 + \\frac{1}{2} (3.5 - 2 \\times 1 - 1 \\times 1) = 1.25 \\\\\nx_2^{(1)} &= x_2^{(0)} + \\frac{1}{A_{22}} \\left( b_2 - A_{21} x_1^{(0)} - A_{22} x_2^{(0)} \\right) \\\\\n&= 1 + \\frac{1}{4} (0.5 - (-1) \\times 1 - 4 \\times 1) = 0.375. \\\\\n\\end{aligned}\n\\] So we have \\(\\vec{x}^{(1)} = (1.25, 0.375)^T\\). Then the second iteration is \\[\n\\begin{aligned}\nx_1^{(2)} &= x_1^{(1)} + \\frac{1}{A_{11}} \\left( b_1 - A_{11} x_1^{(1)} - A_{12} x_2^{(1)} \\right) \\\\\n&= 1.25 + \\frac{1}{2} (3.5 - 2 \\times 1.25 - 1 \\times 0.375) = 1.5625 \\\\\nx_2^{(2)} &= x_2^{(1)} + \\frac{1}{A_{22}} \\left( b_2 - A_{21} x_1^{(1)} - A_{22} x_2^{(1)} \\right) \\\\\n&= 0.375 + \\frac{1}{4} (0.5 - (-1) \\times 1.25 - 4 \\times 0.375) = 0.4375. \\\\\n\\end{aligned}\n\\] So we have \\(\\vec{x}^{(2)} = (1.5625, 0.4375)\\).\nNote the only difference between the formulae for Iteration 1 and 2 is the iteration number, the superscript in brackets. The exact solution is given by \\(\\vec{x} = (1.5, 0.5)^T\\).\n\n\nWe note that we can also slightly simplify the way the Jacobi iteration is written. We can expand \\(A\\) into \\(A = L + D + U\\), where \\(L\\) and \\(U\\) are the parts of the matrix from below and above the diagonal respectively: \\[\nL_{ij} = \\begin{cases}\nA_{ij} &\\quad \\text{if } i &lt; j \\\\\n0 &\\quad \\text{if } i \\ge j,\n\\end{cases}\n\\qquad\nU_{ij} = \\begin{cases}\nA_{ij} &\\quad \\text{if } i &gt; j \\\\\n0 &\\quad \\text{if } i \\le j.\n\\end{cases}\n\\] The we can calculate that: \\[\n\\begin{aligned}\n\\vec{x}^{(k+1)} & = \\vec{x}^{(k)} + D^{-1}(\\vec{b} - A \\vec{x}^{(k)}) \\\\\n& = \\vec{x}^{(k)} + D^{-1}(\\vec{b} - (L + D + U) \\vec{x}^{(k)}) \\\\\n& = \\vec{x}^{(k)} - D^{-1} D \\vec{x}^{(k)} + D^{-1}(\\vec{b} - (L + U) \\vec{x}^{(k)}) \\\\\n& = \\vec{x}^{(k)} - \\vec{x}^{(k)} + D^{-1}(\\vec{b} - (L + U) \\vec{x}^{(k)}) \\\\\n& = D^{-1}(\\vec{b} - (L + U) \\vec{x}^{(k)}).\n\\end{aligned}\n\\] In this formulation, we do not explicitly form the residual as part of the computations. In practical situations, this may be a simpler formulation we can use if we have knowledge of the coefficients of \\(A\\), but this is not always true!",
    "crumbs": [
      "Iterative solutions of linear equations"
    ]
  },
  {
    "objectID": "src/lec05.html#gauss-seidel-iteration",
    "href": "src/lec05.html#gauss-seidel-iteration",
    "title": "1 Iterative solutions of linear equations",
    "section": "",
    "text": "As an alternative to Jacobi iteration, the iteration might use \\(x_i^{(k+1)}\\) as soon as it is calculated (rather than using the previous iteration), giving\n\\[\n\\begin{aligned}\nx_1^{(k+1)}\n& = x_1^{(k)} + \\frac{1}{A_{11}} \\left(\nb_1 - \\sum_{j=1}^n A_{1j} x_j^{(k)}\n\\right) \\\\\nx_2^{(k+1)}\n& = x_2^{(k)} + \\frac{1}{A_{22}} \\left(\nb_2 - A_{21} x_1^{(k+1)} - \\sum_{j=2}^n A_{2j} x_j^{(k)}\n\\right) \\\\\nx_3^{(k+1)}\n& = x_3^{(k)} + \\frac{1}{A_{33}} \\left(\nb_3 - \\sum_{j=1}^2 A_{3j} x_j^{(k+1)} - \\sum_{j=3}^n A_{3j} x_j^{(k)}\n\\right) \\\\\n\\vdots \\quad & \\hphantom{=} \\quad \\vdots \\\\\nx_i^{(k+1)}\n& = x_i^{(k)} + \\frac{1}{A_{ii}} \\left(\nb_i - \\sum_{j=1}^{i-1} A_{ij} x_j^{(k+1)} - \\sum_{j=i}^n A_{ij} x_j^{(k)}\n\\right) \\\\\n\\vdots \\quad & \\hphantom{=} \\quad \\vdots \\\\\nx_n^{(k+1)}\n& = x_n^{(k)} + \\frac{1}{A_{nn}} \\left(\nb_n - \\sum_{j=1}^{n-1} A_{nj} x_j^{(k+1)} - A_{nn} x_n^{(k)}\n\\right).\n\\end{aligned}\n\\]\nConsider the system \\(A \\vec{x}= b\\) with the matrix \\(A\\) split as \\(A = L + D + U\\) where \\(D\\) is the diagonal of \\(A\\), \\(L\\) contains the elements below the diagonal and \\(U\\) contains the elements above the diagonal. The componentwise iteration above can be written in matrix form as \\[\n\\begin{aligned}\n\\vec{x}^{(k+1)} & = \\vec{x}^{(k)} + D^{-1} (\\vec{b} - L \\vec{x}^{(k+1)} - (D + U) \\vec{x}^{(k)}) \\\\\n& = \\vec{x}^{(k)} - D^{-1} L \\vec{x}^{(k+1)} + D^{-1} (\\vec{b} - (D + U) \\vec{x}^{(k)}) \\\\\n& = \\vec{x}^{(k)} - D^{-1} L \\vec{x}^{(k+1)} + D^{-1} L \\vec{x}^{(k)} + D^{-1} (\\vec{b} - (L + D + U) \\vec{x}^{(k)}) \\\\\n\\vec{x}^{(k+1)} + D^{-1} L \\vec{x}^{(k+1)} & = \\vec{x}^{(k)} + D^{-1} L \\vec{x}^{(k)} + D^{-1} (\\vec{b} - (L + D + U) \\vec{x}^{(k)}) \\\\\nD^{-1} (D + L) \\vec{x}^{(k+1)}  & = D^{-1} (D + L) \\vec{x}^{(k)} + D^{-1} (\\vec{b} - A \\vec{x}^{(k)}) \\\\\n(D + L) \\vec{x}^{(k+1)} &= D D^{-1} (D + L) \\vec{x}^{(k)} + D D^{-1} (\\vec{b} - A \\vec{x}^{(k)}) \\\\\n& = (D + L) \\vec{x}^{(k)} + (\\vec{b} - A \\vec{x}^{(k)}) \\\\\n\\vec{x}^{(k+1)} &= (D + L)^{-1} (D + L) \\vec{x}^{(k)} + (D + L)^{-1} (\\vec{b} - A \\vec{x}^{(k)}) \\\\\n& = \\vec{x}^{(k)} + (D + L)^{-1} (\\vec{b} - A \\vec{x}^{(k)}).\n\\end{aligned}\n\\]\n…and hence the Gauss-Seidel iteration\n\\[\n\\vec{x}^{(k+1)} = \\vec{x}^{(k)} + (D + L)^{-1} (\\vec{b} - A \\vec{x}^{(k)}).\n\\] That is, we use \\(P = (D+L)^{-1}\\) in \\(\\eqref{eq:general-iteration}\\).\nIn general, we don’t form the inverse of \\(D + L\\) explicitly here since it is more complicated to do so than for simply computing the inverse of \\(D\\).\n\n\n\n\n\n\nExample 1\n\n\n\nTake two iterations of Gauss-Seidel iteration to approximate the solution of the following system using the initial guess \\(\\vec{x}^{(0)} = (1, 1)^T\\):\n\\[\n\\begin{pmatrix}\n2 & 1 \\\\ -1 & 4\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n3.5 \\\\ 0.5\n\\end{pmatrix}\n\\]\nStarting from \\(\\vec{x}^{(0)} = (1, 1)^T\\) we have\nIteration 1:\n\\[\n\\begin{aligned}\nx^{(1)}_1 & = x^{(0)}_1 + \\frac{1}{A_{11}} (b_1 - A_{11} x^{(0)}_1 - A_{12} x^{(0)}_2) \\\\\n          & = 2 + \\frac{1}{2} (3.5 - 1 \\times 2 - 1 \\times 1) = 2.25 \\\\\nx^{(1)}_2 & = x^{(0)}_2 + \\frac{1}{A_{22}} (b_2 - A_{21} x^{(1)}_1 - A_{22} x^{(0)}_2) \\\\\n          & = 1 + \\frac{1}{4} (0.5 - (-1) \\times 2.25 - 4 \\times 1) = 0.6875.\n\\end{aligned}\n\\]\nIteration 2:\n\\[\n\\begin{aligned}\nx^{(2)}_1 & = x^{(1)}_1 + \\frac{1}{A_{11}} (b_1 - A_{11} x^{(1)}_1 - A_{12} x^{(1)}_2) \\\\\n          & = 1.25 + \\frac{1}{2} (3.5 - 2 \\times 1.25 - 1 \\times 0.4375) = 1.53125 \\\\\nx^{(2)}_2 & = x^{(1)}_2 + \\frac{1}{A_{22}} (b_2 - A_{21} x^{(2)}_1 - A_{22} x^{(1)}_2) \\\\\n          & = 0.4375 + \\frac{1}{4} (0.5 - (-1) \\times 1.53125 - 4 \\times 0.4375) = 0.5078125.\n\\end{aligned}\n\\]\nAgain, note the changes in the iteration number on the right hand side of these equations, especially the differences against the Jacobi method.\n\nWhat happens if the initial estimate is altered to \\(\\vec{x}^{(0)} = (2, 1)^T\\) (homework).\n\n\n\n\n\n\n\n\n\nExample 2 (homework)\n\n\n\nTake one iteration of (i) Jacobi iteration; (ii) Gauss-Seidel iteration to approximate the solution of the following system using the initial guess \\(\\vec{x}^{(0)} = (1, 2, 3)^T\\):\n\\[\n\\begin{pmatrix}\n2 & 1 & 0 \\\\\n1 & 3 & 1 \\\\\n0 & 1 & 2\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n6 \\\\ 10 \\\\ 6\n\\end{pmatrix}.\n\\]\nNote that the exact solution to this system is \\(x_1 = 2, x_2 = 2, x_3 = 2\\).\n\n\n\nRemark. \n\nHere both methods converge, but fairly slowly. They might not converge at all!\nWe will discuss convergence and stopping criteria in the next lecture.\nThe Gauss-Seidel iteration generally out-performs the Jacobi iteration.\nPerformance can depend on the order in which the equations are written.\nBoth iterative algorithms can be made faster and more efficient for sparse systems of equations (far more than direct methods).",
    "crumbs": [
      "Iterative solutions of linear equations"
    ]
  },
  {
    "objectID": "src/lec05.html#python-version-of-iterative-methods",
    "href": "src/lec05.html#python-version-of-iterative-methods",
    "title": "1 Iterative solutions of linear equations",
    "section": "",
    "text": "def jacobi_iteration(A, b, x0, max_iter, verbose=False):\n    \"\"\"\n    TODO\n    \"\"\"\n    n = system_size(A, b)\n\n    x = x0.copy()\n    xnew = np.empty_like(x)\n\n    if verbose:\n        print(\"starting value: \", end=\"\")\n        print_array(x.T, \"x.T\")\n\n    for iter in range(max_iter):\n        for i in range(n):\n            Axi = 0.0\n            for j in range(n):\n                Axi += A[i, j] * x[j]\n            xnew[i] = x[i] + 1.0 / (A[i, i]) * (b[i] - Axi)\n        x = xnew.copy()\n\n        if verbose:\n            print(f\"after {iter=}: \", end=\"\")\n            print_array(x.T, \"x.T\")\n\n    return x\n\n\ndef gauss_seidel_iteration(A, b, x0, max_iter, verbose=False):\n    \"\"\"\n    TODO\n    \"\"\"\n    n = system_size(A, b)\n\n    x = x0.copy()\n    xnew = np.empty_like(x)\n\n    if verbose:\n        print(\"starting value: \", end=\"\")\n        print_array(x.T, \"x.T\")\n\n    for iter in range(max_iter):\n        for i in range(n):\n            Axi = 0.0\n            for j in range(i):\n                Axi += A[i, j] * xnew[j]\n            for j in range(i, n):\n                Axi += A[i, j] * x[j]\n            xnew[i] = x[i] + 1.0 / (A[i, i]) * (b[i] - Axi)\n        x = xnew.copy()\n\n        if verbose:\n            print(f\"after {iter=}: \", end=\"\")\n            print_array(x.T, \"x.T\")\n\n    return x\n\n\nA = np.array([[2.0, 1.0], [-1.0, 4.0]])\nb = np.array([[3.5], [0.5]])\nx0 = np.array([[1.0], [1.0]])\n\nprint(\"jacobi iteration\")\nx = jacobi_iteration(A, b, x0, 5, verbose=True)\nprint()\n\nprint(\"gauss seidel iteration\")\nx = gauss_seidel_iteration(A, b, x0, 5, verbose=True)\nprint()\n\njacobi iteration\nstarting value: x.T = [  1.0,  1.0 ]\nafter iter=0: x.T = [  1.250,  0.375 ]\nafter iter=1: x.T = [  1.5625,  0.4375 ]\nafter iter=2: x.T = [  1.53125,  0.51562 ]\nafter iter=3: x.T = [  1.49219,  0.50781 ]\nafter iter=4: x.T = [  1.49609,  0.49805 ]\n\ngauss seidel iteration\nstarting value: x.T = [  1.0,  1.0 ]\nafter iter=0: x.T = [  1.2500,  0.4375 ]\nafter iter=1: x.T = [  1.53125,  0.50781 ]\nafter iter=2: x.T = [  1.49609,  0.49902 ]\nafter iter=3: x.T = [  1.50049,  0.50012 ]\nafter iter=4: x.T = [  1.49994,  0.49998 ]",
    "crumbs": [
      "Iterative solutions of linear equations"
    ]
  },
  {
    "objectID": "src/lec05.html#sparse-matrices",
    "href": "src/lec05.html#sparse-matrices",
    "title": "1 Iterative solutions of linear equations",
    "section": "",
    "text": "We met sparse matrices as an example of a special matrix format when we first thought about systems of linear equations. Sparse matrices are very common in applications and have a structure which is very useful when used with iterative methods. There are two main ways in which sparse matrices can be exploited in order to obtain benefits within iterative methods.\n\nThe storage can be reduced from \\(O(n^2)\\).\nThe cost per iteration can be reduced from \\(O(n^2)\\).\n\nRecall that a sparse matrix is defined to be such that it has at most \\(\\alpha n\\) non-zero entries (where \\(\\alpha\\) is independent of \\(n\\)). Typically this happens when we know there are at most \\(\\alpha\\) non-zero entries in any row.\nThe simplest way in which a sparse matrix is stored is using three arrays:\n\nan array of floating point numbers (A_real say) that stores the non-zero entries;\nan array of integers (I_row say) that stores the row number of the corresponding entry in the real array;\nan array of integers (I_col say) that stores the column numbers of the corresponding entry in the real array.\n\nThis requires just \\(3 \\alpha n\\) units of storage - i.e. \\(O(n)\\).\nGiven the above storage pattern, the following algorithm will execute a sparse matrix-vector multiplication (\\(\\vec{z} = A \\vec{y}\\)) in \\(O(n)\\) operations:\nz = np.zeros((n, 1))\nfor k in range(nonzero):\n    z[I_row[k]] = z[I_row[k]] + A_real[k] * y[I_col[k]]\n\nHere nonzero is the number of non-zero entries in the matrix.\nNote that the cost of this operation is \\(O(n)\\) as required.\n\n\n\n\ndef system_size_sparse(A_real, I_row, I_col, b):\n    n = len(b)\n    nonzero = len(A_real)\n    assert nonzero == len(I_row)\n    assert nonzero == len(I_col)\n\n    return n, nonzero\n\nFirst let’s adapt our implementations to use this sparse matrix format:\n\ndef jacobi_iteration_sparse(A_real, I_row, I_col, b, x0, max_iter, verbose=False):\n    \"\"\"\n    TODO\n    \"\"\"\n    n, nonzero = system_size_sparse(A_real, I_row, I_col, b)\n\n    x = x0.copy()\n    xnew = np.empty_like(x)\n\n    if verbose:\n        print(\"starting value: \", end=\"\")\n        print_array(x.T, \"x.T\")\n\n    # determine diagonal\n    # D[i] should be A_{ii}\n    D = np.zeros_like(x)\n    for k in range(nonzero):\n        if I_row[k] == I_col[k]:\n            D[I_row[k]] = A_real[k]\n\n    for iter in range(max_iter):\n        # precompute Ax\n        Ax = np.zeros_like(x)\n        for k in range(nonzero):\n            Ax[I_row[k]] = Ax[I_row[k]] + A_real[k] * x[I_col[k]]\n\n        for i in range(n):\n            xnew[i] = x[i] + 1.0 / D[i] * (b[i] - Ax[i])\n        x = xnew.copy()\n\n        if verbose:\n            print(f\"after {iter=}: \", end=\"\")\n            print_array(x.T, \"x.T\")\n\n    return x\n\n\ndef gauss_seidel_iteration_sparse(A_real, I_row, I_col, b, x0, max_iter, verbose=False):\n    \"\"\"\n    TODO\n    \"\"\"\n    n, nonzero = system_size_sparse(A_real, I_row, I_col, b)\n\n    x = x0.copy()\n    xnew = np.empty_like(x)\n\n    if verbose:\n        print(\"starting value: \", end=\"\")\n        print_array(x.T, \"x.T\")\n\n    for iter in range(max_iter):\n        # precompute Ax using xnew if i &lt; j\n        Ax = np.zeros_like(x)\n        for k in range(nonzero):\n            if I_row[k] &lt; I_col[k]:\n                Ax[I_row[k]] = Ax[I_row[k]] + A_real[k] * xnew[I_col[k]]\n            else:\n                Ax[I_row[k]] = Ax[I_row[k]] + A_real[k] * x[I_col[k]]\n\n        for i in range(n):\n            xnew[i] = x[i] + 1.0 / (A[i, i]) * (b[i] - Ax[i])\n        x = xnew.copy()\n\n        if verbose:\n            print(f\"after {iter=}: \", end=\"\")\n            print_array(x.T, \"x.T\")\n\n    return x\n\nThen we can test the two different implementations of the methods:\n\n# random matrix\nn = 4\nnonzero = 10\nA_real, I_row, I_col, b = random_sparse_system(n, nonzero)\nprint(\"sparse matrix:\")\nprint(\"A_real =\", A_real)\nprint(\"I_row = \", I_row)\nprint(\"I_col = \", I_col)\nprint()\n\n# convert to dense for comparison\nA_dense = to_dense(A_real, I_row, I_col)\nprint(\"dense matrix:\")\nprint_array(A_dense)\nprint()\n\n# starting guess\nx0 = np.zeros((n, 1))\n\nprint(\"jacobi with sparse matrix\")\nx_sparse = jacobi_iteration_sparse(A_real, I_row, I_col, b, x0, max_iter=5, verbose=True)\nprint() \n\nprint(\"jacobi with dense matrix\")\nx_dense = jacobi_iteration(A_dense, b, x0, max_iter=5, verbose=True)\nprint()\n\nsparse matrix:\nA_real = [11.25 -3.   21.25  6.25 11.25  3.   31.5  -3.    4.    3.  ]\nI_row =  [0 0 0 1 1 2 2 2 3 3]\nI_col =  [1 2 0 1 0 3 2 0 3 2]\n\ndense matrix:\nA_dense = [ 21.25, 11.25, -3.00,  0.00 ]\n          [ 11.25,  6.25,  0.00,  0.00 ]\n          [ -3.00,  0.00, 31.50,  3.00 ]\n          [  0.00,  0.00,  3.00,  4.00 ]\n\njacobi with sparse matrix\nstarting value: x.T = [  0.0,  0.0,  0.0,  0.0 ]\nafter iter=0: x.T = [  1.38824,  2.80000,  1.00000,  1.75000 ]\nafter iter=1: x.T = [  0.047059,  0.301176,  0.965546,  1.000000 ]\nafter iter=2: x.T = [  1.36510,  2.71529,  0.90924,  1.02584 ]\nafter iter=3: x.T = [  0.07909,  0.34282,  1.03231,  1.06807 ]\nafter iter=4: x.T = [  1.35248,  2.65764,  0.90581,  0.97577 ]\n\njacobi with dense matrix\nstarting value: x.T = [  0.0,  0.0,  0.0,  0.0 ]\nafter iter=0: x.T = [  1.38824,  2.80000,  1.00000,  1.75000 ]\nafter iter=1: x.T = [  0.047059,  0.301176,  0.965546,  1.000000 ]\nafter iter=2: x.T = [  1.36510,  2.71529,  0.90924,  1.02584 ]\nafter iter=3: x.T = [  0.07909,  0.34282,  1.03231,  1.06807 ]\nafter iter=4: x.T = [  1.35248,  2.65764,  0.90581,  0.97577 ]\n\n\n\nWe see that we get the same results!\nNow let’s see how long it takes to get a solution. The following plot shows the run times of using the two different implementations of the Jacobi method. We see that, as expected, the run time of the dense formulation is \\(O(n^2)\\) and the run time of the sparse formulation is \\(O(n)\\).\n\n\n\n\n\n\n\n\n\nWe say “as expected” because we have already counted the number of operations per iteration and these implementations compute for a fixed number of iterations. In the next section, we look at alternative stopping criteria.",
    "crumbs": [
      "Iterative solutions of linear equations"
    ]
  },
  {
    "objectID": "src/lec05.html#convergence-of-an-iterative-method",
    "href": "src/lec05.html#convergence-of-an-iterative-method",
    "title": "1 Iterative solutions of linear equations",
    "section": "",
    "text": "We have discussed the construction of iterations which aim to find the solution of the equations \\(A \\vec{x} = \\vec{b}\\) through a sequence of better and better approximations \\(\\vec{x}^{(k)}\\).\nIn general the iteration takes the form \\[\n\\vec{x}^{(k+1)} = \\vec{F}(\\vec{x}^{(k)})\n\\] here \\(\\vec{x}^{(k)}\\) is a vector of values and \\(\\vec{F}\\) is some vector-valued function which we have defined.\nHow can we decide if this iteration has converged? We need \\(\\vec{x} - \\vec{x}^{(k)}\\) to be small, but we don’t have access to the exact solution \\(\\vec{x}\\) so we have to do something else!\nHow do we decide that a vector/array is small? The most common measure is to use the “Euclidean norm” of an array (which you met last year!). This is defined to be the square root of the sum of squares of the entries of the array: \\[\n\\| \\vec{r} \\| = \\sqrt{ \\sum_{i=1}^n r_i^2 }.\n\\] where \\(\\vec{r}\\) is a vector with \\(n\\) entries.\n\n\n\n\n\n\nExamples\n\n\n\nConsider the following sequence \\(\\vec{x}^{(k)}\\):\n\\[\n\\begin{pmatrix}\n1 \\\\ -1\n\\end{pmatrix},\n\\begin{pmatrix}\n1.5 \\\\ 0.5\n\\end{pmatrix},\n\\begin{pmatrix}\n1.75 \\\\ 0.25\n\\end{pmatrix},\n\\begin{pmatrix}\n1.875 \\\\ 0.125\n\\end{pmatrix},\n\\begin{pmatrix}\n1.9375 \\\\ -0.0625\n\\end{pmatrix},\n\\begin{pmatrix}\n1.96875 \\\\ -0.03125\n\\end{pmatrix},\n\\ldots\n\\]\n\nWhat is \\(\\|\\vec{x}^{(1)} - \\vec{x}^{(0)}\\|\\)?\nWhat is \\(\\|\\vec{x}^{(5)} - \\vec{x}^{(4)}\\|\\)?\n\nLet \\(\\vec{x} = \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix}\\).\n\nWhat is \\(\\|\\vec{x} - \\vec{x}^{(3)}\\|\\)?\nWhat is \\(\\|\\vec{x} - \\vec{x}^{(4)}\\|\\)?\nWhat is \\(\\|\\vec{x} - \\vec{x}^{(5)}\\|\\)?\n\n\n\nRather than decide in advance how many iterations (of the Jacobi or Gauss-Seidel methods) to use stopping criteria:\n\nThis could be a maximum number of iterations.\nThis could be the change in values is small enough:\n\\[\n\\|x^{(k+1)} - \\vec{x}^{(k)}\\| &lt; tol,\n\\]\nThis could be the norm of the residual is small enough:\n\\[\n\\| \\vec{r} \\| = \\| \\vec{b} - A \\vec{x}^{(k)} \\| &lt; tol\n\\]\n\nIn both cases, we call \\(tol\\) the convergence tolerance and the choice of \\(tol\\) will control the accuracy of the solution.\n\n\n\n\n\n\nDiscussion\n\n\n\nWhat is a good convergence tolerance?\n\n\nIn general there are two possible reasons that an iteration may fail to converge.\n\nIt may diverge - this means that \\(\\|\\vec{x}^{(k)}\\| \\to \\infty\\) as \\(k\\) (the number of iterations) increases, e.g.:\n\\[\n\\begin{pmatrix}\n1 \\\\ 1\n\\end{pmatrix},\n\\begin{pmatrix}\n4 \\\\ 2\n\\end{pmatrix},\n\\begin{pmatrix}\n16 \\\\ 4\n\\end{pmatrix},\n\\begin{pmatrix}\n64 \\\\ 8\n\\end{pmatrix},\n\\begin{pmatrix}\n256 \\\\ 16\n\\end{pmatrix},\n\\begin{pmatrix}\n1024 \\\\ 32\n\\end{pmatrix},\n\\ldots\n\\]\nIt may neither converge nor diverge, e.g.:\n\\[\n\\begin{pmatrix}\n1 \\\\ 1\n\\end{pmatrix},\n\\begin{pmatrix}\n2 \\\\ 0\n\\end{pmatrix},\n\\begin{pmatrix}\n3 \\\\ 1\n\\end{pmatrix},\n\\begin{pmatrix}\n1 \\\\ 0\n\\end{pmatrix},\n\\begin{pmatrix}\n2 \\\\ 1\n\\end{pmatrix},\n\\begin{pmatrix}\n3 \\\\ 0\n\\end{pmatrix},\n\\ldots\n\\]\n\nIn addition to testing for convergence it is also necessary to include tests for failure to converge.\n\nDivergence may be detected by monitoring \\(\\|\\vec{x}^{(k)}\\|\\).\nImpose a maximum number of iterations to ensure that the loop is not repeated forever!",
    "crumbs": [
      "Iterative solutions of linear equations"
    ]
  },
  {
    "objectID": "src/lec05.html#summary",
    "href": "src/lec05.html#summary",
    "title": "1 Iterative solutions of linear equations",
    "section": "",
    "text": "Many complex computational problems simply cannot be solved with today’s computers using direct methods. Iterative methods are used instead since they can massively reduce the computational cost and storage required to get a “good enough” solution.\nThese basic iterative methods are simple to describe and program but generally slow to converge to an accurate answer - typically \\(O(n)\\) iterations are required! Their usefulness for general matrix systems is very limited therefore - but we have shown their value in the solution of sparse systems however.\nMore advanced iterative methods do exist but are beyond the scope of this module - see Final year projects, MSc projects, PhD, and beyond!",
    "crumbs": [
      "Iterative solutions of linear equations"
    ]
  },
  {
    "objectID": "src/lec05.html#further-reading",
    "href": "src/lec05.html#further-reading",
    "title": "1 Iterative solutions of linear equations",
    "section": "",
    "text": "More details on these basic (and related) methods:\n\nWikipedia: Jacobi method\nWikipedia: Gauss-Seidel method\nWikipedia: Iterative methods\n\nsee also Richardson method, Damped Jacobi method, Successive over-relaxation method (SOR), Symmetric successive over-relaxation method (SSOR) and Krylov subspace methods\n\n\nMore details on sparse matrices:\n\nWikipedia Sparse matrix - including a long detailed list of software libraries support sparse matrices.\nStackoverflow: Using a sparse matrix vs numpy array\nJason Brownlee: A gentle introduction to sparse matrices for machine learning, Machine learning mastery\n\nSome related textbooks:\n\nJack Dongarra Templates for the solution of linear systems: Stopping criteria\nJack Dongarra Templates for the solution of linear systems: Stationary iterative methods\nGolub, Gene H.; Van Loan, Charles F. (1996), Matrix Computations (3rd ed.), Baltimore: Johns Hopkins, ISBN 978-0-8018-5414-9.\nSaad, Yousef (2003). Iterative Methods for Sparse Linear Systems (2nd ed.). SIAM. p. 414. ISBN 0898715342.\n\nSome software implementations:\n\nscipy.sparse custom routines specialised to sparse matrices\nSuiteSparse, a suite of sparse matrix algorithms, geared toward the direct solution of sparse linear systems\nscipy.sparse iterative solvers: Solving linear problems\nPETSc: Linear system solvers - a high performance linear algebra toolkit",
    "crumbs": [
      "Iterative solutions of linear equations"
    ]
  },
  {
    "objectID": "src/lec02.html",
    "href": "src/lec02.html",
    "title": "1 Introduction to systems of linear equations",
    "section": "",
    "text": "The basic problem is to solve a set of \\(n\\) linear equations for \\(n\\) unknown values \\(x_j\\), \\(j = 1, \\ldots, n\\).\nNotation:\n\\[\n\\begin{aligned}\n\\text{Equation 1:} && a_{11} x_1 + a_{12} x_2 + a_{13} x_3 + \\cdots + a_{1n} x_n & = b_1 \\\\\n\\text{Equation 2:} && a_{21} x_1 + a_{22} x_2 + a_{23} x_3 + \\cdots + a_{2n} x_n & = b_2 \\\\\n\\vdots \\\\\n\\text{Equation i:} && a_{i1} x_1 + a_{i2} x_2 + a_{i3} x_3 + \\cdots + a_{in} x_n & = b_i \\\\\n\\vdots \\\\\n\\text{Equation n:} && a_{n1} x_1 + a_{n2} x_2 + a_{n3} x_3 + \\cdots + a_{nn} x_n & = b_n.\n\\end{aligned}\n\\]\nNotes:\n\nThe values \\(a_{ij}\\) are known as coefficients.\nThe right hand side values \\(b_i\\) are known and are given to you as part of the problem.\n\\(x_1, x_2, x_3, \\ldots, x_n\\) are not known and are what you need to find to solve the problem.\n\nMany computational algorithms require the solution of linear equations, e.g. in fields such as\n\nScientific computation;\nNetwork design and optimisation;\nGraphics and visualisation;\nMachine learning.\n\nTODO precise examples\nTypically these systems are very large (\\(n \\approx 10^9\\)).\nIt is therefore important that this problem can be solved\n\naccurately: we are allowed to make small errors but not big errors;\nefficiently: we need to find the answer quickly;\nreliably: we need to know that our algorithm will give us an answer that we are happy with.\n\n\n\n\n\n\n\nRemark\n\n\n\nOne way to solve a system of linear equations is to compute the inverse of \\(A\\), \\(A^{-1}\\), directly, then the solution is found through matrix multiplication: \\(\\vec{x} = A^{-1} \\vec{b}\\). This turns out to be an inefficient approach and we can do better with specialised algorithms.\n\n\n\n\nTrying different approaches for problem size 100\nApproach 1 - inverting the matrix and applying the inverse. Time = 0.001664876937866211\nApproach 2 - solving the system of linear equations. Time = 0.00039386749267578125\nApproach 2 is faster by a factor of  4.226997578692494\n\n\nTrying different approaches for problem size 10000\nApproach 1 - inverting the matrix and applying the inverse. Time = 18.403971195220947\nApproach 2 - solving the system of linear equations. Time = 5.693244218826294\nApproach 2 is faster by a factor of  3.2325982318417155\n\n\n\n\n\n\nSolve the system of equations given by\n\\[\n\\begin{pmatrix}\na_{11} & a_{12} & a_{13} & \\cdots & a_{1n} \\\\\na_{21} & a_{22} & a_{23} & \\cdots & a_{2n} \\\\\na_{31} & a_{32} & a_{33} & \\cdots & a_{3n} \\\\\n\\vdots & \\vdots & \\vdots & & \\vdots \\\\\na_{n1} & a_{n2} & a_{n3} & \\cdots & a_{nn}\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3 \\\\ \\vdots \\\\ x_n\n\\end{pmatrix} =\n\\begin{pmatrix}\nb_1 \\\\ b_2 \\\\ b_3 \\\\ \\vdots \\\\ b_n\n\\end{pmatrix}.\n\\]\nIn other words, given an \\(n \\times n\\) matrix \\(A\\) and an \\(n\\)-vector \\(\\vec{b}\\), find the \\(n\\)-vector \\(\\vec{x}\\) which satisfies \\[\nA \\vec{x} = \\vec{b}.\n\\]\n\n\n\n\n\n\nExample 1: Temperature in a sealed room\n\n\n\nSuppose we wish to estimate the temperature distribution inside an object:\n\n\n\n\n\nImage showing temperature sample points and relations in a room.\n\n\n\n\nWe can place a network of points inside the object and use the following model: the temperature at each interior point is the average of its neighbours.\nThis example leads to the system:\n\\[\n\\begin{pmatrix}\n  1   & -1/6 & -1/6 &  0   & -1/6 &  0   & 0 \\\\\n-1/6 &  1   & -1/6 & -1/6 &  0   & -1/6 & 0 \\\\\n-1/4 & -1/4 &  1   &  0   & -1/4 & -1/4 & 0 \\\\\n  0   & -1/5 &  0   &  1   &  0   & -1/5 & 0 \\\\\n-1/6 &  0   & -1/6 &  0   &  1   & -1/6 & -1/6 \\\\\n  0   & -1/8 & -1/8 & -1/8 & -1/8 & 1 & -1/8 \\\\\n  0   &  0   &  0   &  0   & -1/5 & -1/5 &   1\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\\\ x_5 \\\\ x_6 \\\\ x_7\n\\end{pmatrix} =\n\\begin{pmatrix}\n400/6 \\\\ 100/6 \\\\ 0 \\\\ 0 \\\\ 100/6 \\\\ 0 \\\\ 0\n\\end{pmatrix}.\n\\]\n\n\n\n\n\n\n\n\nExample 2: Traffic network\n\n\n\nSuppose we wish to monitor the flow of traffic in a city centre:\n\n\n\n\n\nExample network showing traffic flow in a city\n\n\n\n\nAs the above example shows, it is not necessary to monitor at every single road. If we know all of the \\(y\\) values we can calculate the \\(x\\) values!\nThis example leads to the system:\n\\[\n\\begin{pmatrix}\n1 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\\\\n1 & -1 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & -1 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & 1 & 0 & 0 & -1 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 1 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1 & 1 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & 1 & 0 & -1 \\\\\n0 & 0 & 0 & 0 & 0 & 0 & 1 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\\\ x_5 \\\\ x_6 \\\\ x_7 \\\\ x_8\n\\end{pmatrix} =\n\\begin{pmatrix}\ny_5 \\\\ y_{12} - y_6 \\\\ y_8 - y_7 \\\\ y_{11} - y_4 \\\\\ny_{11} + y_{12} - y_{10} \\\\ y_9 \\\\ y_2 - y_3 \\\\ y_1\n\\end{pmatrix}.\n\\]\n\n\n\n\n\n\nThe general matrix \\(A\\) before the examples is known as a full matrix: any of its components \\(a_{ij}\\) might be nonzero.\nAlmost always the problem being solved leads to a matrix with a particular structure of entries: Some entries may be known to be zero. If this is the case then it is often possible to use this knowledge to improve the efficiency of the algorithm (in terms of both speed and/or storage).\n\n\n\n\n\n\nExample 1: Triangular matrix\n\n\n\nOne common (and important) structure takes the form\n\\[\n\\begin{pmatrix}\na_{11} & 0 & 0 & \\cdots & 0 \\\\\na_{21} & a_{22} & 0 & \\cdots & 0 \\\\\na_{31} & a_{32} & a_{33} & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{n1} & a_{n2} & a_{n3} & \\cdots & a_{nn}\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3 \\\\ \\vdots \\\\ x_n\n\\end{pmatrix} =\n\\begin{pmatrix}\nb_1 \\\\ b_2 \\\\ b_3 \\\\ \\vdots \\\\ b_n\n\\end{pmatrix}.\n\\]\n\nA is a lower triangular matrix. Every entry above the leading diagonal is zero:\n\\[\na_{ij} = 0 \\quad \\text{ for } \\quad j &gt; i.\n\\]\nThe transpose of this matrix is an upper triangular matrix and can be treated in a very similar manner.\n\n\n\n\n\n\n\n\n\nExample 2: Sparse matrices\n\n\n\nSparse matrices are extremely common in any application which relies on some form of graph structure (see both temperature and traffic network examples).\n\nThe \\(a_{ij}\\) typically represents some form of “communication” between vertices \\(i\\) and \\(j\\) of the graph, so the element is only nonzero if the vertices are connected.\nThere is no generic pattern for these entries, though there is usually one that is specific to the problem solved.\nUsually \\(a_{ii} \\neq 0\\) - the diagonal is nonzero.\nA “large” portion of the matrix is zero.\n\nA full \\(n \\times n\\) matrix has \\(n^2\\) nonzero entries.\nA sparse \\(n \\times n\\) has \\(\\alpha n\\) nonzero entries, where \\(\\alpha \\ll n\\).\n\nMany special techniques exist for handling sparse matrices, some of which can be used automatically within Python (scipy.sparse documentation)\n\n\n\nWhat is the significance of these special examples?\n\nIn the next section we will discuss a general numerical algorithm for the solution of linear systems of equations.\nThis will involve reducing the problem to one involving a triangular matrix which, as we show below, is relatively easy to solve.\nIn subsequent lectures, we will see that, for sparse matrix systems, alternative solution techniques are available.\n\n\n\n\nFor the time-being we will only consider square systems of equations: for which the number of equations is equal to the number of unknowns (\\(n\\), say).\nIn this case the following statements are equivalent:\n\nThe linear system \\(A \\vec{x} = \\vec{b}\\) has a unique solution.\nThere exists a matrix (let’s call it \\(A^{-1}\\)) such that \\(A^{-1} A = I\\), and we say that the matrix \\(A\\) is invertible.\nThe linear system \\(A \\vec{x} = \\vec{b}\\) is non-singular.\n\n\n\n\n\nWikipedia: Systems of linear equations (includes a nice geometric picture of what a system of linear equations means).\nMaths is fun: Systems of linear equations (very basic!)\nGregory Gundersen Why shouldn’t I invert that matrix?",
    "crumbs": [
      "Introduction to systems of linear equations"
    ]
  },
  {
    "objectID": "src/lec02.html#introduction",
    "href": "src/lec02.html#introduction",
    "title": "1 Introduction to systems of linear equations",
    "section": "",
    "text": "The basic problem is to solve a set of \\(n\\) linear equations for \\(n\\) unknown values \\(x_j\\), \\(j = 1, \\ldots, n\\).\nNotation:\n\\[\n\\begin{aligned}\n\\text{Equation 1:} && a_{11} x_1 + a_{12} x_2 + a_{13} x_3 + \\cdots + a_{1n} x_n & = b_1 \\\\\n\\text{Equation 2:} && a_{21} x_1 + a_{22} x_2 + a_{23} x_3 + \\cdots + a_{2n} x_n & = b_2 \\\\\n\\vdots \\\\\n\\text{Equation i:} && a_{i1} x_1 + a_{i2} x_2 + a_{i3} x_3 + \\cdots + a_{in} x_n & = b_i \\\\\n\\vdots \\\\\n\\text{Equation n:} && a_{n1} x_1 + a_{n2} x_2 + a_{n3} x_3 + \\cdots + a_{nn} x_n & = b_n.\n\\end{aligned}\n\\]\nNotes:\n\nThe values \\(a_{ij}\\) are known as coefficients.\nThe right hand side values \\(b_i\\) are known and are given to you as part of the problem.\n\\(x_1, x_2, x_3, \\ldots, x_n\\) are not known and are what you need to find to solve the problem.\n\nMany computational algorithms require the solution of linear equations, e.g. in fields such as\n\nScientific computation;\nNetwork design and optimisation;\nGraphics and visualisation;\nMachine learning.\n\nTODO precise examples\nTypically these systems are very large (\\(n \\approx 10^9\\)).\nIt is therefore important that this problem can be solved\n\naccurately: we are allowed to make small errors but not big errors;\nefficiently: we need to find the answer quickly;\nreliably: we need to know that our algorithm will give us an answer that we are happy with.\n\n\n\n\n\n\n\nRemark\n\n\n\nOne way to solve a system of linear equations is to compute the inverse of \\(A\\), \\(A^{-1}\\), directly, then the solution is found through matrix multiplication: \\(\\vec{x} = A^{-1} \\vec{b}\\). This turns out to be an inefficient approach and we can do better with specialised algorithms.\n\n\n\n\nTrying different approaches for problem size 100\nApproach 1 - inverting the matrix and applying the inverse. Time = 0.001664876937866211\nApproach 2 - solving the system of linear equations. Time = 0.00039386749267578125\nApproach 2 is faster by a factor of  4.226997578692494\n\n\nTrying different approaches for problem size 10000\nApproach 1 - inverting the matrix and applying the inverse. Time = 18.403971195220947\nApproach 2 - solving the system of linear equations. Time = 5.693244218826294\nApproach 2 is faster by a factor of  3.2325982318417155\n\n\n\n\n\n\nSolve the system of equations given by\n\\[\n\\begin{pmatrix}\na_{11} & a_{12} & a_{13} & \\cdots & a_{1n} \\\\\na_{21} & a_{22} & a_{23} & \\cdots & a_{2n} \\\\\na_{31} & a_{32} & a_{33} & \\cdots & a_{3n} \\\\\n\\vdots & \\vdots & \\vdots & & \\vdots \\\\\na_{n1} & a_{n2} & a_{n3} & \\cdots & a_{nn}\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3 \\\\ \\vdots \\\\ x_n\n\\end{pmatrix} =\n\\begin{pmatrix}\nb_1 \\\\ b_2 \\\\ b_3 \\\\ \\vdots \\\\ b_n\n\\end{pmatrix}.\n\\]\nIn other words, given an \\(n \\times n\\) matrix \\(A\\) and an \\(n\\)-vector \\(\\vec{b}\\), find the \\(n\\)-vector \\(\\vec{x}\\) which satisfies \\[\nA \\vec{x} = \\vec{b}.\n\\]\n\n\n\n\n\n\nExample 1: Temperature in a sealed room\n\n\n\nSuppose we wish to estimate the temperature distribution inside an object:\n\n\n\n\n\nImage showing temperature sample points and relations in a room.\n\n\n\n\nWe can place a network of points inside the object and use the following model: the temperature at each interior point is the average of its neighbours.\nThis example leads to the system:\n\\[\n\\begin{pmatrix}\n  1   & -1/6 & -1/6 &  0   & -1/6 &  0   & 0 \\\\\n-1/6 &  1   & -1/6 & -1/6 &  0   & -1/6 & 0 \\\\\n-1/4 & -1/4 &  1   &  0   & -1/4 & -1/4 & 0 \\\\\n  0   & -1/5 &  0   &  1   &  0   & -1/5 & 0 \\\\\n-1/6 &  0   & -1/6 &  0   &  1   & -1/6 & -1/6 \\\\\n  0   & -1/8 & -1/8 & -1/8 & -1/8 & 1 & -1/8 \\\\\n  0   &  0   &  0   &  0   & -1/5 & -1/5 &   1\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\\\ x_5 \\\\ x_6 \\\\ x_7\n\\end{pmatrix} =\n\\begin{pmatrix}\n400/6 \\\\ 100/6 \\\\ 0 \\\\ 0 \\\\ 100/6 \\\\ 0 \\\\ 0\n\\end{pmatrix}.\n\\]\n\n\n\n\n\n\n\n\nExample 2: Traffic network\n\n\n\nSuppose we wish to monitor the flow of traffic in a city centre:\n\n\n\n\n\nExample network showing traffic flow in a city\n\n\n\n\nAs the above example shows, it is not necessary to monitor at every single road. If we know all of the \\(y\\) values we can calculate the \\(x\\) values!\nThis example leads to the system:\n\\[\n\\begin{pmatrix}\n1 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\\\\n1 & -1 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & -1 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & 1 & 0 & 0 & -1 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 1 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1 & 1 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & 1 & 0 & -1 \\\\\n0 & 0 & 0 & 0 & 0 & 0 & 1 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\\\ x_5 \\\\ x_6 \\\\ x_7 \\\\ x_8\n\\end{pmatrix} =\n\\begin{pmatrix}\ny_5 \\\\ y_{12} - y_6 \\\\ y_8 - y_7 \\\\ y_{11} - y_4 \\\\\ny_{11} + y_{12} - y_{10} \\\\ y_9 \\\\ y_2 - y_3 \\\\ y_1\n\\end{pmatrix}.\n\\]",
    "crumbs": [
      "Introduction to systems of linear equations"
    ]
  },
  {
    "objectID": "src/lec02.html#special-types-of-matrices",
    "href": "src/lec02.html#special-types-of-matrices",
    "title": "1 Introduction to systems of linear equations",
    "section": "",
    "text": "The general matrix \\(A\\) before the examples is known as a full matrix: any of its components \\(a_{ij}\\) might be nonzero.\nAlmost always the problem being solved leads to a matrix with a particular structure of entries: Some entries may be known to be zero. If this is the case then it is often possible to use this knowledge to improve the efficiency of the algorithm (in terms of both speed and/or storage).\n\n\n\n\n\n\nExample 1: Triangular matrix\n\n\n\nOne common (and important) structure takes the form\n\\[\n\\begin{pmatrix}\na_{11} & 0 & 0 & \\cdots & 0 \\\\\na_{21} & a_{22} & 0 & \\cdots & 0 \\\\\na_{31} & a_{32} & a_{33} & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{n1} & a_{n2} & a_{n3} & \\cdots & a_{nn}\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3 \\\\ \\vdots \\\\ x_n\n\\end{pmatrix} =\n\\begin{pmatrix}\nb_1 \\\\ b_2 \\\\ b_3 \\\\ \\vdots \\\\ b_n\n\\end{pmatrix}.\n\\]\n\nA is a lower triangular matrix. Every entry above the leading diagonal is zero:\n\\[\na_{ij} = 0 \\quad \\text{ for } \\quad j &gt; i.\n\\]\nThe transpose of this matrix is an upper triangular matrix and can be treated in a very similar manner.\n\n\n\n\n\n\n\n\n\nExample 2: Sparse matrices\n\n\n\nSparse matrices are extremely common in any application which relies on some form of graph structure (see both temperature and traffic network examples).\n\nThe \\(a_{ij}\\) typically represents some form of “communication” between vertices \\(i\\) and \\(j\\) of the graph, so the element is only nonzero if the vertices are connected.\nThere is no generic pattern for these entries, though there is usually one that is specific to the problem solved.\nUsually \\(a_{ii} \\neq 0\\) - the diagonal is nonzero.\nA “large” portion of the matrix is zero.\n\nA full \\(n \\times n\\) matrix has \\(n^2\\) nonzero entries.\nA sparse \\(n \\times n\\) has \\(\\alpha n\\) nonzero entries, where \\(\\alpha \\ll n\\).\n\nMany special techniques exist for handling sparse matrices, some of which can be used automatically within Python (scipy.sparse documentation)\n\n\n\nWhat is the significance of these special examples?\n\nIn the next section we will discuss a general numerical algorithm for the solution of linear systems of equations.\nThis will involve reducing the problem to one involving a triangular matrix which, as we show below, is relatively easy to solve.\nIn subsequent lectures, we will see that, for sparse matrix systems, alternative solution techniques are available.",
    "crumbs": [
      "Introduction to systems of linear equations"
    ]
  },
  {
    "objectID": "src/lec02.html#uniqueness-of-solutions",
    "href": "src/lec02.html#uniqueness-of-solutions",
    "title": "1 Introduction to systems of linear equations",
    "section": "",
    "text": "For the time-being we will only consider square systems of equations: for which the number of equations is equal to the number of unknowns (\\(n\\), say).\nIn this case the following statements are equivalent:\n\nThe linear system \\(A \\vec{x} = \\vec{b}\\) has a unique solution.\nThere exists a matrix (let’s call it \\(A^{-1}\\)) such that \\(A^{-1} A = I\\), and we say that the matrix \\(A\\) is invertible.\nThe linear system \\(A \\vec{x} = \\vec{b}\\) is non-singular.",
    "crumbs": [
      "Introduction to systems of linear equations"
    ]
  },
  {
    "objectID": "src/lec02.html#further-reading",
    "href": "src/lec02.html#further-reading",
    "title": "1 Introduction to systems of linear equations",
    "section": "",
    "text": "Wikipedia: Systems of linear equations (includes a nice geometric picture of what a system of linear equations means).\nMaths is fun: Systems of linear equations (very basic!)\nGregory Gundersen Why shouldn’t I invert that matrix?",
    "crumbs": [
      "Introduction to systems of linear equations"
    ]
  }
]